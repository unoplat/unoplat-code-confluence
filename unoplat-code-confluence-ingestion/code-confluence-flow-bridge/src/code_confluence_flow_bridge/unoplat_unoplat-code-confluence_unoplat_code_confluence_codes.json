[{"NodeName":"ArchGuardHandler","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/codebaseparser/arc_guard_handler.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"jar_path","TypeType":""},{"TypeValue":"language","TypeType":""},{"TypeValue":"codebase_path","TypeType":""},{"TypeValue":"codebase_name","TypeType":""},{"TypeValue":"output_path","TypeType":""},{"TypeValue":"extension","TypeType":"str"}],"FunctionCalls":[{"FunctionName":"TotalFileCount","Position":{"StartLine":23,"StartLinePosition":42,"StopLine":23,"StopLinePosition":83}}],"Position":{"StartLine":13,"StartLinePosition":4,"StopLine":25,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.jar_path","TypeType":"jar_path"},{"TypeValue":"self.language","TypeType":"language"},{"TypeValue":"self.codebase_path","TypeType":"codebase_path"},{"TypeValue":"self.output_path","TypeType":"output_path"},{"TypeValue":"self.total_files","TypeType":"0"},{"TypeValue":"self.current_file_count","TypeType":"0"},{"TypeValue":"self.codebase_name","TypeType":"codebase_name"},{"TypeValue":"self.extension","TypeType":"extension"},{"TypeValue":"self.file_counter","TypeType":"TotalFileCount"}],"Content":"def __init__(self, jar_path, language, codebase_path,codebase_name, output_path,extension:str):        self.jar_path = jar_path        self.language = language        self.codebase_path = codebase_path        self.output_path = output_path        self.total_files = 0        self.current_file_count = 0        self.codebase_name = codebase_name        self.extension  = extension        # Initialize FileCounter in the constructor        self.file_counter = TotalFileCount(self.codebase_path, f'.{self.extension}')    "},{"Name":"run_scan","FunctionCalls":[{"NodeName":"logger","FunctionName":"info","Position":{"StartLine":63,"StartLinePosition":14,"StopLine":63,"StopLinePosition":62}}],"Position":{"StartLine":25,"StartLinePosition":4,"StopLine":67,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.jar_path","TypeType":"jar_path"},{"TypeValue":"self.language","TypeType":"language"},{"TypeValue":"self.codebase_path","TypeType":"codebase_path"},{"TypeValue":"self.output_path","TypeType":"output_path"},{"TypeValue":"self.total_files","TypeType":"self"},{"TypeValue":"self.current_file_count","TypeType":"0"},{"TypeValue":"self.codebase_name","TypeType":"codebase_name"},{"TypeValue":"self.extension","TypeType":"extension"},{"TypeValue":"self.file_counter","TypeType":"TotalFileCount"},{"TypeValue":"command","TypeType":"[\"java\",\"-jar\",self.jar_path,\"--with-function-code\",f\"--language={self.language}\",\"--output=arrow\",\"--output=json\",f\"--path={self.codebase_path}\",f\"--output-dir={self.output_path}\"]"},{"TypeValue":"process","TypeType":"subprocess"},{"TypeValue":"output","TypeType":"process"},{"TypeValue":"progress_value","TypeType":"self"},{"TypeValue":"","TypeType":"process"},{"TypeValue":"chapi_metadata_path","TypeType":"self"}],"Content":"def run_scan(self) -> str:        # Get total number of files in run_scan        self.total_files = self.file_counter.count_files()        logger.info(\"Starting scan...\")        command = [            \"java\", \"-jar\", self.jar_path,            \"--with-function-code\",            f\"--language={self.language}\",            \"--output=arrow\", \"--output=json\",            f\"--path={self.codebase_path}\",            f\"--output-dir={self.output_path}\"        ]        logger.info(f\"Command: {' '.join(command)}\")                process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)                if process.stdout is None:            raise RuntimeError(\"Failed to open subprocess stdout\")                    while True:            output = process.stdout.readline()            logger.debug(output)            if output == '' and process.poll() is not None:                break            if output:                logger.info(output.strip())                progress_value = self.parse_progress(output, total_files=self.total_files)                logger.info(f\"Progress: {progress_value}%\")        _stdout, stderr = process.communicate()        if process.returncode == 0:            logger.info(\"Scan completed successfully\")            chapi_metadata_path = self.modify_output_filename(\"0_codes.json\", f\"{self.codebase_name}_codes.json\")        else:            logger.error(f\"Error in scanning: {stderr}\")        logger.info(f\"Total files scanned: {self.total_files}\")        return chapi_metadata_path    "},{"Name":"parse_progress","Parameters":[{"TypeValue":"output","TypeType":""},{"TypeValue":"total_files","TypeType":""}],"Position":{"StartLine":67,"StartLinePosition":4,"StopLine":77,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.jar_path","TypeType":"jar_path"},{"TypeValue":"self.language","TypeType":"language"},{"TypeValue":"self.codebase_path","TypeType":"codebase_path"},{"TypeValue":"self.output_path","TypeType":"output_path"},{"TypeValue":"self.total_files","TypeType":"self"},{"TypeValue":"self.current_file_count","TypeType":""},{"TypeValue":"self.codebase_name","TypeType":"codebase_name"},{"TypeValue":"self.extension","TypeType":"extension"},{"TypeValue":"self.file_counter","TypeType":"TotalFileCount"},{"TypeValue":"command","TypeType":"[\"java\",\"-jar\",self.jar_path,\"--with-function-code\",f\"--language={self.language}\",\"--output=arrow\",\"--output=json\",f\"--path={self.codebase_path}\",f\"--output-dir={self.output_path}\"]"},{"TypeValue":"process","TypeType":"subprocess"},{"TypeValue":"output","TypeType":"process"},{"TypeValue":"progress_value","TypeType":"self"},{"TypeValue":"","TypeType":"process"},{"TypeValue":"chapi_metadata_path","TypeType":"self"},{"TypeValue":"progress_percentage","TypeType":""}],"Content":"def parse_progress(self, output, total_files):        if total_files == 0:            return 0        else:            if \"analysis file\" in output:                self.current_file_count += 1            progress_percentage = (self.current_file_count / total_files) * 100                        return int(progress_percentage)            "},{"Name":"modify_output_filename","Parameters":[{"TypeValue":"old_filename","TypeType":""},{"TypeValue":"new_filename","TypeType":""}],"FunctionCalls":[{"NodeName":"os","FunctionName":"rename","Position":{"StartLine":81,"StartLinePosition":10,"StopLine":81,"StopLinePosition":46}}],"Position":{"StartLine":77,"StartLinePosition":4,"StopLine":84},"LocalVariables":[{"TypeValue":"self.jar_path","TypeType":"jar_path"},{"TypeValue":"self.language","TypeType":"language"},{"TypeValue":"self.codebase_path","TypeType":"codebase_path"},{"TypeValue":"self.output_path","TypeType":"output_path"},{"TypeValue":"self.total_files","TypeType":"self"},{"TypeValue":"self.current_file_count","TypeType":""},{"TypeValue":"self.codebase_name","TypeType":"codebase_name"},{"TypeValue":"self.extension","TypeType":"extension"},{"TypeValue":"self.file_counter","TypeType":"TotalFileCount"},{"TypeValue":"command","TypeType":"[\"java\",\"-jar\",self.jar_path,\"--with-function-code\",f\"--language={self.language}\",\"--output=arrow\",\"--output=json\",f\"--path={self.codebase_path}\",f\"--output-dir={self.output_path}\"]"},{"TypeValue":"process","TypeType":"subprocess"},{"TypeValue":"output","TypeType":"process"},{"TypeValue":"progress_value","TypeType":"self"},{"TypeValue":"","TypeType":"process"},{"TypeValue":"chapi_metadata_path","TypeType":"self"},{"TypeValue":"progress_percentage","TypeType":""},{"TypeValue":"current_directory","TypeType":"os"},{"TypeValue":"old_file_path","TypeType":"os"},{"TypeValue":"new_file_path","TypeType":"os"}],"Content":"def modify_output_filename(self, old_filename, new_filename) -> str:        current_directory = os.getcwd()        old_file_path = os.path.join(current_directory, old_filename)        new_file_path = os.path.join(current_directory, new_filename)        os.rename(old_file_path, new_file_path)        return new_file_path"}],"Imports":[{"Source":"unoplat_code_confluence.utility.total_file_count","UsageName":["TotalFileCount"]},{"Source":"os"},{"Source":"subprocess"},{"Source":"loguru","UsageName":["logger"]}],"Position":{"StartLine":12,"StopLine":84},"Content":"class ArchGuardHandler:    def __init__(self, jar_path, language, codebase_path,codebase_name, output_path,extension:str):        self.jar_path = jar_path        self.language = language        self.codebase_path = codebase_path        self.output_path = output_path        self.total_files = 0        self.current_file_count = 0        self.codebase_name = codebase_name        self.extension  = extension        # Initialize FileCounter in the constructor        self.file_counter = TotalFileCount(self.codebase_path, f'.{self.extension}')    def run_scan(self) -> str:        # Get total number of files in run_scan        self.total_files = self.file_counter.count_files()        logger.info(\"Starting scan...\")        command = [            \"java\", \"-jar\", self.jar_path,            \"--with-function-code\",            f\"--language={self.language}\",            \"--output=arrow\", \"--output=json\",            f\"--path={self.codebase_path}\",            f\"--output-dir={self.output_path}\"        ]        logger.info(f\"Command: {' '.join(command)}\")                process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1)                if process.stdout is None:            raise RuntimeError(\"Failed to open subprocess stdout\")                    while True:            output = process.stdout.readline()            logger.debug(output)            if output == '' and process.poll() is not None:                break            if output:                logger.info(output.strip())                progress_value = self.parse_progress(output, total_files=self.total_files)                logger.info(f\"Progress: {progress_value}%\")        _stdout, stderr = process.communicate()        if process.returncode == 0:            logger.info(\"Scan completed successfully\")            chapi_metadata_path = self.modify_output_filename(\"0_codes.json\", f\"{self.codebase_name}_codes.json\")        else:            logger.error(f\"Error in scanning: {stderr}\")        logger.info(f\"Total files scanned: {self.total_files}\")        return chapi_metadata_path    def parse_progress(self, output, total_files):        if total_files == 0:            return 0        else:            if \"analysis file\" in output:                self.current_file_count += 1            progress_percentage = (self.current_file_count / total_files) * 100                        return int(progress_percentage)            def modify_output_filename(self, old_filename, new_filename) -> str:        current_directory = os.getcwd()        old_file_path = os.path.join(current_directory, old_filename)        new_file_path = os.path.join(current_directory, new_filename)        os.rename(old_file_path, new_file_path)        return new_file_path"},{"NodeName":"ForgeUnoplatCodebaseSummary","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/forge_summary/forge_unoplat_codebase_summary.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Dict","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]},{"Source":"unoplat_code_confluence.data_models.forge_summary.forge_unoplat_package_summary","UsageName":["DspyUnoplatPackageSummary"]}],"Position":{"StartLine":12,"StopLine":17,"StopLinePosition":145},"Content":"class ForgeUnoplatCodebaseSummary(BaseModel):    codebase_summary: Optional[str] = Field(default=None, description=\"A summary of the codebase\")    codebase_objective: Optional[str] = Field(default=None, description=\"The objective of the codebase\")    metadata: Optional[dict] = Field(default=None, description=\"The metadata of the codebase\")    codebase_name: Optional[str] = Field( default=None,description=\"The file id of the codebase summary\")    codebase_package: Optional[Dict[str,DspyUnoplatPackageSummary]] = Field(default_factory=dict,description=\"A summary of the codebase package\")"},{"NodeName":"DspyUnoplatNodeSummary","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/forge_summary/forge_unoplat_node_summary.py","MultipleExtend":["UnoplatChapiForgeNode"],"Imports":[{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["Field"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_chapi_forge_node","UsageName":["UnoplatChapiForgeNode"]},{"Source":"unoplat_code_confluence.data_models.forge_summary.forge_unoplat_function_summary","UsageName":["DspyUnoplatFunctionSummary"]}],"Position":{"StartLine":12,"StopLine":19,"StopLinePosition":4},"Content":"class DspyUnoplatNodeSummary(UnoplatChapiForgeNode):    node_summary: str = Field(alias=\"NodeSummary\",description=\"A summary of the class\")    node_objective: str = Field(alias=\"NodeObjective\",description=\"The objective of the class\")    functions_summary: Optional[List[DspyUnoplatFunctionSummary]] = Field(default=None, alias=\"FunctionsSummary\",description=\"A list of functions in the class\")    class_objective_embedding: List[float] = Field(default_factory=list, alias=\"ClassObjectiveEmbedding\")    class_implementation_summary_embedding: List[float] = Field(default_factory=list, alias=\"ClassImplementationSummaryEmbedding\")        "},{"NodeName":"DspyUnoplatPackageSummary","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/forge_summary/forge_unoplat_package_summary.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]},{"Source":"unoplat_code_confluence.data_models.forge_summary.forge_unoplat_node_summary","UsageName":["DspyUnoplatNodeSummary"]}],"Position":{"StartLine":12,"StopLine":19},"Content":"class DspyUnoplatPackageSummary(BaseModel):    package_objective: str = Field( description=\"The objective of the package in a concise manner\")    package_summary: str = Field( description=\"The detailed summary of the package\")    class_summary: List[DspyUnoplatNodeSummary] = Field( default_factory=list,description=\"List of the class summaries for the package\")    metadata: Optional[dict] = Field(default=None, description=\"Additional metadata for the package\")    sub_package_summaries: Dict[str, 'DspyUnoplatPackageSummary'] = Field(default_factory=dict, description=\"Dictionary of sub-package summaries, keyed by package name\")    "},{"NodeName":"DspyUnoplatFunctionSummary","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/forge_summary/forge_unoplat_function_summary.py","MultipleExtend":["UnoplatChapiForgeFunction"],"Imports":[{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["Field"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_chapi_forge_function","UsageName":["UnoplatChapiForgeFunction"]}],"Position":{"StartLine":11,"StopLine":17},"Content":"class DspyUnoplatFunctionSummary(UnoplatChapiForgeFunction):    objective: Optional[str] = Field(default=None, alias=\"Objective\",description=\"This should include high level objective of what function does based on function content and function metadata. Should not be more than 3 lines.\")    implementation_summary: Optional[str] = Field(default=None, alias=\"ImplementationSummary\",description=\"This should include implementation details of the function. make sure if this function makes internal calls to other functions of same class and to external calls to other classes/libs is also covered. Use all metadata shared for the function to answer .\")    function_objective_embedding: List[float] = Field(default_factory=list, alias=\"FunctionObjectiveEmbedding\")    function_implementation_summary_embedding: List[float] = Field(default_factory=list, alias=\"FunctionImplementationSummaryEmbedding\")    "},{"NodeName":"UnoplatPackageManagerMetadata","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi_forge/unoplat_package_manager_metadata.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_project_dependency","UsageName":["UnoplatProjectDependency"]}],"Position":{"StartLine":11,"StopLine":24,"StopLinePosition":5},"Content":"class UnoplatPackageManagerMetadata(BaseModel):    dependencies: Dict[str,UnoplatProjectDependency] = Field(default_factory=dict, description=\"The dependencies of the project\")    package_name: Optional[str] = Field(default=None, description=\"The name of the package\")    programming_language: str = Field(description=\"The programming language of the project\")    package_manager: str = Field(description=\"The package manager of the project\")    programming_language_version: Optional[Dict[str,str]] = Field(default=None, description=\"The version of the programming language\")    project_version: Optional[str] = Field(default=None, description=\"The version of the project\")    description: Optional[str] = Field(default=None, description=\"The description of the project\")    authors: Optional[List[str]] = Field(default=None, description=\"The authors of the project\")    license: Optional[str] = Field(default=None, description=\"The license of the project\")    entry_points: Dict[str, str] = Field(        default_factory=dict,         description=\"Dictionary of script names to their entry points. Example: {'cli': 'package.module:main', 'serve': 'uvicorn app:main'}\"    )"},{"NodeName":"UnoplatChapiForgeNode","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi_forge/unoplat_chapi_forge_node.py","MultipleExtend":["ChapiNode"],"Functions":[{"Name":"from_chapi_node","Parameters":[{"TypeValue":"cls","TypeType":""},{"TypeValue":"chapi_node","TypeType":"ChapiNode"}],"Annotations":[{"Name":"classmethod","Position":{"StartLine":25,"StartLinePosition":4,"StopLine":26,"StopLinePosition":4}}],"Position":{"StartLine":26,"StartLinePosition":4,"StopLine":46,"StopLinePosition":11},"LocalVariables":[{"TypeValue":"qualified_name","TypeType":""},{"TypeValue":"comments_description","TypeType":""},{"TypeValue":"segregated_imports","TypeType":""},{"TypeValue":"dependent_internal_classes","TypeType":""},{"TypeValue":"functions","TypeType":""},{"TypeValue":"global_variables","TypeType":""},{"TypeValue":"chapi_node_data","TypeType":""},{"TypeValue":"func_qualified_name","TypeType":"f\"{additional_fields['qualified_name']}.{function['Name']}\""},{"TypeValue":"function[\"QualifiedName\"]","TypeType":"func_qualified_name"}],"Content":"def from_chapi_node(cls, chapi_node: ChapiNode, **additional_fields):        chapi_node_data: dict[str,Any] = chapi_node.model_dump(by_alias=True)                # Check for qualified_name (assuming this is required)        if \"qualified_name\" in additional_fields and additional_fields[\"qualified_name\"]:            chapi_node_data.update({\"QualifiedName\": additional_fields[\"qualified_name\"]})                # More thorough checking for segregated_imports        if \"segregated_imports\" in additional_fields and additional_fields[\"segregated_imports\"] is not None:            chapi_node_data.update({\"SegregatedImports\": additional_fields[\"segregated_imports\"]})                # Check functions if they exist        if chapi_node_data.get(\"Functions\"):  # Using .get() is safer than direct access            for function in chapi_node_data[\"Functions\"]:                func_qualified_name = f\"{additional_fields['qualified_name']}.{function['Name']}\"                function[\"QualifiedName\"] = func_qualified_name                # Create new child instance with combined data        return cls.model_validate(chapi_node_data)                   "}],"Imports":[{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["Field"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_class_global_fieldmodel","UsageName":["ClassGlobalFieldModel"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_node","UsageName":["ChapiNode"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_chapi_forge_function","UsageName":["UnoplatChapiForgeFunction"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_import","UsageName":["UnoplatImport"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_import_type","UsageName":["ImportType"]}],"Position":{"StartLine":15,"StopLine":46,"StopLinePosition":11},"Content":"class UnoplatChapiForgeNode(ChapiNode):        qualified_name: str = Field(default=None, alias=\"QualifiedName\",exclude=True,description=\"name of class with absolute path\")    comments_description: Optional[str] = Field(default=None, alias=\"CommentsDescription\",description=\"description of the node from comments\")    segregated_imports: Optional[Dict[ImportType,List[UnoplatImport]]] = Field(default=None,alias=\"SegregatedImports\", description=\"SegregatedImports in terms of internal ,external ,standard and local libraries\")    dependent_internal_classes: Optional[List[str]] = Field(default_factory=list,alias=\"DependentInternalClasses\",description=\"list of unique internalclasses that this node is dependent on\")    functions: Optional[List[UnoplatChapiForgeFunction]] = Field(default_factory=list,alias=\"Functions\",description=\"functions of the node\") #type: ignore    global_variables: Optional[List[ClassGlobalFieldModel]] = Field(default_factory=list,alias=\"GlobalVariables\",description=\"global variables of the node\") #type: ignore            @classmethod     def from_chapi_node(cls, chapi_node: ChapiNode, **additional_fields):        chapi_node_data: dict[str,Any] = chapi_node.model_dump(by_alias=True)                # Check for qualified_name (assuming this is required)        if \"qualified_name\" in additional_fields and additional_fields[\"qualified_name\"]:            chapi_node_data.update({\"QualifiedName\": additional_fields[\"qualified_name\"]})                # More thorough checking for segregated_imports        if \"segregated_imports\" in additional_fields and additional_fields[\"segregated_imports\"] is not None:            chapi_node_data.update({\"SegregatedImports\": additional_fields[\"segregated_imports\"]})                # Check functions if they exist        if chapi_node_data.get(\"Functions\"):  # Using .get() is safer than direct access            for function in chapi_node_data[\"Functions\"]:                func_qualified_name = f\"{additional_fields['qualified_name']}.{function['Name']}\"                function[\"QualifiedName\"] = func_qualified_name                # Create new child instance with combined data        return cls.model_validate(chapi_node_data)                   "},{"NodeName":"UnoplatGitRepository","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi_forge/unoplat_git_repository.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Any","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_codebase","UsageName":["UnoplatCodebase"]}],"Position":{"StartLine":11,"StopLine":18,"StopLinePosition":117},"Content":"class UnoplatGitRepository(BaseModel):    repository_url: str = Field(description=\"The URL of the repository\")    repository_name: str = Field(description=\"The name of the repository\")    repository_metadata: dict[str, Any] = Field(description=\"The metadata of the repository\")    codebases: List[UnoplatCodebase] = Field(default_factory=list, description=\"The codebases of the repository\")    readme: Optional[str] = Field(default=None, description=\"The readme of the repository\")    domain: Optional[str] = Field(default=None, description=\"The domain of the repository\")    github_organization: Optional[str] = Field(default=None, description=\"The github organization of the repository\")"},{"NodeName":"UnoplatChapiForgeFunction","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi_forge/unoplat_chapi_forge_function.py","MultipleExtend":["ChapiFunction"],"Imports":[{"Source":"typing","UsageName":["Optional"]},{"Source":"pydantic","UsageName":["Field"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_function","UsageName":["ChapiFunction"]}],"Position":{"StartLine":11,"StopLine":16,"StopLinePosition":4},"Content":"class UnoplatChapiForgeFunction(ChapiFunction):    qualified_name: str = Field(alias=\"QualifiedName\", description=\"The qualified name of the function\")    comments_description: Optional[str] = Field(default=None, alias=\"CommentsDescription\", description=\"description of the function from comments\")        "},{"NodeName":"ImportType","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi_forge/unoplat_import_type.py","MultipleExtend":["Enum"],"Imports":[{"Source":"enum","UsageName":["Enum"]}],"Position":{"StartLine":6,"StopLine":12},"Content":"class ImportType(Enum):    INTERNAL = \"INTERNAL\"  # First party imports    EXTERNAL = \"EXTERNAL\"  # Third party imports    STANDARD = \"STANDARD\"  # Standard library imports    LOCAL = \"LOCAL\"    "},{"NodeName":"UnoplatProjectDependency","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi_forge/unoplat_project_dependency.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_version","UsageName":["UnoplatVersion"]}],"Position":{"StartLine":11,"StopLine":19,"StopLinePosition":4},"Content":"class UnoplatProjectDependency(BaseModel):    version: UnoplatVersion = Field(description=\"Version of the dependency\")    group: Optional[str] = Field(default=None, description=\"Group of the dependency (e.g. dev, test)\")    extras: Optional[List[str]] = Field(default=None, description=\"Extra features or options for the dependency\")    source: Optional[str] = Field(default=None, description=\"Source of the dependency (e.g. git, pypi)\")    source_url: Optional[str] = Field(default=None, description=\"URL of the dependency source\")    source_reference: Optional[str] = Field(default=None, description=\"Reference (e.g. commit hash, branch) for source\")    subdirectory: Optional[str] = Field(default=None, description=\"Subdirectory within source\")    "},{"NodeName":"UnoplatCodebase","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi_forge/unoplat_codebase.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_package","UsageName":["UnoplatPackage"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]}],"Position":{"StartLine":12,"StopLine":18},"Content":"class UnoplatCodebase(BaseModel):    name: str = Field(description=\"Name of the codebase usually the root package name\")    readme: Optional[str] = Field(default=None)    packages: Optional[UnoplatPackage] = Field(default=None)    package_manager_metadata: UnoplatPackageManagerMetadata = Field(description=\"The package manager metadata of the codebase\")    local_path: str = Field(description=\"Local path of the codebase\")"},{"NodeName":"UnoplatVersion","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi_forge/unoplat_version.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":8,"StopLine":12},"Content":"class UnoplatVersion(BaseModel):    minimum_version: Optional[str] = Field(default=None, description=\"The minimum version of the project\")    maximum_version: Optional[str] = Field(default=None, description=\"The maximum version of the project\")    current_version: Optional[str] = Field(default=None, description=\"The current version of the project\")"},{"NodeName":"ImportedName","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi_forge/unoplat_import.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_import_type","UsageName":["ImportType"]}],"Position":{"StartLine":11,"StopLine":15},"Content":"class ImportedName(BaseModel):    original_name: Optional[str] = None    alias: Optional[str] = None"},{"NodeName":"UnoplatImport","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi_forge/unoplat_import.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_import_type","UsageName":["ImportType"]}],"Position":{"StartLine":15,"StopLine":19},"Content":"class UnoplatImport(BaseModel):    source: Optional[str] = Field(default=None, alias=\"Source\")    usage_names: Optional[List[ImportedName]] = Field(default_factory=list, alias=\"UsageName\")    import_type: Optional[ImportType] = Field(default=None, alias=\"ImportType\")"},{"NodeName":"UnoplatPackage","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi_forge/unoplat_package.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_chapi_forge_node","UsageName":["UnoplatChapiForgeNode"]}],"Position":{"StartLine":12,"StopLine":17},"Content":"class UnoplatPackage(BaseModel):    name: Optional[str] = Field(default=None,description=\"Name of the package\")    nodes: Dict[str, List[UnoplatChapiForgeNode]] = Field( default_factory=dict,description=\"Dict of file paths to their nodes (classes/procedural code)\")    sub_packages: Optional[Dict[str, 'UnoplatPackage']] = Field( default_factory=dict,description=\"Dict of the sub-packages for the package\")"},{"NodeName":"ChapiFunctionFieldModel","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi/chapi_function_field_model.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":8,"StopLine":14},"Content":"class ChapiFunctionFieldModel(BaseModel):    function_variable_name: str = Field(default=None, alias=\"TypeValue\",description=\"function field name\")    function_variable_type: Optional[str] = Field(default=None,alias=\"TypeType\",description=\"function field type hint\")    function_variable_value: Optional[str] = Field(default=None, alias=\"DefaultValue\",description=\"function field value\")        "},{"NodeName":"ChapiNode","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi/chapi_node.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_annotation","UsageName":["ChapiAnnotation"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_class_global_fieldmodel","UsageName":["ClassGlobalFieldModel"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_function","UsageName":["ChapiFunction"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_import","UsageName":["ChapiImport"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_position","UsageName":["Position"]}],"Position":{"StartLine":15,"StopLine":29,"StopLinePosition":4},"Content":"class ChapiNode(BaseModel):    node_name: Optional[str] = Field(default=None, alias=\"NodeName\",description=\"name of the class, method, function, etc.\")    type: Optional[str] = Field(default=None, alias=\"Type\")    file_path: Optional[str] = Field(default=None, alias=\"FilePath\")    module: Optional[str] = Field(default=None, alias=\"Module\")    package: Optional[str] = Field(default=None, alias=\"Package\")    multiple_extend: Optional[list[str]] = Field(default_factory=list, alias=\"MultipleExtend\")    fields: Optional[List[ClassGlobalFieldModel]] = Field(default_factory=list, alias=\"Fields\")    extend: Optional[str] = Field(default=None, alias=\"Extend\")    imports: Optional[List[ChapiImport]] = Field(default_factory=list, alias=\"Imports\")    functions: Optional[List[ChapiFunction]] = Field(default_factory=list, alias=\"Functions\")    position: Optional[Position] = Field(default=None, alias=\"Position\")    content: Optional[str] = Field(default=None, alias=\"Content\")    annotations: Optional[List[ChapiAnnotation]] = Field(default_factory=list, alias=\"Annotations\")    "},{"NodeName":"ChapiAnnotationKeyVal","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi/chapi_annotation_key_val.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":11,"StopLine":13,"StopLinePosition":99},"Content":"class ChapiAnnotationKeyVal(BaseModel):    key: Optional[str] = Field(default=None, alias=\"Key\",description=\"Key of the annotation\")    value: Optional[str] = Field(default=None, alias=\"Value\",description=\"Value of the annotation\")"},{"NodeName":"ChapiFunctionCall","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi/chapi_functioncall.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_parameter","UsageName":["ChapiParameter"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_position","UsageName":["Position"]}],"Position":{"StartLine":16,"StopLine":26,"StopLinePosition":1},"Content":"class ChapiFunctionCall(BaseModel):    \"\"\"    FunctionCall is a data model for a function call being made in a function.    \"\"\"    package: Optional[str] = Field(default=None, alias=\"Package\",description=\"package name of the function call\")    type: Optional[str] = Field(default=None, alias=\"Type\",description=\"type of the function call\")    node_name: Optional[str] = Field(default=None, alias=\"NodeName\",description=\"name of the class being called\")    function_name: Optional[str] = Field(default=None, alias=\"FunctionName\",description=\"name of the function being called\")    parameters: List[ChapiParameter] = Field(default_factory=list, alias=\"Parameters\",description=\"parameters of the function call\")    position: Optional[Position] = Field(default=None, alias=\"Position\",exclude=True,description=\"position of the function call in the code\") "},{"NodeName":"ChapiImport","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi/chapi_import.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":8,"StopLine":13,"StopLinePosition":4},"Content":"class ChapiImport(BaseModel):    source: Optional[str] = Field(default=None, alias=\"Source\")    usage_name: Optional[List[str]] = Field(default_factory=list, alias=\"UsageName\")            "},{"NodeName":"ChapiParameter","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi/chapi_parameter.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":8,"StopLine":11,"StopLinePosition":114},"Content":"class ChapiParameter(BaseModel):    type_value: Optional[str] = Field(default=None,description=\"parameter name\", alias=\"TypeValue\")    type_type: Optional[str] = Field(default=None,description=\"parameter type\", alias=\"TypeType\")    default_value: Optional[str] = Field(default=None,description=\"parameter default value\", alias=\"DefaultValue\")"},{"NodeName":"ChapiFunction","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi/chapi_function.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_annotation","UsageName":["ChapiAnnotation"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_function_field_model","UsageName":["ChapiFunctionFieldModel"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_functioncall","UsageName":["ChapiFunctionCall"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_parameter","UsageName":["ChapiParameter"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_position","UsageName":["Position"]}],"Position":{"StartLine":15,"StopLine":24,"StopLinePosition":65},"Content":"class ChapiFunction(BaseModel):    name: Optional[str] = Field(default=None, alias=\"Name\")    return_type: Optional[str] = Field(default=None, alias=\"ReturnType\")    function_calls: List[ChapiFunctionCall] = Field(default_factory=list, alias=\"FunctionCalls\")    parameters: List[ChapiParameter] = Field(default_factory=list, alias=\"Parameters\",description=\"parameters of the function\")    annotations: List[ChapiAnnotation] = Field(default_factory=list, alias=\"Annotations\")    position: Optional[Position] = Field(default=None, alias=\"Position\")    local_variables: List[ChapiFunctionFieldModel] = Field(default_factory=list, alias=\"LocalVariables\")    body_hash: Optional[int] = Field(default=None, alias=\"BodyHash\")    content: Optional[str] = Field(default=None, alias=\"Content\")"},{"NodeName":"ChapiAnnotation","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi/chapi_annotation.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_annotation_key_val","UsageName":["ChapiAnnotationKeyVal"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_position","UsageName":["Position"]}],"Position":{"StartLine":12,"StopLine":16},"Content":"class ChapiAnnotation(BaseModel):    name: Optional[str] = Field(default=None, alias=\"Name\")    key_values: Optional[list[ChapiAnnotationKeyVal]] = Field(default_factory=list, alias=\"KeyValues\")    position: Optional[Position] = Field(default=None, alias=\"Position\")"},{"NodeName":"Position","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi/chapi_position.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":9,"StopLine":13,"StopLinePosition":85},"Content":"class Position(BaseModel):    start_line: Optional[int] = Field(default=None, alias=\"StartLine\")    start_line_position: Optional[int] = Field(default=None, alias=\"StartLinePosition\")    stop_line: Optional[int] = Field(default=None, alias=\"StopLine\")    stop_line_position: Optional[int] = Field(default=None, alias=\"StopLinePosition\")"},{"NodeName":"ClassGlobalFieldModel","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/data_models/chapi/chapi_class_global_fieldmodel.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_annotation","UsageName":["ChapiAnnotation"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_position","UsageName":["Position"]}],"Position":{"StartLine":12,"StopLine":17,"StopLinePosition":107},"Content":"class ClassGlobalFieldModel(BaseModel):    class_field_name: str = Field(..., alias=\"TypeValue\",description=\"Class Field Name\")    class_field_type: Optional[str] = Field(default=None, alias=\"TypeType\",description=\"Class Field Type\")    class_field_value: Optional[str] = Field(default=None,alias=\"DefaultValue\",description=\"Class Variable Value\")    annotations: Optional[List[ChapiAnnotation]]= Field(default=None, alias=\"Annotations\",description=\"Class Field Annotation\")    position: Optional[Position] = Field(default=None, alias=\"Position\",description=\"Class Field Position\")"},{"NodeName":"Neo4jConnector","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/database/graph/__init__.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"uri","TypeType":"str"},{"TypeValue":"username","TypeType":"str"},{"TypeValue":"password","TypeType":"str"}],"FunctionCalls":[{"NodeName":"GraphDatabase","FunctionName":"driver","Position":{"StartLine":10,"StartLinePosition":35,"StopLine":10,"StopLinePosition":73}}],"Position":{"StartLine":9,"StartLinePosition":4,"StopLine":12,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.driver","TypeType":"GraphDatabase"}],"Content":"def __init__(self, uri: str, username: str, password: str):        self.driver = GraphDatabase.driver(uri, auth=(username, password))    "},{"Name":"close","FunctionCalls":[{"NodeName":"self","FunctionName":"driver","Position":{"StartLine":13,"StartLinePosition":12,"StopLine":13,"StopLinePosition":13}},{"NodeName":"self","FunctionName":"close","Position":{"StartLine":13,"StartLinePosition":19,"StopLine":13,"StopLinePosition":26}}],"Position":{"StartLine":12,"StartLinePosition":4,"StopLine":15,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.driver","TypeType":"GraphDatabase"}],"Content":"def close(self):        self.driver.close()    "},{"Name":"create_node","Parameters":[{"TypeValue":"label","TypeType":"str"},{"TypeValue":"properties","TypeType":"Dict[str,Any]"}],"FunctionCalls":[{"NodeName":"session","FunctionName":"run","Position":{"StartLine":17,"StartLinePosition":19,"StopLine":17,"StopLinePosition":69}}],"Position":{"StartLine":15,"StartLinePosition":4,"StopLine":19,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.driver","TypeType":"GraphDatabase"}],"Content":"def create_node(self, label: str, properties: Dict[str, Any]) -> None:        with self.driver.session() as session:            session.run(f\"MERGE (n:{label} $props)\", props=properties)    "},{"Name":"create_relationship","Parameters":[{"TypeValue":"start_node_label","TypeType":"str"},{"TypeValue":"start_node_property","TypeType":"str"},{"TypeValue":"end_node_label","TypeType":"str"},{"TypeValue":"end_node_property","TypeType":"str"},{"TypeValue":"relationship_type","TypeType":"str"}],"FunctionCalls":[{"NodeName":"session","FunctionName":"run","Position":{"StartLine":28,"StartLinePosition":19,"StopLine":28,"StopLinePosition":91}}],"Position":{"StartLine":19,"StartLinePosition":4,"StopLine":29},"LocalVariables":[{"TypeValue":"self.driver","TypeType":"GraphDatabase"},{"TypeValue":"query","TypeType":"f\"\"\"\n        MATCH (a:{start_node_label}), (b:{end_node_label})\n        WHERE a.{start_node_property} = $start_value AND b.{end_node_property} = $end_value\n        CREATE (a)-[:{relationship_type}]->(b)\n        \"\"\""}],"Content":"def create_relationship(self, start_node_label: str, start_node_property: str,                             end_node_label: str, end_node_property: str,                             relationship_type: str) -> None:        query = f\"\"\"        MATCH (a:{start_node_label}), (b:{end_node_label})        WHERE a.{start_node_property} = $start_value AND b.{end_node_property} = $end_value        CREATE (a)-[:{relationship_type}]->(b)        \"\"\"        with self.driver.session() as session:            session.run(query, start_value=start_node_property, end_value=end_node_property)"}],"Imports":[{"Source":"typing","UsageName":["Any","Dict"]},{"Source":"neo4j","UsageName":["GraphDatabase"]}],"Position":{"StartLine":8,"StopLine":29},"Content":"class Neo4jConnector:    def __init__(self, uri: str, username: str, password: str):        self.driver = GraphDatabase.driver(uri, auth=(username, password))    def close(self):        self.driver.close()    def create_node(self, label: str, properties: Dict[str, Any]) -> None:        with self.driver.session() as session:            session.run(f\"MERGE (n:{label} $props)\", props=properties)    def create_relationship(self, start_node_label: str, start_node_property: str,                             end_node_label: str, end_node_property: str,                             relationship_type: str) -> None:        query = f\"\"\"        MATCH (a:{start_node_label}), (b:{end_node_label})        WHERE a.{start_node_property} = $start_value AND b.{end_node_property} = $end_value        CREATE (a)-[:{relationship_type}]->(b)        \"\"\"        with self.driver.session() as session:            session.run(query, start_value=start_node_property, end_value=end_node_property)"},{"NodeName":"Environment","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/configuration/settings.py","MultipleExtend":["str","Enum"],"Imports":[{"Source":"os"},{"Source":"enum","UsageName":["Enum"]},{"Source":"json"},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","ValidationError"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":12,"StopLine":18},"Content":"class Environment(str, Enum):    DEV = \"dev\"    TEST = \"test\"    PROD = \"prod\"# Enums for type safety"},{"NodeName":"ProgrammingLanguage","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/configuration/settings.py","MultipleExtend":["str","Enum"],"Imports":[{"Source":"os"},{"Source":"enum","UsageName":["Enum"]},{"Source":"json"},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","ValidationError"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":18,"StopLine":21},"Content":"class ProgrammingLanguage(str, Enum):    PYTHON = 'python'"},{"NodeName":"PackageManagerType","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/configuration/settings.py","MultipleExtend":["str","Enum"],"Imports":[{"Source":"os"},{"Source":"enum","UsageName":["Enum"]},{"Source":"json"},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","ValidationError"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":21,"StopLine":25},"Content":"class PackageManagerType(str, Enum):    POETRY = \"poetry\"    PIP = \"pip\""},{"NodeName":"DatabaseType","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/configuration/settings.py","MultipleExtend":["str","Enum"],"Imports":[{"Source":"os"},{"Source":"enum","UsageName":["Enum"]},{"Source":"json"},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","ValidationError"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":25,"StopLine":29},"Content":"class DatabaseType(str, Enum):    NEO4J = \"neo4j\"# Configuration Models (BaseModel for JSON config)"},{"NodeName":"ProgrammingLanguageMetadata","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/configuration/settings.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"os"},{"Source":"enum","UsageName":["Enum"]},{"Source":"json"},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","ValidationError"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":29,"StopLine":34},"Content":"class ProgrammingLanguageMetadata(BaseModel):    language: ProgrammingLanguage    package_manager: PackageManagerType    language_version: Optional[str] = None"},{"NodeName":"CodebaseConfig","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/configuration/settings.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"os"},{"Source":"enum","UsageName":["Enum"]},{"Source":"json"},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","ValidationError"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":34,"StopLine":39},"Content":"class CodebaseConfig(BaseModel):    codebase_folder_name: str    root_package_name: Optional[str] = None    programming_language_metadata: ProgrammingLanguageMetadata"},{"NodeName":"RepositorySettings","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/configuration/settings.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"os"},{"Source":"enum","UsageName":["Enum"]},{"Source":"json"},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","ValidationError"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":39,"StopLine":44},"Content":"class RepositorySettings(BaseModel):    git_url: str    output_path: str    codebases: List[CodebaseConfig]"},{"NodeName":"ArchGuardConfig","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/configuration/settings.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"os"},{"Source":"enum","UsageName":["Enum"]},{"Source":"json"},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","ValidationError"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":44,"StopLine":48},"Content":"class ArchGuardConfig(BaseModel):    download_url: str    download_directory: str"},{"NodeName":"LLMProviderConfig","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/configuration/settings.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"os"},{"Source":"enum","UsageName":["Enum"]},{"Source":"json"},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","ValidationError"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":48,"StopLine":52},"Content":"class LLMProviderConfig(BaseModel):    llm_model_provider: str    llm_model_provider_args: Dict[str, Any]"},{"NodeName":"DatabaseConfig","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/configuration/settings.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"os"},{"Source":"enum","UsageName":["Enum"]},{"Source":"json"},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","ValidationError"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":52,"StopLine":56},"Content":"class DatabaseConfig(BaseModel):    name: DatabaseType    uri: str"},{"NodeName":"DatabaseSettings","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/configuration/settings.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"os"},{"Source":"enum","UsageName":["Enum"]},{"Source":"json"},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","ValidationError"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":56,"StopLine":61},"Content":"class DatabaseSettings(BaseModel):    host: str    port: int# Environment Settings (BaseSettings for environment variables)"},{"NodeName":"EnvironmentSettings","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/configuration/settings.py","MultipleExtend":["BaseSettings"],"Functions":[{"Name":"load","Parameters":[{"TypeValue":"cls","TypeType":""}],"Annotations":[{"Name":"classmethod","Position":{"StartLine":95,"StartLinePosition":4,"StopLine":96,"StopLinePosition":4}}],"Position":{"StartLine":96,"StartLinePosition":4,"StopLine":112},"LocalVariables":[{"TypeValue":"DEV","TypeType":"\"dev\""},{"TypeValue":"TEST","TypeType":"\"test\""},{"TypeValue":"PROD","TypeType":"\"prod\""},{"TypeValue":"PYTHON","TypeType":"'python'"},{"TypeValue":"POETRY","TypeType":"\"poetry\""},{"TypeValue":"PIP","TypeType":"\"pip\""},{"TypeValue":"NEO4J","TypeType":"\"neo4j\""},{"TypeValue":"language","TypeType":""},{"TypeValue":"package_manager","TypeType":""},{"TypeValue":"language_version","TypeType":""},{"TypeValue":"codebase_folder_name","TypeType":""},{"TypeValue":"root_package_name","TypeType":""},{"TypeValue":"programming_language_metadata","TypeType":""},{"TypeValue":"git_url","TypeType":""},{"TypeValue":"output_path","TypeType":""},{"TypeValue":"codebases","TypeType":""},{"TypeValue":"download_url","TypeType":""},{"TypeValue":"download_directory","TypeType":""},{"TypeValue":"llm_model_provider","TypeType":""},{"TypeValue":"llm_model_provider_args","TypeType":""},{"TypeValue":"name","TypeType":""},{"TypeValue":"uri","TypeType":""},{"TypeValue":"host","TypeType":""},{"TypeValue":"port","TypeType":""},{"TypeValue":"model_config","TypeType":"SettingsConfigDict"},{"TypeValue":"env","TypeType":""},{"TypeValue":"debug","TypeType":""},{"TypeValue":"github_token","TypeType":""},{"TypeValue":"llm_api_key","TypeType":""},{"TypeValue":"neo4j_username","TypeType":""},{"TypeValue":"neo4j_password","TypeType":""},{"TypeValue":"env_vars","TypeType":"{key:valueforkey,valueinos.environ.items()ifkey.startswith(\"UNOPLAT_\")}"}],"Content":"def load(cls) -> \"EnvironmentSettings\":        \"\"\"Load environment settings from .env file and environment variables\"\"\"        try:            return cls()        except ValidationError as e:            env_vars = {                key: value for key, value in os.environ.items()                 if key.startswith(\"UNOPLAT_\")            }            raise ValueError(                f\"Failed to load environment settings. \\n\"                f\"Current environment variables: {env_vars}\\n\"                f\"Validation errors: {str(e)}\"            )# Main Configuration (BaseModel for JSON config)"}],"Imports":[{"Source":"os"},{"Source":"enum","UsageName":["Enum"]},{"Source":"json"},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","ValidationError"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":61,"StopLine":112},"Content":"class EnvironmentSettings(BaseSettings):    \"\"\"Environment variables and secrets\"\"\"    model_config = SettingsConfigDict(        env_prefix=\"UNOPLAT_\",        env_file='.env.dev',        env_file_encoding='utf-8',        case_sensitive=True,        extra='ignore',  # Ignore extra fields        populate_by_name=True,        env_parse_none_str=None  # Changed from list to None to fix type error    )    # Environment    env: Environment = Field(default=Environment.DEV)    debug: bool = Field(default=False)    # Secrets - field names should match env vars without prefix    github_token: str = Field(        default=...,        alias=\"GITHUB_TOKEN\"    )    llm_api_key: Optional[str] = Field(        default=None,        alias=\"LLM_API_KEY\"    )    neo4j_username: Optional[str] = Field(        default=None,        alias=\"NEO4J_USERNAME\"    )    neo4j_password: Optional[str] = Field(        default=None,        alias=\"NEO4J_PASSWORD\"    )    @classmethod    def load(cls) -> \"EnvironmentSettings\":        \"\"\"Load environment settings from .env file and environment variables\"\"\"        try:            return cls()        except ValidationError as e:            env_vars = {                key: value for key, value in os.environ.items()                 if key.startswith(\"UNOPLAT_\")            }            raise ValueError(                f\"Failed to load environment settings. \\n\"                f\"Current environment variables: {env_vars}\\n\"                f\"Validation errors: {str(e)}\"            )# Main Configuration (BaseModel for JSON config)"},{"NodeName":"AppConfig","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/configuration/settings.py","MultipleExtend":["BaseModel"],"Functions":[{"Name":"load","Parameters":[{"TypeValue":"cls","TypeType":""},{"DefaultValue":"None","TypeValue":"config_path","TypeType":"Optional[str]"}],"FunctionCalls":[{"NodeName":"json","FunctionName":"loads","Position":{"StartLine":132,"StartLinePosition":30,"StopLine":132,"StopLinePosition":45}}],"Annotations":[{"Name":"classmethod","Position":{"StartLine":122,"StartLinePosition":4,"StopLine":123,"StopLinePosition":4}}],"Position":{"StartLine":123,"StartLinePosition":4,"StopLine":136},"LocalVariables":[{"TypeValue":"DEV","TypeType":"\"dev\""},{"TypeValue":"TEST","TypeType":"\"test\""},{"TypeValue":"PROD","TypeType":"\"prod\""},{"TypeValue":"PYTHON","TypeType":"'python'"},{"TypeValue":"POETRY","TypeType":"\"poetry\""},{"TypeValue":"PIP","TypeType":"\"pip\""},{"TypeValue":"NEO4J","TypeType":"\"neo4j\""},{"TypeValue":"language","TypeType":""},{"TypeValue":"package_manager","TypeType":""},{"TypeValue":"language_version","TypeType":""},{"TypeValue":"codebase_folder_name","TypeType":""},{"TypeValue":"root_package_name","TypeType":""},{"TypeValue":"programming_language_metadata","TypeType":""},{"TypeValue":"git_url","TypeType":""},{"TypeValue":"output_path","TypeType":""},{"TypeValue":"codebases","TypeType":""},{"TypeValue":"download_url","TypeType":""},{"TypeValue":"download_directory","TypeType":""},{"TypeValue":"llm_model_provider","TypeType":""},{"TypeValue":"llm_model_provider_args","TypeType":""},{"TypeValue":"name","TypeType":""},{"TypeValue":"uri","TypeType":""},{"TypeValue":"host","TypeType":""},{"TypeValue":"port","TypeType":""},{"TypeValue":"model_config","TypeType":"SettingsConfigDict"},{"TypeValue":"env","TypeType":"EnvironmentSettings"},{"TypeValue":"debug","TypeType":""},{"TypeValue":"github_token","TypeType":""},{"TypeValue":"llm_api_key","TypeType":""},{"TypeValue":"neo4j_username","TypeType":""},{"TypeValue":"neo4j_password","TypeType":""},{"TypeValue":"env_vars","TypeType":"{key:valueforkey,valueinos.environ.items()ifkey.startswith(\"UNOPLAT_\")}"},{"TypeValue":"repositories","TypeType":""},{"TypeValue":"archguard","TypeType":""},{"TypeValue":"logging_handlers","TypeType":""},{"TypeValue":"llm_provider_config","TypeType":""},{"TypeValue":"databases","TypeType":""},{"TypeValue":"json_output","TypeType":""},{"TypeValue":"sentence_transformer_model","TypeType":""},{"TypeValue":"config_file","TypeType":"f'config.{env}.json'"},{"TypeValue":"config_data","TypeType":"json"}],"Content":"def load(cls, config_path: Optional[str] = None) -> \"AppConfig\":        \"\"\"Load configuration from JSON file\"\"\"        if config_path:            config_file = config_path        else:            env = EnvironmentSettings.load().env            config_file = f'config.{env}.json'        with open(config_file, 'r') as f:            config_data = json.loads(f.read())        return cls(**config_data)# Main Settings Class"}],"Imports":[{"Source":"os"},{"Source":"enum","UsageName":["Enum"]},{"Source":"json"},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","ValidationError"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":112,"StopLine":136},"Content":"class AppConfig(BaseModel):    \"\"\"JSON configuration\"\"\"    repositories: List[RepositorySettings]    archguard: ArchGuardConfig    logging_handlers: List[Dict[str, Any]]    llm_provider_config: Optional[LLMProviderConfig] = None    databases: Optional[List[DatabaseConfig]] = None    json_output: Optional[bool] = None    sentence_transformer_model: Optional[str] = None    @classmethod    def load(cls, config_path: Optional[str] = None) -> \"AppConfig\":        \"\"\"Load configuration from JSON file\"\"\"        if config_path:            config_file = config_path        else:            env = EnvironmentSettings.load().env            config_file = f'config.{env}.json'        with open(config_file, 'r') as f:            config_data = json.loads(f.read())        return cls(**config_data)# Main Settings Class"},{"NodeName":"AppSettings","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/configuration/settings.py","Functions":[{"Name":"__init__","Parameters":[{"DefaultValue":"None","TypeValue":"config_path","TypeType":"Optional[str]"}],"FunctionCalls":[{"NodeName":"AppConfig","FunctionName":"load","Position":{"StartLine":140,"StartLinePosition":32,"StopLine":140,"StopLinePosition":49}}],"Position":{"StartLine":138,"StartLinePosition":4,"StopLine":142,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"DEV","TypeType":"\"dev\""},{"TypeValue":"TEST","TypeType":"\"test\""},{"TypeValue":"PROD","TypeType":"\"prod\""},{"TypeValue":"PYTHON","TypeType":"'python'"},{"TypeValue":"POETRY","TypeType":"\"poetry\""},{"TypeValue":"PIP","TypeType":"\"pip\""},{"TypeValue":"NEO4J","TypeType":"\"neo4j\""},{"TypeValue":"language","TypeType":""},{"TypeValue":"package_manager","TypeType":""},{"TypeValue":"language_version","TypeType":""},{"TypeValue":"codebase_folder_name","TypeType":""},{"TypeValue":"root_package_name","TypeType":""},{"TypeValue":"programming_language_metadata","TypeType":""},{"TypeValue":"git_url","TypeType":""},{"TypeValue":"output_path","TypeType":""},{"TypeValue":"codebases","TypeType":""},{"TypeValue":"download_url","TypeType":""},{"TypeValue":"download_directory","TypeType":""},{"TypeValue":"llm_model_provider","TypeType":""},{"TypeValue":"llm_model_provider_args","TypeType":""},{"TypeValue":"name","TypeType":""},{"TypeValue":"uri","TypeType":""},{"TypeValue":"host","TypeType":""},{"TypeValue":"port","TypeType":""},{"TypeValue":"model_config","TypeType":"SettingsConfigDict"},{"TypeValue":"env","TypeType":"EnvironmentSettings"},{"TypeValue":"debug","TypeType":""},{"TypeValue":"github_token","TypeType":""},{"TypeValue":"llm_api_key","TypeType":""},{"TypeValue":"neo4j_username","TypeType":""},{"TypeValue":"neo4j_password","TypeType":""},{"TypeValue":"env_vars","TypeType":"{key:valueforkey,valueinos.environ.items()ifkey.startswith(\"UNOPLAT_\")}"},{"TypeValue":"repositories","TypeType":""},{"TypeValue":"archguard","TypeType":""},{"TypeValue":"logging_handlers","TypeType":""},{"TypeValue":"llm_provider_config","TypeType":""},{"TypeValue":"databases","TypeType":""},{"TypeValue":"json_output","TypeType":""},{"TypeValue":"sentence_transformer_model","TypeType":""},{"TypeValue":"config_file","TypeType":"f'config.{env}.json'"},{"TypeValue":"config_data","TypeType":"json"},{"TypeValue":"self._env_settings","TypeType":"EnvironmentSettings"},{"TypeValue":"self._config","TypeType":"AppConfig"}],"Content":"def __init__(self, config_path: Optional[str] = None):        self._env_settings = EnvironmentSettings.load()        self._config = AppConfig.load(config_path)    "},{"Name":"env","Annotations":[{"Name":"property","Position":{"StartLine":142,"StartLinePosition":4,"StopLine":143,"StopLinePosition":4}}],"Position":{"StartLine":143,"StartLinePosition":4,"StopLine":146,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"DEV","TypeType":"\"dev\""},{"TypeValue":"TEST","TypeType":"\"test\""},{"TypeValue":"PROD","TypeType":"\"prod\""},{"TypeValue":"PYTHON","TypeType":"'python'"},{"TypeValue":"POETRY","TypeType":"\"poetry\""},{"TypeValue":"PIP","TypeType":"\"pip\""},{"TypeValue":"NEO4J","TypeType":"\"neo4j\""},{"TypeValue":"language","TypeType":""},{"TypeValue":"package_manager","TypeType":""},{"TypeValue":"language_version","TypeType":""},{"TypeValue":"codebase_folder_name","TypeType":""},{"TypeValue":"root_package_name","TypeType":""},{"TypeValue":"programming_language_metadata","TypeType":""},{"TypeValue":"git_url","TypeType":""},{"TypeValue":"output_path","TypeType":""},{"TypeValue":"codebases","TypeType":""},{"TypeValue":"download_url","TypeType":""},{"TypeValue":"download_directory","TypeType":""},{"TypeValue":"llm_model_provider","TypeType":""},{"TypeValue":"llm_model_provider_args","TypeType":""},{"TypeValue":"name","TypeType":""},{"TypeValue":"uri","TypeType":""},{"TypeValue":"host","TypeType":""},{"TypeValue":"port","TypeType":""},{"TypeValue":"model_config","TypeType":"SettingsConfigDict"},{"TypeValue":"env","TypeType":"EnvironmentSettings"},{"TypeValue":"debug","TypeType":""},{"TypeValue":"github_token","TypeType":""},{"TypeValue":"llm_api_key","TypeType":""},{"TypeValue":"neo4j_username","TypeType":""},{"TypeValue":"neo4j_password","TypeType":""},{"TypeValue":"env_vars","TypeType":"{key:valueforkey,valueinos.environ.items()ifkey.startswith(\"UNOPLAT_\")}"},{"TypeValue":"repositories","TypeType":""},{"TypeValue":"archguard","TypeType":""},{"TypeValue":"logging_handlers","TypeType":""},{"TypeValue":"llm_provider_config","TypeType":""},{"TypeValue":"databases","TypeType":""},{"TypeValue":"json_output","TypeType":""},{"TypeValue":"sentence_transformer_model","TypeType":""},{"TypeValue":"config_file","TypeType":"f'config.{env}.json'"},{"TypeValue":"config_data","TypeType":"json"},{"TypeValue":"self._env_settings","TypeType":"EnvironmentSettings"},{"TypeValue":"self._config","TypeType":"AppConfig"}],"Content":"def env(self) -> Environment:        return self._env_settings.env    "},{"Name":"debug","Annotations":[{"Name":"property","Position":{"StartLine":146,"StartLinePosition":4,"StopLine":147,"StopLinePosition":4}}],"Position":{"StartLine":147,"StartLinePosition":4,"StopLine":150,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"DEV","TypeType":"\"dev\""},{"TypeValue":"TEST","TypeType":"\"test\""},{"TypeValue":"PROD","TypeType":"\"prod\""},{"TypeValue":"PYTHON","TypeType":"'python'"},{"TypeValue":"POETRY","TypeType":"\"poetry\""},{"TypeValue":"PIP","TypeType":"\"pip\""},{"TypeValue":"NEO4J","TypeType":"\"neo4j\""},{"TypeValue":"language","TypeType":""},{"TypeValue":"package_manager","TypeType":""},{"TypeValue":"language_version","TypeType":""},{"TypeValue":"codebase_folder_name","TypeType":""},{"TypeValue":"root_package_name","TypeType":""},{"TypeValue":"programming_language_metadata","TypeType":""},{"TypeValue":"git_url","TypeType":""},{"TypeValue":"output_path","TypeType":""},{"TypeValue":"codebases","TypeType":""},{"TypeValue":"download_url","TypeType":""},{"TypeValue":"download_directory","TypeType":""},{"TypeValue":"llm_model_provider","TypeType":""},{"TypeValue":"llm_model_provider_args","TypeType":""},{"TypeValue":"name","TypeType":""},{"TypeValue":"uri","TypeType":""},{"TypeValue":"host","TypeType":""},{"TypeValue":"port","TypeType":""},{"TypeValue":"model_config","TypeType":"SettingsConfigDict"},{"TypeValue":"env","TypeType":"EnvironmentSettings"},{"TypeValue":"debug","TypeType":""},{"TypeValue":"github_token","TypeType":""},{"TypeValue":"llm_api_key","TypeType":""},{"TypeValue":"neo4j_username","TypeType":""},{"TypeValue":"neo4j_password","TypeType":""},{"TypeValue":"env_vars","TypeType":"{key:valueforkey,valueinos.environ.items()ifkey.startswith(\"UNOPLAT_\")}"},{"TypeValue":"repositories","TypeType":""},{"TypeValue":"archguard","TypeType":""},{"TypeValue":"logging_handlers","TypeType":""},{"TypeValue":"llm_provider_config","TypeType":""},{"TypeValue":"databases","TypeType":""},{"TypeValue":"json_output","TypeType":""},{"TypeValue":"sentence_transformer_model","TypeType":""},{"TypeValue":"config_file","TypeType":"f'config.{env}.json'"},{"TypeValue":"config_data","TypeType":"json"},{"TypeValue":"self._env_settings","TypeType":"EnvironmentSettings"},{"TypeValue":"self._config","TypeType":"AppConfig"}],"Content":"def debug(self) -> bool:        return self._env_settings.debug    "},{"Name":"secrets","Annotations":[{"Name":"property","Position":{"StartLine":150,"StartLinePosition":4,"StopLine":151,"StopLinePosition":4}}],"Position":{"StartLine":151,"StartLinePosition":4,"StopLine":154,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"DEV","TypeType":"\"dev\""},{"TypeValue":"TEST","TypeType":"\"test\""},{"TypeValue":"PROD","TypeType":"\"prod\""},{"TypeValue":"PYTHON","TypeType":"'python'"},{"TypeValue":"POETRY","TypeType":"\"poetry\""},{"TypeValue":"PIP","TypeType":"\"pip\""},{"TypeValue":"NEO4J","TypeType":"\"neo4j\""},{"TypeValue":"language","TypeType":""},{"TypeValue":"package_manager","TypeType":""},{"TypeValue":"language_version","TypeType":""},{"TypeValue":"codebase_folder_name","TypeType":""},{"TypeValue":"root_package_name","TypeType":""},{"TypeValue":"programming_language_metadata","TypeType":""},{"TypeValue":"git_url","TypeType":""},{"TypeValue":"output_path","TypeType":""},{"TypeValue":"codebases","TypeType":""},{"TypeValue":"download_url","TypeType":""},{"TypeValue":"download_directory","TypeType":""},{"TypeValue":"llm_model_provider","TypeType":""},{"TypeValue":"llm_model_provider_args","TypeType":""},{"TypeValue":"name","TypeType":""},{"TypeValue":"uri","TypeType":""},{"TypeValue":"host","TypeType":""},{"TypeValue":"port","TypeType":""},{"TypeValue":"model_config","TypeType":"SettingsConfigDict"},{"TypeValue":"env","TypeType":"EnvironmentSettings"},{"TypeValue":"debug","TypeType":""},{"TypeValue":"github_token","TypeType":""},{"TypeValue":"llm_api_key","TypeType":""},{"TypeValue":"neo4j_username","TypeType":""},{"TypeValue":"neo4j_password","TypeType":""},{"TypeValue":"env_vars","TypeType":"{key:valueforkey,valueinos.environ.items()ifkey.startswith(\"UNOPLAT_\")}"},{"TypeValue":"repositories","TypeType":""},{"TypeValue":"archguard","TypeType":""},{"TypeValue":"logging_handlers","TypeType":""},{"TypeValue":"llm_provider_config","TypeType":""},{"TypeValue":"databases","TypeType":""},{"TypeValue":"json_output","TypeType":""},{"TypeValue":"sentence_transformer_model","TypeType":""},{"TypeValue":"config_file","TypeType":"f'config.{env}.json'"},{"TypeValue":"config_data","TypeType":"json"},{"TypeValue":"self._env_settings","TypeType":"EnvironmentSettings"},{"TypeValue":"self._config","TypeType":"AppConfig"}],"Content":"def secrets(self) -> EnvironmentSettings:        return self._env_settings    "},{"Name":"config","Annotations":[{"Name":"property","Position":{"StartLine":154,"StartLinePosition":4,"StopLine":155,"StopLinePosition":4}}],"Position":{"StartLine":155,"StartLinePosition":4,"StopLine":159,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"DEV","TypeType":"\"dev\""},{"TypeValue":"TEST","TypeType":"\"test\""},{"TypeValue":"PROD","TypeType":"\"prod\""},{"TypeValue":"PYTHON","TypeType":"'python'"},{"TypeValue":"POETRY","TypeType":"\"poetry\""},{"TypeValue":"PIP","TypeType":"\"pip\""},{"TypeValue":"NEO4J","TypeType":"\"neo4j\""},{"TypeValue":"language","TypeType":""},{"TypeValue":"package_manager","TypeType":""},{"TypeValue":"language_version","TypeType":""},{"TypeValue":"codebase_folder_name","TypeType":""},{"TypeValue":"root_package_name","TypeType":""},{"TypeValue":"programming_language_metadata","TypeType":""},{"TypeValue":"git_url","TypeType":""},{"TypeValue":"output_path","TypeType":""},{"TypeValue":"codebases","TypeType":""},{"TypeValue":"download_url","TypeType":""},{"TypeValue":"download_directory","TypeType":""},{"TypeValue":"llm_model_provider","TypeType":""},{"TypeValue":"llm_model_provider_args","TypeType":""},{"TypeValue":"name","TypeType":""},{"TypeValue":"uri","TypeType":""},{"TypeValue":"host","TypeType":""},{"TypeValue":"port","TypeType":""},{"TypeValue":"model_config","TypeType":"SettingsConfigDict"},{"TypeValue":"env","TypeType":"EnvironmentSettings"},{"TypeValue":"debug","TypeType":""},{"TypeValue":"github_token","TypeType":""},{"TypeValue":"llm_api_key","TypeType":""},{"TypeValue":"neo4j_username","TypeType":""},{"TypeValue":"neo4j_password","TypeType":""},{"TypeValue":"env_vars","TypeType":"{key:valueforkey,valueinos.environ.items()ifkey.startswith(\"UNOPLAT_\")}"},{"TypeValue":"repositories","TypeType":""},{"TypeValue":"archguard","TypeType":""},{"TypeValue":"logging_handlers","TypeType":""},{"TypeValue":"llm_provider_config","TypeType":""},{"TypeValue":"databases","TypeType":""},{"TypeValue":"json_output","TypeType":""},{"TypeValue":"sentence_transformer_model","TypeType":""},{"TypeValue":"config_file","TypeType":"f'config.{env}.json'"},{"TypeValue":"config_data","TypeType":"json"},{"TypeValue":"self._env_settings","TypeType":"EnvironmentSettings"},{"TypeValue":"self._config","TypeType":"AppConfig"}],"Content":"def config(self) -> AppConfig:        return self._config    # Convenience properties    "},{"Name":"repositories","Annotations":[{"Name":"property","Position":{"StartLine":159,"StartLinePosition":4,"StopLine":160,"StopLinePosition":4}}],"Position":{"StartLine":160,"StartLinePosition":4,"StopLine":163,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"DEV","TypeType":"\"dev\""},{"TypeValue":"TEST","TypeType":"\"test\""},{"TypeValue":"PROD","TypeType":"\"prod\""},{"TypeValue":"PYTHON","TypeType":"'python'"},{"TypeValue":"POETRY","TypeType":"\"poetry\""},{"TypeValue":"PIP","TypeType":"\"pip\""},{"TypeValue":"NEO4J","TypeType":"\"neo4j\""},{"TypeValue":"language","TypeType":""},{"TypeValue":"package_manager","TypeType":""},{"TypeValue":"language_version","TypeType":""},{"TypeValue":"codebase_folder_name","TypeType":""},{"TypeValue":"root_package_name","TypeType":""},{"TypeValue":"programming_language_metadata","TypeType":""},{"TypeValue":"git_url","TypeType":""},{"TypeValue":"output_path","TypeType":""},{"TypeValue":"codebases","TypeType":""},{"TypeValue":"download_url","TypeType":""},{"TypeValue":"download_directory","TypeType":""},{"TypeValue":"llm_model_provider","TypeType":""},{"TypeValue":"llm_model_provider_args","TypeType":""},{"TypeValue":"name","TypeType":""},{"TypeValue":"uri","TypeType":""},{"TypeValue":"host","TypeType":""},{"TypeValue":"port","TypeType":""},{"TypeValue":"model_config","TypeType":"SettingsConfigDict"},{"TypeValue":"env","TypeType":"EnvironmentSettings"},{"TypeValue":"debug","TypeType":""},{"TypeValue":"github_token","TypeType":""},{"TypeValue":"llm_api_key","TypeType":""},{"TypeValue":"neo4j_username","TypeType":""},{"TypeValue":"neo4j_password","TypeType":""},{"TypeValue":"env_vars","TypeType":"{key:valueforkey,valueinos.environ.items()ifkey.startswith(\"UNOPLAT_\")}"},{"TypeValue":"repositories","TypeType":""},{"TypeValue":"archguard","TypeType":""},{"TypeValue":"logging_handlers","TypeType":""},{"TypeValue":"llm_provider_config","TypeType":""},{"TypeValue":"databases","TypeType":""},{"TypeValue":"json_output","TypeType":""},{"TypeValue":"sentence_transformer_model","TypeType":""},{"TypeValue":"config_file","TypeType":"f'config.{env}.json'"},{"TypeValue":"config_data","TypeType":"json"},{"TypeValue":"self._env_settings","TypeType":"EnvironmentSettings"},{"TypeValue":"self._config","TypeType":"AppConfig"}],"Content":"def repositories(self) -> List[RepositorySettings]:        return self.config.repositories    "},{"Name":"databases","Annotations":[{"Name":"property","Position":{"StartLine":163,"StartLinePosition":4,"StopLine":164,"StopLinePosition":4}}],"Position":{"StartLine":164,"StartLinePosition":4,"StopLine":169,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"DEV","TypeType":"\"dev\""},{"TypeValue":"TEST","TypeType":"\"test\""},{"TypeValue":"PROD","TypeType":"\"prod\""},{"TypeValue":"PYTHON","TypeType":"'python'"},{"TypeValue":"POETRY","TypeType":"\"poetry\""},{"TypeValue":"PIP","TypeType":"\"pip\""},{"TypeValue":"NEO4J","TypeType":"\"neo4j\""},{"TypeValue":"language","TypeType":""},{"TypeValue":"package_manager","TypeType":""},{"TypeValue":"language_version","TypeType":""},{"TypeValue":"codebase_folder_name","TypeType":""},{"TypeValue":"root_package_name","TypeType":""},{"TypeValue":"programming_language_metadata","TypeType":""},{"TypeValue":"git_url","TypeType":""},{"TypeValue":"output_path","TypeType":""},{"TypeValue":"codebases","TypeType":""},{"TypeValue":"download_url","TypeType":""},{"TypeValue":"download_directory","TypeType":""},{"TypeValue":"llm_model_provider","TypeType":""},{"TypeValue":"llm_model_provider_args","TypeType":""},{"TypeValue":"name","TypeType":""},{"TypeValue":"uri","TypeType":""},{"TypeValue":"host","TypeType":""},{"TypeValue":"port","TypeType":""},{"TypeValue":"model_config","TypeType":"SettingsConfigDict"},{"TypeValue":"env","TypeType":"EnvironmentSettings"},{"TypeValue":"debug","TypeType":""},{"TypeValue":"github_token","TypeType":""},{"TypeValue":"llm_api_key","TypeType":""},{"TypeValue":"neo4j_username","TypeType":""},{"TypeValue":"neo4j_password","TypeType":""},{"TypeValue":"env_vars","TypeType":"{key:valueforkey,valueinos.environ.items()ifkey.startswith(\"UNOPLAT_\")}"},{"TypeValue":"repositories","TypeType":""},{"TypeValue":"archguard","TypeType":""},{"TypeValue":"logging_handlers","TypeType":""},{"TypeValue":"llm_provider_config","TypeType":""},{"TypeValue":"databases","TypeType":""},{"TypeValue":"json_output","TypeType":""},{"TypeValue":"sentence_transformer_model","TypeType":""},{"TypeValue":"config_file","TypeType":"f'config.{env}.json'"},{"TypeValue":"config_data","TypeType":"json"},{"TypeValue":"self._env_settings","TypeType":"EnvironmentSettings"},{"TypeValue":"self._config","TypeType":"AppConfig"}],"Content":"def databases(self) -> List[DatabaseConfig]:        if self.config.databases is None:            return []  # Return empty list instead of None        return self.config.databases    "},{"Name":"get_settings","Parameters":[{"TypeValue":"cls","TypeType":""},{"DefaultValue":"None","TypeValue":"config_path","TypeType":"Optional[str]"}],"Annotations":[{"Name":"classmethod","Position":{"StartLine":169,"StartLinePosition":4,"StopLine":170,"StopLinePosition":4}}],"Position":{"StartLine":170,"StartLinePosition":4,"StopLine":176,"StopLinePosition":31},"LocalVariables":[{"TypeValue":"DEV","TypeType":"\"dev\""},{"TypeValue":"TEST","TypeType":"\"test\""},{"TypeValue":"PROD","TypeType":"\"prod\""},{"TypeValue":"PYTHON","TypeType":"'python'"},{"TypeValue":"POETRY","TypeType":"\"poetry\""},{"TypeValue":"PIP","TypeType":"\"pip\""},{"TypeValue":"NEO4J","TypeType":"\"neo4j\""},{"TypeValue":"language","TypeType":""},{"TypeValue":"package_manager","TypeType":""},{"TypeValue":"language_version","TypeType":""},{"TypeValue":"codebase_folder_name","TypeType":""},{"TypeValue":"root_package_name","TypeType":""},{"TypeValue":"programming_language_metadata","TypeType":""},{"TypeValue":"git_url","TypeType":""},{"TypeValue":"output_path","TypeType":""},{"TypeValue":"codebases","TypeType":""},{"TypeValue":"download_url","TypeType":""},{"TypeValue":"download_directory","TypeType":""},{"TypeValue":"llm_model_provider","TypeType":""},{"TypeValue":"llm_model_provider_args","TypeType":""},{"TypeValue":"name","TypeType":""},{"TypeValue":"uri","TypeType":""},{"TypeValue":"host","TypeType":""},{"TypeValue":"port","TypeType":""},{"TypeValue":"model_config","TypeType":"SettingsConfigDict"},{"TypeValue":"env","TypeType":"EnvironmentSettings"},{"TypeValue":"debug","TypeType":""},{"TypeValue":"github_token","TypeType":""},{"TypeValue":"llm_api_key","TypeType":""},{"TypeValue":"neo4j_username","TypeType":""},{"TypeValue":"neo4j_password","TypeType":""},{"TypeValue":"env_vars","TypeType":"{key:valueforkey,valueinos.environ.items()ifkey.startswith(\"UNOPLAT_\")}"},{"TypeValue":"repositories","TypeType":""},{"TypeValue":"archguard","TypeType":""},{"TypeValue":"logging_handlers","TypeType":""},{"TypeValue":"llm_provider_config","TypeType":""},{"TypeValue":"databases","TypeType":""},{"TypeValue":"json_output","TypeType":""},{"TypeValue":"sentence_transformer_model","TypeType":""},{"TypeValue":"config_file","TypeType":"f'config.{env}.json'"},{"TypeValue":"config_data","TypeType":"json"},{"TypeValue":"self._env_settings","TypeType":"EnvironmentSettings"},{"TypeValue":"self._config","TypeType":"AppConfig"}],"Content":"def get_settings(cls, config_path: Optional[str] = None) -> \"AppSettings\":        \"\"\"        Get application settings with optional config file override        Args:            config_path: Optional path to JSON config file        \"\"\"        return cls(config_path)"}],"Imports":[{"Source":"os"},{"Source":"enum","UsageName":["Enum"]},{"Source":"json"},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","ValidationError"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":136,"StopLine":176,"StopLinePosition":31},"Content":"class AppSettings:    \"\"\"Application settings combining environment variables and JSON config\"\"\"    def __init__(self, config_path: Optional[str] = None):        self._env_settings = EnvironmentSettings.load()        self._config = AppConfig.load(config_path)    @property    def env(self) -> Environment:        return self._env_settings.env    @property    def debug(self) -> bool:        return self._env_settings.debug    @property    def secrets(self) -> EnvironmentSettings:        return self._env_settings    @property    def config(self) -> AppConfig:        return self._config    # Convenience properties    @property    def repositories(self) -> List[RepositorySettings]:        return self.config.repositories    @property    def databases(self) -> List[DatabaseConfig]:        if self.config.databases is None:            return []  # Return empty list instead of None        return self.config.databases    @classmethod    def get_settings(cls, config_path: Optional[str] = None) -> \"AppSettings\":        \"\"\"        Get application settings with optional config file override        Args:            config_path: Optional path to JSON config file        \"\"\"        return cls(config_path)"},{"NodeName":"GithubHelper","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/confluence_git/github_helper.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"app_settings","TypeType":"AppSettings"}],"Position":{"StartLine":17,"StartLinePosition":4,"StopLine":21,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.settings","TypeType":"app_settings"}],"Content":"def __init__(self,app_settings: AppSettings):        self.settings = app_settings            # works with - vhttps://github.com/organization/repository,https://github.com/organization/repository.git,git@github.com:organization/repository.git    "},{"Name":"clone_repository","Parameters":[{"TypeValue":"repository_config","TypeType":"RepositorySettings"}],"FunctionCalls":[{"NodeName":"codebases","FunctionName":"append","Position":{"StartLine":101,"StartLinePosition":25,"StopLine":101,"StopLinePosition":41}}],"Position":{"StartLine":21,"StartLinePosition":4,"StopLine":116,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.settings","TypeType":"app_settings"},{"TypeValue":"auth","TypeType":"Auth"},{"TypeValue":"github_client","TypeType":"Github"},{"TypeValue":"repo_url","TypeType":"repository_config"},{"TypeValue":"repo_path","TypeType":"os"},{"TypeValue":"repo_name","TypeType":"repo_path"},{"TypeValue":"github_repo","TypeType":"github_client"},{"TypeValue":"local_path","TypeType":"os"},{"TypeValue":"repo_metadata","TypeType":"{\"stars\":github_repo.stargazers_count,\"forks\":github_repo.forks_count,\"default_branch\":github_repo.default_branch,\"created_at\":str(github_repo.created_at),\"updated_at\":str(github_repo.updated_at),\"language\":github_repo.language,}"},{"TypeValue":"readme_content","TypeType":"None"},{"TypeValue":"codebases","TypeType":""},{"TypeValue":"path_components","TypeType":"codebase_config"},{"TypeValue":"programming_language_metadata","TypeType":""},{"TypeValue":"codebase","TypeType":"UnoplatCodebase"}],"Content":"def clone_repository(self,repository_config: RepositorySettings) -> UnoplatGitRepository:        \"\"\"        Clone the repository and return repository details        Works with URL formats:        - https://github.com/organization/repository        - https://github.com/organization/repository.git        - git@github.com:organization/repository.git        \"\"\"                # Initialize Github client with personal access token        auth = Auth.Token(self.settings.secrets.github_token)        github_client = Github(auth=auth)            # Get repository from URL        repo_url = repository_config.git_url                # Extract owner and repo name from different URL formats        if repo_url.startswith('git@'):            # Handle SSH format: git@github.com:org/repo.git            repo_path = repo_url.split('github.com:')[-1]        else:            # Handle HTTPS format: https://github.com/org/repo[.git]            repo_path = repo_url.split('github.com/')[-1]                # Remove .git suffix if present        repo_path = repo_path.replace('.git', '')        repo_name = repo_path.split('/')[-1]                try:            # Get repository object from Github using owner/repo format            github_repo = github_client.get_repo(repo_path)                        # Create local directory if it doesn't exist            local_path = os.path.join(os.path.expanduser(\"~\"), \".unoplat\", \"repositories\")            os.makedirs(local_path, exist_ok=True)            repo_path = os.path.join(local_path, repo_name)                        # Clone repository if not already cloned            if not os.path.exists(repo_path):                repo_metadata: Repo = Repo.clone_from(repo_url, repo_path)                        # Get repository metadata            repo_metadata = {                \"stars\": github_repo.stargazers_count,                \"forks\": github_repo.forks_count,                \"default_branch\": github_repo.default_branch,                \"created_at\": str(github_repo.created_at),                \"updated_at\": str(github_repo.updated_at),                \"language\": github_repo.language,            }                        # Get README content            try:                readme_content = github_repo.get_readme().decoded_content.decode('utf-8')            except:                readme_content = None                            # Create UnoplatCodebase objects for each codebase config            codebases: List[UnoplatCodebase] = []            for codebase_config in repository_config.codebases:                # Split the path and join each component properly                path_components = codebase_config.codebase_folder_name.split('/')                local_path = repo_path                for component in path_components:                    local_path = os.path.join(local_path, component)                                programming_language_metadata: ProgrammingLanguageMetadata = codebase_config.programming_language_metadata                # Verify the path exists                if not os.path.exists(local_path):                    raise Exception(f\"Codebase path not found: {local_path}\")                                codebase = UnoplatCodebase(                    name=codebase_config.root_package_name, #type: ignore                    local_path=local_path,                    package_manager_metadata=UnoplatPackageManagerMetadata(                        programming_language=programming_language_metadata.language.value,                        package_manager=programming_language_metadata.package_manager,                        programming_language_version={'version': programming_language_metadata.language_version} if programming_language_metadata.language_version else None                    )                )                codebases.append(codebase)                        # Create and return UnoplatGitRepository            return UnoplatGitRepository(                repository_url=repo_url,                repository_name=repo_name,                repository_metadata=repo_metadata,                codebases=codebases,                readme=readme_content,                github_organization=github_repo.organization.login if github_repo.organization else None            )                    except Exception as e:            raise Exception(f\"Failed to clone repository: {str(e)}\")            "},{"Name":"close","FunctionCalls":[{"NodeName":"self","FunctionName":"github_client","Position":{"StartLine":120,"StartLinePosition":12,"StopLine":120,"StopLinePosition":13}},{"NodeName":"self","FunctionName":"close","Position":{"StartLine":120,"StartLinePosition":26,"StopLine":120,"StopLinePosition":33}}],"Position":{"StartLine":116,"StartLinePosition":4,"StopLine":126,"StopLinePosition":8},"LocalVariables":[{"TypeValue":"self.settings","TypeType":"app_settings"},{"TypeValue":"auth","TypeType":"Auth"},{"TypeValue":"github_client","TypeType":"Github"},{"TypeValue":"repo_url","TypeType":"repository_config"},{"TypeValue":"repo_path","TypeType":"os"},{"TypeValue":"repo_name","TypeType":"repo_path"},{"TypeValue":"github_repo","TypeType":"github_client"},{"TypeValue":"local_path","TypeType":"os"},{"TypeValue":"repo_metadata","TypeType":"{\"stars\":github_repo.stargazers_count,\"forks\":github_repo.forks_count,\"default_branch\":github_repo.default_branch,\"created_at\":str(github_repo.created_at),\"updated_at\":str(github_repo.updated_at),\"language\":github_repo.language,}"},{"TypeValue":"readme_content","TypeType":"None"},{"TypeValue":"codebases","TypeType":""},{"TypeValue":"path_components","TypeType":"codebase_config"},{"TypeValue":"programming_language_metadata","TypeType":""},{"TypeValue":"codebase","TypeType":"UnoplatCodebase"}],"Content":"def close(self):        \"\"\"        Close the Github client connection        \"\"\"        self.github_client.close()                                                 "}],"Imports":[{"Source":"unoplat_code_confluence.configuration.settings","UsageName":["AppSettings","ProgrammingLanguageMetadata","RepositorySettings"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_codebase","UsageName":["UnoplatCodebase"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_git_repository","UsageName":["UnoplatGitRepository"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"os"},{"Source":"typing","UsageName":["List"]},{"Source":"git","UsageName":["Repo"]},{"Source":"github","UsageName":["Auth","Github"]}],"Position":{"StartLine":16,"StopLine":126,"StopLinePosition":8},"Content":"class GithubHelper:    def __init__(self,app_settings: AppSettings):        self.settings = app_settings            # works with - vhttps://github.com/organization/repository,https://github.com/organization/repository.git,git@github.com:organization/repository.git    def clone_repository(self,repository_config: RepositorySettings) -> UnoplatGitRepository:        \"\"\"        Clone the repository and return repository details        Works with URL formats:        - https://github.com/organization/repository        - https://github.com/organization/repository.git        - git@github.com:organization/repository.git        \"\"\"                # Initialize Github client with personal access token        auth = Auth.Token(self.settings.secrets.github_token)        github_client = Github(auth=auth)            # Get repository from URL        repo_url = repository_config.git_url                # Extract owner and repo name from different URL formats        if repo_url.startswith('git@'):            # Handle SSH format: git@github.com:org/repo.git            repo_path = repo_url.split('github.com:')[-1]        else:            # Handle HTTPS format: https://github.com/org/repo[.git]            repo_path = repo_url.split('github.com/')[-1]                # Remove .git suffix if present        repo_path = repo_path.replace('.git', '')        repo_name = repo_path.split('/')[-1]                try:            # Get repository object from Github using owner/repo format            github_repo = github_client.get_repo(repo_path)                        # Create local directory if it doesn't exist            local_path = os.path.join(os.path.expanduser(\"~\"), \".unoplat\", \"repositories\")            os.makedirs(local_path, exist_ok=True)            repo_path = os.path.join(local_path, repo_name)                        # Clone repository if not already cloned            if not os.path.exists(repo_path):                repo_metadata: Repo = Repo.clone_from(repo_url, repo_path)                        # Get repository metadata            repo_metadata = {                \"stars\": github_repo.stargazers_count,                \"forks\": github_repo.forks_count,                \"default_branch\": github_repo.default_branch,                \"created_at\": str(github_repo.created_at),                \"updated_at\": str(github_repo.updated_at),                \"language\": github_repo.language,            }                        # Get README content            try:                readme_content = github_repo.get_readme().decoded_content.decode('utf-8')            except:                readme_content = None                            # Create UnoplatCodebase objects for each codebase config            codebases: List[UnoplatCodebase] = []            for codebase_config in repository_config.codebases:                # Split the path and join each component properly                path_components = codebase_config.codebase_folder_name.split('/')                local_path = repo_path                for component in path_components:                    local_path = os.path.join(local_path, component)                                programming_language_metadata: ProgrammingLanguageMetadata = codebase_config.programming_language_metadata                # Verify the path exists                if not os.path.exists(local_path):                    raise Exception(f\"Codebase path not found: {local_path}\")                                codebase = UnoplatCodebase(                    name=codebase_config.root_package_name, #type: ignore                    local_path=local_path,                    package_manager_metadata=UnoplatPackageManagerMetadata(                        programming_language=programming_language_metadata.language.value,                        package_manager=programming_language_metadata.package_manager,                        programming_language_version={'version': programming_language_metadata.language_version} if programming_language_metadata.language_version else None                    )                )                codebases.append(codebase)                        # Create and return UnoplatGitRepository            return UnoplatGitRepository(                repository_url=repo_url,                repository_name=repo_name,                repository_metadata=repo_metadata,                codebases=codebases,                readme=readme_content,                github_organization=github_repo.organization.login if github_repo.organization else None            )                    except Exception as e:            raise Exception(f\"Failed to clone repository: {str(e)}\")            def close(self):        \"\"\"        Close the Github client connection        \"\"\"        self.github_client.close()                                                 "},{"NodeName":"UnoplatEmbeddingGenerator","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/embedding/unoplat_embedding_gen.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"config","TypeType":"AppConfig"}],"FunctionCalls":[{"NodeName":"self","FunctionName":"model","Position":{"StartLine":14,"StartLinePosition":30,"StopLine":14,"StopLinePosition":31}},{"NodeName":"self","FunctionName":"get_sentence_embedding_dimension","Position":{"StartLine":14,"StartLinePosition":36,"StopLine":14,"StopLinePosition":70}}],"Position":{"StartLine":12,"StartLinePosition":4,"StopLine":16,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.model","TypeType":"SentenceTransformer"},{"TypeValue":"self.dimensions","TypeType":"self"}],"Content":"def __init__(self, config: AppConfig):        self.model = SentenceTransformer(config.sentence_transformer_model, trust_remote_code=True)        self.dimensions = self.model.get_sentence_embedding_dimension()    "},{"Name":"generate_embeddings","Parameters":[{"TypeValue":"texts","TypeType":"List[str]"}],"Position":{"StartLine":16,"StartLinePosition":4,"StopLine":20,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.model","TypeType":"SentenceTransformer"},{"TypeValue":"self.dimensions","TypeType":"self"},{"TypeValue":"task","TypeType":"'retrieval.query'"}],"Content":"def generate_embeddings(self, texts: List[str]) -> List[List[float]]:        task = 'retrieval.query'        return self.model.encode(texts, task=task).tolist()       "},{"Name":"generate_embeddings_for_single_text","Parameters":[{"TypeValue":"text","TypeType":"str"}],"Position":{"StartLine":20,"StartLinePosition":4,"StopLine":24,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.model","TypeType":"SentenceTransformer"},{"TypeValue":"self.dimensions","TypeType":"self"},{"TypeValue":"task","TypeType":"'retrieval.query'"}],"Content":"def generate_embeddings_for_single_text(self, text: str) -> List[float]:        task = 'retrieval.query'        return self.model.encode(text, task=task).tolist()        "},{"Name":"get_dimensions","Position":{"StartLine":24,"StartLinePosition":4,"StopLine":27},"LocalVariables":[{"TypeValue":"self.model","TypeType":"SentenceTransformer"},{"TypeValue":"self.dimensions","TypeType":"self"},{"TypeValue":"task","TypeType":"'retrieval.query'"}],"Content":"def get_dimensions(self) -> int:        return self.dimensions"}],"Imports":[{"Source":"unoplat_code_confluence.configuration.external_config","UsageName":["AppConfig"]},{"Source":"typing","UsageName":["List"]},{"Source":"sentence_transformers","UsageName":["SentenceTransformer"]}],"Position":{"StartLine":11,"StopLine":27},"Content":"class UnoplatEmbeddingGenerator:    def __init__(self, config: AppConfig):        self.model = SentenceTransformer(config.sentence_transformer_model, trust_remote_code=True)        self.dimensions = self.model.get_sentence_embedding_dimension()    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:        task = 'retrieval.query'        return self.model.encode(texts, task=task).tolist()       def generate_embeddings_for_single_text(self, text: str) -> List[float]:        task = 'retrieval.query'        return self.model.encode(text, task=task).tolist()        def get_dimensions(self) -> int:        return self.dimensions"},{"NodeName":"Downloader","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/downloader/downloader.py","Functions":[{"Name":"get_specific_release_info","Parameters":[{"TypeValue":"repo_name","TypeType":""},{"TypeValue":"release_tag","TypeType":""},{"DefaultValue":"None","TypeValue":"github_token","TypeType":""}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"info","Position":{"StartLine":20,"StartLinePosition":14,"StopLine":20,"StopLinePosition":72}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":14,"StartLinePosition":4,"StopLine":15,"StopLinePosition":4}}],"Position":{"StartLine":15,"StartLinePosition":4,"StopLine":23,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"g","TypeType":"Github"},{"TypeValue":"repo","TypeType":"g"},{"TypeValue":"specific_release","TypeType":"repo"}],"Content":"def get_specific_release_info(repo_name, release_tag, github_token=None):        logger.info(f\"Fetching release info for repository: {repo_name} with tag: {release_tag}\")        g = Github(github_token) if github_token else Github()  # Optionally use a token for higher rate limits: Github(\"your_github_access_token\")        repo = g.get_repo(repo_name)        specific_release = repo.get_release(release_tag)        logger.info(f\"Specific release tag: {specific_release.tag_name}\")        return specific_release.tag_name, specific_release.get_assets()    "},{"Name":"download_file","Parameters":[{"TypeValue":"url","TypeType":""},{"TypeValue":"download_dir","TypeType":""},{"TypeValue":"filename","TypeType":""}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"info","Position":{"StartLine":40,"StartLinePosition":14,"StopLine":40,"StopLinePosition":69}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":23,"StartLinePosition":4,"StopLine":24,"StopLinePosition":4}}],"Position":{"StartLine":24,"StartLinePosition":4,"StopLine":43,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"g","TypeType":"Github"},{"TypeValue":"repo","TypeType":"g"},{"TypeValue":"specific_release","TypeType":"repo"},{"TypeValue":"local_filename","TypeType":"os"},{"TypeValue":"response","TypeType":"requests"},{"TypeValue":"total_size_in_bytes","TypeType":"int"},{"TypeValue":"block_size","TypeType":"1024"},{"TypeValue":"progress_bar","TypeType":"tqdm"}],"Content":"def download_file(url, download_dir, filename):        logger.info(f\"Downloading file from URL: {url} to directory: {download_dir}\")        os.makedirs(download_dir, exist_ok=True)  # Ensure the directory exists        local_filename = os.path.join(download_dir, filename)        response = requests.get(url, stream=True)        total_size_in_bytes = int(response.headers.get('content-length', 0))        block_size = 1024  # 1 Kibibyte        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)        with open(local_filename, 'wb') as file:            for data in response.iter_content(block_size):                progress_bar.update(len(data))                file.write(data)        progress_bar.close()        if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:            logger.error(\"ERROR, something went wrong during file download\")        logger.info(f\"File downloaded successfully: {local_filename}\")        return local_filename    "},{"Name":"download_latest_jar","Parameters":[{"TypeValue":"repo_name","TypeType":""},{"TypeValue":"download_dir","TypeType":""},{"DefaultValue":"None","TypeValue":"github_token","TypeType":""}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"info","Position":{"StartLine":65,"StartLinePosition":14,"StopLine":65,"StopLinePosition":84}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":43,"StartLinePosition":4,"StopLine":44,"StopLinePosition":4}}],"Position":{"StartLine":44,"StartLinePosition":4,"StopLine":66,"StopLinePosition":108},"LocalVariables":[{"TypeValue":"g","TypeType":"Github"},{"TypeValue":"repo","TypeType":"g"},{"TypeValue":"specific_release","TypeType":"repo"},{"TypeValue":"local_filename","TypeType":"os"},{"TypeValue":"response","TypeType":"requests"},{"TypeValue":"total_size_in_bytes","TypeType":"int"},{"TypeValue":"block_size","TypeType":"1024"},{"TypeValue":"progress_bar","TypeType":"tqdm"},{"TypeValue":"","TypeType":"Downloader"},{"TypeValue":"jar_pattern","TypeType":"re"},{"TypeValue":"jar_asset","TypeType":"next"},{"TypeValue":"highest_version_asset_name","TypeType":"jar_asset"},{"TypeValue":"highest_version_asset_url","TypeType":"jar_asset"},{"TypeValue":"existing_jars","TypeType":"[fforfinos.listdir(download_dir)ifjar_pattern.match(f)]"},{"TypeValue":"matching_jar","TypeType":"next"}],"Content":"def download_latest_jar(repo_name, download_dir, github_token=None):        logger.info(f\"Downloading latest JAR for repository: {repo_name}\")        #todo: make this dynamic but there are breaking upstream changes as of now so hardcoding for now.        tag_name, assets = Downloader.get_specific_release_info(repo_name, 'v2.2.8',github_token)        jar_pattern = re.compile(r\"scanner_cli-(.*)-all\\.jar\")  # Regex to match the jar file        jar_asset = next((asset for asset in assets if jar_pattern.match(asset.name)), None)        if not jar_asset:            logger.error(\"No matching .jar file found in the latest release assets.\")            raise FileNotFoundError(\"No matching .jar file found in the latest release assets.\")                highest_version_asset_name = jar_asset.name        highest_version_asset_url = jar_asset.browser_download_url        existing_jars = [f for f in os.listdir(download_dir) if jar_pattern.match(f)]        if existing_jars:            matching_jar = next((jar for jar in existing_jars if tag_name[1:] in jar), None)            if matching_jar:                logger.info(f\"Using existing JAR for version {tag_name}: {matching_jar}\")                return os.path.join(download_dir, matching_jar)                # If no local JAR is higher version, download the latest        logger.info(f\"JAR found: {highest_version_asset_name}, starting download...\")        return Downloader.download_file(highest_version_asset_url, download_dir, highest_version_asset_name)"}],"Imports":[{"Source":"os"},{"Source":"re"},{"Source":"github","UsageName":["Github"]},{"Source":"loguru","UsageName":["logger"]},{"Source":"requests"},{"Source":"tqdm","UsageName":["tqdm"]}],"Position":{"StartLine":13,"StopLine":66,"StopLinePosition":108},"Content":"class Downloader:    @staticmethod    def get_specific_release_info(repo_name, release_tag, github_token=None):        logger.info(f\"Fetching release info for repository: {repo_name} with tag: {release_tag}\")        g = Github(github_token) if github_token else Github()  # Optionally use a token for higher rate limits: Github(\"your_github_access_token\")        repo = g.get_repo(repo_name)        specific_release = repo.get_release(release_tag)        logger.info(f\"Specific release tag: {specific_release.tag_name}\")        return specific_release.tag_name, specific_release.get_assets()    @staticmethod    def download_file(url, download_dir, filename):        logger.info(f\"Downloading file from URL: {url} to directory: {download_dir}\")        os.makedirs(download_dir, exist_ok=True)  # Ensure the directory exists        local_filename = os.path.join(download_dir, filename)        response = requests.get(url, stream=True)        total_size_in_bytes = int(response.headers.get('content-length', 0))        block_size = 1024  # 1 Kibibyte        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)        with open(local_filename, 'wb') as file:            for data in response.iter_content(block_size):                progress_bar.update(len(data))                file.write(data)        progress_bar.close()        if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:            logger.error(\"ERROR, something went wrong during file download\")        logger.info(f\"File downloaded successfully: {local_filename}\")        return local_filename    @staticmethod    def download_latest_jar(repo_name, download_dir, github_token=None):        logger.info(f\"Downloading latest JAR for repository: {repo_name}\")        #todo: make this dynamic but there are breaking upstream changes as of now so hardcoding for now.        tag_name, assets = Downloader.get_specific_release_info(repo_name, 'v2.2.8',github_token)        jar_pattern = re.compile(r\"scanner_cli-(.*)-all\\.jar\")  # Regex to match the jar file        jar_asset = next((asset for asset in assets if jar_pattern.match(asset.name)), None)        if not jar_asset:            logger.error(\"No matching .jar file found in the latest release assets.\")            raise FileNotFoundError(\"No matching .jar file found in the latest release assets.\")                highest_version_asset_name = jar_asset.name        highest_version_asset_url = jar_asset.browser_download_url        existing_jars = [f for f in os.listdir(download_dir) if jar_pattern.match(f)]        if existing_jars:            matching_jar = next((jar for jar in existing_jars if tag_name[1:] in jar), None)            if matching_jar:                logger.info(f\"Using existing JAR for version {tag_name}: {matching_jar}\")                return os.path.join(download_dir, matching_jar)                # If no local JAR is higher version, download the latest        logger.info(f\"JAR found: {highest_version_asset_name}, starting download...\")        return Downloader.download_file(highest_version_asset_url, download_dir, highest_version_asset_name)"},{"NodeName":"PythonPackageNamingStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/python_package_naming_strategy.py","Functions":[{"Name":"get_package_name","Parameters":[{"TypeValue":"file_path","TypeType":"str"},{"TypeValue":"workspace_path","TypeType":"str"}],"FunctionCalls":[{"NodeName":"os","FunctionName":"path","Position":{"StartLine":10,"StartLinePosition":25,"StopLine":10,"StopLinePosition":26}},{"NodeName":"os","FunctionName":"dirname","Position":{"StartLine":10,"StartLinePosition":30,"StopLine":10,"StopLinePosition":52}},{"NodeName":"os","FunctionName":"replace","Position":{"StartLine":10,"StartLinePosition":53,"StopLine":10,"StopLinePosition":78}}],"Position":{"StartLine":8,"StartLinePosition":4,"StopLine":12,"StopLinePosition":27},"LocalVariables":[{"TypeValue":"relative_path","TypeType":"os"},{"TypeValue":"package_name","TypeType":"os"}],"Content":"def get_package_name(self, file_path: str, workspace_path: str) -> str:        relative_path = os.path.relpath(file_path, workspace_path)        package_name = os.path.dirname(relative_path).replace(os.path.sep, '.')                return package_name"}],"Imports":[{"Source":"os"}],"Position":{"StartLine":5,"StopLine":12,"StopLinePosition":27},"Content":"class PythonPackageNamingStrategy:    \"\"\"Python-specific package naming strategy.\"\"\"        def get_package_name(self, file_path: str, workspace_path: str) -> str:        relative_path = os.path.relpath(file_path, workspace_path)        package_name = os.path.dirname(relative_path).replace(os.path.sep, '.')                return package_name"},{"NodeName":"PythonExtractInheritance","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/python_extract_inheritance.py","Functions":[{"Name":"extract_inheritance","Parameters":[{"TypeValue":"node","TypeType":"UnoplatChapiForgeNode"},{"TypeValue":"internal_imports","TypeType":"List[UnoplatImport]"}],"FunctionCalls":[{"NodeName":"internal_imports","FunctionName":"remove","Position":{"StartLine":54,"StartLinePosition":28,"StopLine":54,"StopLinePosition":39}}],"Position":{"StartLine":12,"StartLinePosition":4,"StopLine":56,"StopLinePosition":31},"LocalVariables":[{"TypeValue":"class_map","TypeType":"{}"},{"TypeValue":"source","TypeType":"imp"},{"TypeValue":"class_name","TypeType":"usage"},{"TypeValue":"class_map[class_name]","TypeType":"(source,usage.original_name,imp)"},{"TypeValue":"imports_to_remove","TypeType":"[]"},{"TypeValue":"","TypeType":"class_map"},{"TypeValue":"node.multiple_extend[i]","TypeType":"f\"{source}.{original_name}\""}],"Content":"def extract_inheritance(self, node: UnoplatChapiForgeNode,internal_imports: List[UnoplatImport]) -> List[UnoplatImport]:        \"\"\"Extract inheritance information from a Python node.           we have list of mulitple extends but they have just class names. We need full qualified names to resolve the actual class.           We have internal imports which consists of always absolute paths as source and then usage names are in form of orignal name and alias.           Now what we have to do is match usage name (be it original or alias if alias exists) with name in multiple extend and replace the name with full qualified name with original name(original name always)           Then in house remove the internal import from list of internal import. Do all modifications in place.        Args:            node: The ChapiUnoplatNode to extract inheritance from            internal_imports: The list of unoplat imports                     Returns:            List[UnoplatImport]: Modified list of internal imports with matched imports removed        \"\"\"                        if not node.multiple_extend or not internal_imports:            return internal_imports                    # Create a map of class names to their full paths        class_map = {}        for imp in internal_imports:            source = imp.source            if imp.usage_names:                for usage in imp.usage_names:                    # If there's an alias, use that, otherwise use original name                    class_name = usage.alias if usage.alias else usage.original_name                    class_map[class_name] = (source, usage.original_name, imp)                # Track imports to remove using list        imports_to_remove = []                # Process each inherited class        for i, class_name in enumerate(node.multiple_extend):            if class_name in class_map:                # Replace the class name with its fully qualified name                source, original_name, imp = class_map[class_name]                node.multiple_extend[i] = f\"{source}.{original_name}\"                if imp not in imports_to_remove:  # Avoid duplicates                    imports_to_remove.append(imp)                # Remove matched imports in-place        for imp in imports_to_remove:            internal_imports.remove(imp)                return internal_imports"}],"Imports":[{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_chapi_forge_node","UsageName":["UnoplatChapiForgeNode"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_import","UsageName":["UnoplatImport"]},{"Source":"typing","UsageName":["List"]}],"Position":{"StartLine":9,"StopLine":56,"StopLinePosition":31},"Content":"class PythonExtractInheritance:                def extract_inheritance(self, node: UnoplatChapiForgeNode,internal_imports: List[UnoplatImport]) -> List[UnoplatImport]:        \"\"\"Extract inheritance information from a Python node.           we have list of mulitple extends but they have just class names. We need full qualified names to resolve the actual class.           We have internal imports which consists of always absolute paths as source and then usage names are in form of orignal name and alias.           Now what we have to do is match usage name (be it original or alias if alias exists) with name in multiple extend and replace the name with full qualified name with original name(original name always)           Then in house remove the internal import from list of internal import. Do all modifications in place.        Args:            node: The ChapiUnoplatNode to extract inheritance from            internal_imports: The list of unoplat imports                     Returns:            List[UnoplatImport]: Modified list of internal imports with matched imports removed        \"\"\"                        if not node.multiple_extend or not internal_imports:            return internal_imports                    # Create a map of class names to their full paths        class_map = {}        for imp in internal_imports:            source = imp.source            if imp.usage_names:                for usage in imp.usage_names:                    # If there's an alias, use that, otherwise use original name                    class_name = usage.alias if usage.alias else usage.original_name                    class_map[class_name] = (source, usage.original_name, imp)                # Track imports to remove using list        imports_to_remove = []                # Process each inherited class        for i, class_name in enumerate(node.multiple_extend):            if class_name in class_map:                # Replace the class name with its fully qualified name                source, original_name, imp = class_map[class_name]                node.multiple_extend[i] = f\"{source}.{original_name}\"                if imp not in imports_to_remove:  # Avoid duplicates                    imports_to_remove.append(imp)                # Remove matched imports in-place        for imp in imports_to_remove:            internal_imports.remove(imp)                return internal_imports"},{"NodeName":"PythonImportSegregationStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/python_import_segregation_strategy.py","Functions":[{"Name":"__init__","FunctionCalls":[{"FunctionName":"PythonImportCommentParser","Position":{"StartLine":36,"StartLinePosition":55,"StopLine":36,"StopLinePosition":56}}],"Position":{"StartLine":33,"StartLinePosition":4,"StopLine":38,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.file_reader","TypeType":"ProgrammingFileReader"},{"TypeValue":"self.comment_parser","TypeType":"PythonImportCommentParser"}],"Content":"def __init__(self):        \"\"\"Initialize the import segregation strategy with required components.\"\"\"        self.file_reader = ProgrammingFileReader()        self.comment_parser = PythonImportCommentParser()            "},{"Name":"process_imports","Parameters":[{"TypeValue":"class_metadata","TypeType":"ChapiNode"}],"Position":{"StartLine":38,"StartLinePosition":4,"StopLine":151,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.file_reader","TypeType":"ProgrammingFileReader"},{"TypeValue":"self.comment_parser","TypeType":"PythonImportCommentParser"},{"TypeValue":"file_content","TypeType":""},{"TypeValue":"import_sections","TypeType":""},{"TypeValue":"final_segregated_dict","TypeType":""},{"TypeValue":"imports_list","TypeType":"[]"},{"TypeValue":"parts","TypeType":"import_str"},{"TypeValue":"source","TypeType":"parts"},{"TypeValue":"import_names","TypeType":"' '"},{"TypeValue":"imported_names","TypeType":"[]"},{"TypeValue":"name_parts","TypeType":"name"},{"TypeValue":"final_segregated_dict[section_type]","TypeType":"imports_list"}],"Content":"def process_imports(        self, class_metadata: ChapiNode    ) -> Dict[ImportType, List[UnoplatImport]]:        \"\"\"Process and categorize imports from a Python source file.                Reads a Python source file and processes its imports, categorizing them into        different types (standard library, external, internal, local) based on section        comments. Handles various import patterns including:        - Simple imports: import os        - Aliased imports: import pandas as pd        - From imports: from pathlib import Path        - Multiple imports: from os import path, getcwd        - Aliased from imports: from datetime import datetime as dt        - Multiline imports: from sqlalchemy import (Column, Integer)                Args:            class_metadata: Metadata about the class/file being processed,                          including file path                Returns:            Dictionary mapping ImportType to list of UnoplatImport objects.            Each UnoplatImport contains:            - Source: The module being imported from            - UsageName: List of ImportedName objects containing:                - original_name: The original name being imported                - alias: Optional alias if the import uses 'as'            - ImportType: The category of the import (STANDARD, EXTERNAL, etc.)                    Example:            For input file content:            ```python            # Standard Library            import os            from datetime import datetime as dt                        # Third Party            import pandas as pd                        # First Party            from myproject.utils import helper            ```                        Returns:            {                ImportType.STANDARD: [                    UnoplatImport(Source=\"os\", UsageName=[ImportedName(original_name=\"os\")]),                    UnoplatImport(Source=\"datetime\",                                 UsageName=[ImportedName(original_name=\"datetime\", alias=\"dt\")])                ],                ImportType.EXTERNAL: [                    UnoplatImport(Source=\"pandas\",                                 UsageName=[ImportedName(original_name=\"pandas\", alias=\"pd\")])                ],                ImportType.INTERNAL: [                    UnoplatImport(Source=\"myproject.utils\",                                 UsageName=[ImportedName(original_name=\"helper\")])                ]            }        \"\"\"        file_content: str = self.file_reader.read_file(class_metadata.file_path)                # Parse imports based on comments        import_sections: Dict[ImportType, List[str]] = self.comment_parser.parse_import_sections(            file_content        )                # Convert each section to UnoplatImport format        final_segregated_dict: Dict[ImportType, List[UnoplatImport]] = {}                for section_type, imports in import_sections.items():            if imports:                imports_list = []                for import_str in imports:                    parts = import_str.split()                                        if parts[0] == 'from':                        # Handle 'from module import name1 as n1, name2 as n2, ...' case                        source = parts[1]                        # Join all parts after 'import' and split by commas                        import_names = ' '.join(parts[3:]).split(',')                        # Clean up any whitespace and handle aliases                        imported_names = []                        for name in import_names:                            name_parts = name.strip().split(' as ')                            if len(name_parts) > 1:                                # If there's an alias                                imported_names.append(                                    ImportedName(                                        original_name=name_parts[0].strip(),                                        alias=name_parts[1].strip().rstrip(',')                                    )                                )                            else:                                # If no alias                                imported_names.append(                                    ImportedName(original_name=name_parts[0].strip())                                )                    else:                        # Handle 'import module as m1, module2 as m2' case                        imports_list.extend(self._process_simple_imports(parts, section_type))                                        if parts[0] == 'from':                        imports_list.append(                            UnoplatImport(                                Source=source,                                UsageName=imported_names,                                ImportType=section_type                            )                        )                final_segregated_dict[section_type] = imports_list                return final_segregated_dict    "},{"Name":"_process_simple_imports","Parameters":[{"TypeValue":"parts","TypeType":"List[str]"},{"TypeValue":"section_type","TypeType":"ImportType"}],"FunctionCalls":[{"NodeName":"result","FunctionName":"append","Position":{"StartLine":217,"StartLinePosition":22,"StopLine":225,"StopLinePosition":16}}],"Position":{"StartLine":151,"StartLinePosition":4,"StopLine":228},"LocalVariables":[{"TypeValue":"self.file_reader","TypeType":"ProgrammingFileReader"},{"TypeValue":"self.comment_parser","TypeType":"PythonImportCommentParser"},{"TypeValue":"file_content","TypeType":""},{"TypeValue":"import_sections","TypeType":""},{"TypeValue":"final_segregated_dict","TypeType":""},{"TypeValue":"imports_list","TypeType":"[]"},{"TypeValue":"parts","TypeType":"import_str"},{"TypeValue":"source","TypeType":"parts"},{"TypeValue":"import_names","TypeType":"' '"},{"TypeValue":"imported_names","TypeType":"[]"},{"TypeValue":"name_parts","TypeType":"name"},{"TypeValue":"final_segregated_dict[section_type]","TypeType":"imports_list"},{"TypeValue":"result","TypeType":""},{"TypeValue":"current_module","TypeType":"[]"}],"Content":"def _process_simple_imports(self, parts: List[str], section_type: ImportType) -> List[UnoplatImport]:        \"\"\"Process simple imports like 'import module as alias, module2 as alias2'.                Args:            parts: List of parts from splitting the import string            section_type: Type of import section (STANDARD, EXTERNAL, etc.)                    Returns:            List of UnoplatImport objects        \"\"\"        result: List[UnoplatImport] = []        current_module: List[str] = []                for part in parts[1:]:  # Skip 'import'            if part == 'as':                continue            if part.endswith(','):                # End of current module                current_module.append(part.rstrip(','))                if len(current_module) > 1:                    # Has alias                    result.append(                        UnoplatImport(                            Source=current_module[0],                            UsageName=[                                ImportedName(                                    original_name=current_module[0],                                    alias=current_module[1]                                )                            ],                            ImportType=section_type                        )                    )                else:                    # No alias                    result.append(                        UnoplatImport(                            Source=current_module[0],                            UsageName=[                                ImportedName(original_name=current_module[0])                            ],                            ImportType=section_type                        )                    )                current_module = []            else:                current_module.append(part)                # Handle last module        if current_module:            if len(current_module) > 1:                # Has alias                result.append(                    UnoplatImport(                        Source=current_module[0],                        UsageName=[                            ImportedName(                                original_name=current_module[0],                                alias=current_module[1]                            )                        ],                        ImportType=section_type                    )                )            else:                # No alias                result.append(                    UnoplatImport(                        Source=current_module[0],                        UsageName=[                            ImportedName(original_name=current_module[0])                        ],                        ImportType=section_type                    )                )                return result"}],"Imports":[{"Source":"unoplat_code_confluence.data_models.chapi.chapi_node","UsageName":["ChapiNode"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_import","UsageName":["ImportedName","UnoplatImport"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_import_type","UsageName":["ImportType"]},{"Source":"unoplat_code_confluence.parser.python.utils.python_import_comment_parser","UsageName":["PythonImportCommentParser"]},{"Source":"unoplat_code_confluence.parser.python.utils.read_programming_file","UsageName":["ProgrammingFileReader"]},{"Source":"typing","UsageName":["Dict","List"]}],"Position":{"StartLine":12,"StopLine":228},"Content":"class PythonImportSegregationStrategy:    \"\"\"Strategy for segregating Python imports into different categories based on their types.        This class processes Python source files to extract and categorize imports based on their    types (standard library, external, internal, local). It handles various import patterns    including aliased imports and multiline imports.        Attributes:        file_reader: Reader for accessing Python source files        comment_parser: Parser for extracting imports based on section comments            Note:        Current limitations:        1. Does not preserve inline comments on imports        2. Does not handle comments between imports        3. Only preserves section comments (# Standard Library, # Third Party, etc.)                For handling comments, it's recommended to use tools like isort for import formatting        before processing with this strategy.    \"\"\"        def __init__(self):        \"\"\"Initialize the import segregation strategy with required components.\"\"\"        self.file_reader = ProgrammingFileReader()        self.comment_parser = PythonImportCommentParser()            def process_imports(        self, class_metadata: ChapiNode    ) -> Dict[ImportType, List[UnoplatImport]]:        \"\"\"Process and categorize imports from a Python source file.                Reads a Python source file and processes its imports, categorizing them into        different types (standard library, external, internal, local) based on section        comments. Handles various import patterns including:        - Simple imports: import os        - Aliased imports: import pandas as pd        - From imports: from pathlib import Path        - Multiple imports: from os import path, getcwd        - Aliased from imports: from datetime import datetime as dt        - Multiline imports: from sqlalchemy import (Column, Integer)                Args:            class_metadata: Metadata about the class/file being processed,                          including file path                Returns:            Dictionary mapping ImportType to list of UnoplatImport objects.            Each UnoplatImport contains:            - Source: The module being imported from            - UsageName: List of ImportedName objects containing:                - original_name: The original name being imported                - alias: Optional alias if the import uses 'as'            - ImportType: The category of the import (STANDARD, EXTERNAL, etc.)                    Example:            For input file content:            ```python            # Standard Library            import os            from datetime import datetime as dt                        # Third Party            import pandas as pd                        # First Party            from myproject.utils import helper            ```                        Returns:            {                ImportType.STANDARD: [                    UnoplatImport(Source=\"os\", UsageName=[ImportedName(original_name=\"os\")]),                    UnoplatImport(Source=\"datetime\",                                 UsageName=[ImportedName(original_name=\"datetime\", alias=\"dt\")])                ],                ImportType.EXTERNAL: [                    UnoplatImport(Source=\"pandas\",                                 UsageName=[ImportedName(original_name=\"pandas\", alias=\"pd\")])                ],                ImportType.INTERNAL: [                    UnoplatImport(Source=\"myproject.utils\",                                 UsageName=[ImportedName(original_name=\"helper\")])                ]            }        \"\"\"        file_content: str = self.file_reader.read_file(class_metadata.file_path)                # Parse imports based on comments        import_sections: Dict[ImportType, List[str]] = self.comment_parser.parse_import_sections(            file_content        )                # Convert each section to UnoplatImport format        final_segregated_dict: Dict[ImportType, List[UnoplatImport]] = {}                for section_type, imports in import_sections.items():            if imports:                imports_list = []                for import_str in imports:                    parts = import_str.split()                                        if parts[0] == 'from':                        # Handle 'from module import name1 as n1, name2 as n2, ...' case                        source = parts[1]                        # Join all parts after 'import' and split by commas                        import_names = ' '.join(parts[3:]).split(',')                        # Clean up any whitespace and handle aliases                        imported_names = []                        for name in import_names:                            name_parts = name.strip().split(' as ')                            if len(name_parts) > 1:                                # If there's an alias                                imported_names.append(                                    ImportedName(                                        original_name=name_parts[0].strip(),                                        alias=name_parts[1].strip().rstrip(',')                                    )                                )                            else:                                # If no alias                                imported_names.append(                                    ImportedName(original_name=name_parts[0].strip())                                )                    else:                        # Handle 'import module as m1, module2 as m2' case                        imports_list.extend(self._process_simple_imports(parts, section_type))                                        if parts[0] == 'from':                        imports_list.append(                            UnoplatImport(                                Source=source,                                UsageName=imported_names,                                ImportType=section_type                            )                        )                final_segregated_dict[section_type] = imports_list                return final_segregated_dict    def _process_simple_imports(self, parts: List[str], section_type: ImportType) -> List[UnoplatImport]:        \"\"\"Process simple imports like 'import module as alias, module2 as alias2'.                Args:            parts: List of parts from splitting the import string            section_type: Type of import section (STANDARD, EXTERNAL, etc.)                    Returns:            List of UnoplatImport objects        \"\"\"        result: List[UnoplatImport] = []        current_module: List[str] = []                for part in parts[1:]:  # Skip 'import'            if part == 'as':                continue            if part.endswith(','):                # End of current module                current_module.append(part.rstrip(','))                if len(current_module) > 1:                    # Has alias                    result.append(                        UnoplatImport(                            Source=current_module[0],                            UsageName=[                                ImportedName(                                    original_name=current_module[0],                                    alias=current_module[1]                                )                            ],                            ImportType=section_type                        )                    )                else:                    # No alias                    result.append(                        UnoplatImport(                            Source=current_module[0],                            UsageName=[                                ImportedName(original_name=current_module[0])                            ],                            ImportType=section_type                        )                    )                current_module = []            else:                current_module.append(part)                # Handle last module        if current_module:            if len(current_module) > 1:                # Has alias                result.append(                    UnoplatImport(                        Source=current_module[0],                        UsageName=[                            ImportedName(                                original_name=current_module[0],                                alias=current_module[1]                            )                        ],                        ImportType=section_type                    )                )            else:                # No alias                result.append(                    UnoplatImport(                        Source=current_module[0],                        UsageName=[                            ImportedName(original_name=current_module[0])                        ],                        ImportType=section_type                    )                )                return result"},{"NodeName":"PackageManagerStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/package_manager/package_manager_strategy.py","MultipleExtend":["ABC"],"Functions":[{"Name":"process_metadata","Parameters":[{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"metadata","TypeType":"ProgrammingLanguageMetadata"}],"Annotations":[{"Name":"abstractmethod","Position":{"StartLine":10,"StartLinePosition":4,"StopLine":11,"StopLinePosition":4}}],"Position":{"StartLine":11,"StartLinePosition":4,"StopLine":17},"Content":"def process_metadata(self, local_workspace_path: str, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Process package manager specific metadata\"\"\"        pass"}],"Imports":[{"Source":"unoplat_code_confluence.configuration.settings","UsageName":["ProgrammingLanguageMetadata"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"abc","UsageName":["ABC","abstractmethod"]}],"Position":{"StartLine":9,"StopLine":17},"Content":"class PackageManagerStrategy(ABC):    @abstractmethod    def process_metadata(self, local_workspace_path: str, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Process package manager specific metadata\"\"\"        pass"},{"NodeName":"SetupParser","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/package_manager/utils/setup_parser.py","Functions":[{"Name":"_extract_constant_value","Parameters":[{"TypeValue":"node","TypeType":"ast.AST"}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":16,"StartLinePosition":4,"StopLine":17,"StopLinePosition":4}}],"Position":{"StartLine":17,"StartLinePosition":4,"StopLine":23,"StopLinePosition":4},"Content":"def _extract_constant_value(node: ast.AST) -> Optional[str]:        \"\"\"Extract string value from an AST Constant node\"\"\"        if isinstance(node, ast.Constant) and isinstance(node.value, str):            return node.value        return None    "},{"Name":"_extract_list_values","Parameters":[{"TypeValue":"node","TypeType":"ast.List"}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":23,"StartLinePosition":4,"StopLine":24,"StopLinePosition":4}}],"Position":{"StartLine":24,"StartLinePosition":4,"StopLine":31,"StopLinePosition":4},"Content":"def _extract_list_values(node: ast.List) -> List[str]:        \"\"\"Extract string values from an AST List node\"\"\"        return [            elt.value for elt in node.elts             if isinstance(elt, ast.Constant) and isinstance(elt.value, str)        ]    "},{"Name":"_extract_dict_values","Parameters":[{"TypeValue":"node","TypeType":"ast.Dict"}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":31,"StartLinePosition":4,"StopLine":32,"StopLinePosition":4}}],"Position":{"StartLine":32,"StartLinePosition":4,"StopLine":41,"StopLinePosition":4},"Content":"def _extract_dict_values(node: ast.Dict) -> Dict[str, str]:        \"\"\"Extract key-value pairs from an AST Dict node\"\"\"        return {            key.value: value.value            for key, value in zip(node.keys, node.values)            if isinstance(key, ast.Constant) and isinstance(value, ast.Constant)             and isinstance(key.value, str) and isinstance(value.value, str)        }    "},{"Name":"_extract_setup_args_from_ast","Parameters":[{"TypeValue":"node","TypeType":"ast.AST"}],"FunctionCalls":[{"NodeName":"SetupParser","FunctionName":"_extract_dict_values","Position":{"StartLine":61,"StartLinePosition":52,"StopLine":61,"StopLinePosition":87}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":41,"StartLinePosition":4,"StopLine":42,"StopLinePosition":4}}],"Position":{"StartLine":42,"StartLinePosition":4,"StopLine":65,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"args_dict","TypeType":""},{"TypeValue":"value","TypeType":"SetupParser"},{"TypeValue":"args_dict[keyword.arg]","TypeType":"SetupParser"}],"Content":"def _extract_setup_args_from_ast(node: ast.AST) -> Optional[Dict[str, Any]]:        \"\"\"Extract setup() arguments from an AST node\"\"\"        if not (isinstance(node, ast.Call) and                 isinstance(node.func, ast.Name) and                 node.func.id == 'setup'):            return None        args_dict: Dict[str, Any] = {}                for keyword in node.keywords:            if keyword.arg is None:  # Skip if no argument name                continue            if isinstance(keyword.value, ast.Constant):                value = SetupParser._extract_constant_value(keyword.value)                if value is not None:                    args_dict[keyword.arg] = value            elif isinstance(keyword.value, ast.List):                args_dict[keyword.arg] = SetupParser._extract_list_values(keyword.value)            elif isinstance(keyword.value, ast.Dict):                args_dict[keyword.arg] = SetupParser._extract_dict_values(keyword.value)                        return args_dict    "},{"Name":"_parse_entry_points","Parameters":[{"TypeValue":"entry_points","TypeType":"Union[Dict,str,List]"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"warning","Position":{"StartLine":101,"StartLinePosition":26,"StopLine":101,"StopLinePosition":73}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":65,"StartLinePosition":4,"StopLine":66,"StopLinePosition":4}}],"Position":{"StartLine":66,"StartLinePosition":4,"StopLine":105,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"args_dict","TypeType":""},{"TypeValue":"value","TypeType":"SetupParser"},{"TypeValue":"args_dict[keyword.arg]","TypeType":"SetupParser"},{"TypeValue":"result","TypeType":"{}"},{"TypeValue":"console_scripts","TypeType":"entry_points"},{"TypeValue":"","TypeType":"[part.strip()forpartinentry.split('=',1)]"},{"TypeValue":"result[name]","TypeType":"path"},{"TypeValue":"config","TypeType":"configparser"},{"TypeValue":"entry_points","TypeType":""}],"Content":"def _parse_entry_points(entry_points: Union[Dict, str, List]) -> Dict[str, str]:        \"\"\"Parse entry_points data into a standardized dictionary format\"\"\"        result = {}        if isinstance(entry_points, dict):            console_scripts = entry_points.get('console_scripts', [])            if isinstance(console_scripts, list):                for script in console_scripts:                    try:                        name, path = [part.strip() for part in script.split('=', 1)]                        result[name] = path                    except ValueError:                        logger.warning(f\"Invalid entry point format: {script}\")            elif isinstance(console_scripts, dict):                result.update(console_scripts)        elif isinstance(entry_points, str):            try:                # Standard Library                import configparser                config = configparser.ConfigParser()                if not entry_points.strip().startswith('['):                    entry_points = '[console_scripts]\\n' + entry_points                config.read_string(entry_points)                if 'console_scripts' in config:                    result.update(dict(config['console_scripts']))            except Exception as e:                logger.warning(f\"Failed to parse entry_points string: {str(e)}\")        elif isinstance(entry_points, list):            for entry in entry_points:                try:                    name, path = [part.strip() for part in entry.split('=', 1)]                    result[name] = path                except ValueError:                    logger.warning(f\"Invalid entry point format: {entry}\")        return result    "},{"Name":"_update_metadata_from_setup_args","Parameters":[{"TypeValue":"metadata","TypeType":"UnoplatPackageManagerMetadata"},{"TypeValue":"setup_args","TypeType":"Dict[str,Any]"}],"FunctionCalls":[{"NodeName":"SetupParser","FunctionName":"_parse_entry_points","Position":{"StartLine":139,"StartLinePosition":47,"StopLine":139,"StopLinePosition":94}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":105,"StartLinePosition":4,"StopLine":106,"StopLinePosition":4}}],"Position":{"StartLine":106,"StartLinePosition":4,"StopLine":143,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"args_dict","TypeType":""},{"TypeValue":"value","TypeType":"SetupParser"},{"TypeValue":"args_dict[keyword.arg]","TypeType":"SetupParser"},{"TypeValue":"result","TypeType":"{}"},{"TypeValue":"console_scripts","TypeType":"entry_points"},{"TypeValue":"","TypeType":"[part.strip()forpartinentry.split('=',1)]"},{"TypeValue":"result[name]","TypeType":"path"},{"TypeValue":"config","TypeType":"configparser"},{"TypeValue":"entry_points","TypeType":""},{"TypeValue":"metadata.package_name","TypeType":"setup_args"},{"TypeValue":"metadata.project_version","TypeType":"setup_args"},{"TypeValue":"metadata.description","TypeType":"setup_args"},{"TypeValue":"authors","TypeType":"[setup_args['author']]"},{"TypeValue":"authors[0]","TypeType":"f\"{authors[0]} <{setup_args['author_email']}>\""},{"TypeValue":"metadata.authors","TypeType":"authors"},{"TypeValue":"metadata.license","TypeType":"setup_args"},{"TypeValue":"version_str","TypeType":"setup_args"},{"TypeValue":"version_info","TypeType":"{}"},{"TypeValue":"version_info['min']","TypeType":"version_str"},{"TypeValue":"version_info['max']","TypeType":"version_str"},{"TypeValue":"metadata.programming_language_version","TypeType":"version_info"},{"TypeValue":"metadata.entry_points","TypeType":"SetupParser"}],"Content":"def _update_metadata_from_setup_args(        metadata: UnoplatPackageManagerMetadata,         setup_args: Dict[str, Any]    ) -> UnoplatPackageManagerMetadata:        \"\"\"Update metadata instance with setup arguments\"\"\"        if 'name' in setup_args:            metadata.package_name = setup_args['name']        if 'version' in setup_args:            metadata.project_version = setup_args['version']                    if 'description' in setup_args:            metadata.description = setup_args['description']                    if 'author' in setup_args:            authors = [setup_args['author']]            if 'author_email' in setup_args:                authors[0] = f\"{authors[0]} <{setup_args['author_email']}>\"            metadata.authors = authors                    if 'license' in setup_args:            metadata.license = setup_args['license']                    if 'python_requires' in setup_args:            version_str = setup_args['python_requires']            version_info = {}            if '>=' in version_str:                version_info['min'] = version_str            if '<=' in version_str:                version_info['max'] = version_str            metadata.programming_language_version = version_info                    if 'entry_points' in setup_args:            metadata.entry_points = SetupParser._parse_entry_points(setup_args['entry_points'])        return metadata    "},{"Name":"parse_setup_file","Parameters":[{"TypeValue":"root_dir","TypeType":"str"},{"TypeValue":"metadata","TypeType":"UnoplatPackageManagerMetadata"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":180,"StartLinePosition":18,"StopLine":180,"StopLinePosition":60}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":143,"StartLinePosition":4,"StopLine":144,"StopLinePosition":4}}],"Position":{"StartLine":144,"StartLinePosition":4,"StopLine":182},"LocalVariables":[{"TypeValue":"args_dict","TypeType":""},{"TypeValue":"value","TypeType":"SetupParser"},{"TypeValue":"args_dict[keyword.arg]","TypeType":"SetupParser"},{"TypeValue":"result","TypeType":"{}"},{"TypeValue":"console_scripts","TypeType":"entry_points"},{"TypeValue":"","TypeType":"[part.strip()forpartinentry.split('=',1)]"},{"TypeValue":"result[name]","TypeType":"path"},{"TypeValue":"config","TypeType":"configparser"},{"TypeValue":"entry_points","TypeType":""},{"TypeValue":"metadata.package_name","TypeType":"setup_args"},{"TypeValue":"metadata.project_version","TypeType":"setup_args"},{"TypeValue":"metadata.description","TypeType":"setup_args"},{"TypeValue":"authors","TypeType":"[setup_args['author']]"},{"TypeValue":"authors[0]","TypeType":"f\"{authors[0]} <{setup_args['author_email']}>\""},{"TypeValue":"metadata.authors","TypeType":"authors"},{"TypeValue":"metadata.license","TypeType":"setup_args"},{"TypeValue":"version_str","TypeType":"setup_args"},{"TypeValue":"version_info","TypeType":"{}"},{"TypeValue":"version_info['min']","TypeType":"version_str"},{"TypeValue":"version_info['max']","TypeType":"version_str"},{"TypeValue":"metadata.programming_language_version","TypeType":"version_info"},{"TypeValue":"metadata.entry_points","TypeType":"SetupParser"},{"TypeValue":"setup_file_path","TypeType":"os"},{"TypeValue":"setup_content","TypeType":"file"},{"TypeValue":"tree","TypeType":"ast"},{"TypeValue":"setup_args","TypeType":"args"},{"TypeValue":"args","TypeType":"SetupParser"},{"TypeValue":"metadata","TypeType":"SetupParser"}],"Content":"def parse_setup_file(root_dir: str, metadata: UnoplatPackageManagerMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Parse a setup.py file and update the UnoplatPackageManagerMetadata instance                Args:            root_dir: Path to the directory containing setup.py            metadata: Existing UnoplatPackageManagerMetadata instance to update                    Returns:            Updated UnoplatPackageManagerMetadata instance        \"\"\"        try:            setup_file_path = os.path.join(root_dir, \"setup.py\")                    if not os.path.exists(setup_file_path):                raise FileNotFoundError(f\"setup.py not found at {setup_file_path}\")            with open(setup_file_path, 'r', encoding='utf-8') as file:                setup_content = file.read()                                # Parse the AST            tree = ast.parse(setup_content)                        # Find the setup() call            setup_args = None            for node in ast.walk(tree):                args = SetupParser._extract_setup_args_from_ast(node)                if args:                    setup_args = args                    break                                if setup_args:                metadata = SetupParser._update_metadata_from_setup_args(metadata, setup_args)                            return metadata                    except Exception as e:            logger.error(f\"Error parsing setup.py: {str(e)}\")            return metadata"}],"Imports":[{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"os"},{"Source":"ast"},{"Source":"typing","UsageName":["Any","Dict","List","Optional","Union"]},{"Source":"loguru","UsageName":["logger"]},{"Source":"configparser"}],"Position":{"StartLine":13,"StopLine":182},"Content":"class SetupParser:    \"\"\"Parser for Python setup.py files to extract package metadata\"\"\"        @staticmethod    def _extract_constant_value(node: ast.AST) -> Optional[str]:        \"\"\"Extract string value from an AST Constant node\"\"\"        if isinstance(node, ast.Constant) and isinstance(node.value, str):            return node.value        return None    @staticmethod    def _extract_list_values(node: ast.List) -> List[str]:        \"\"\"Extract string values from an AST List node\"\"\"        return [            elt.value for elt in node.elts             if isinstance(elt, ast.Constant) and isinstance(elt.value, str)        ]    @staticmethod    def _extract_dict_values(node: ast.Dict) -> Dict[str, str]:        \"\"\"Extract key-value pairs from an AST Dict node\"\"\"        return {            key.value: value.value            for key, value in zip(node.keys, node.values)            if isinstance(key, ast.Constant) and isinstance(value, ast.Constant)             and isinstance(key.value, str) and isinstance(value.value, str)        }    @staticmethod    def _extract_setup_args_from_ast(node: ast.AST) -> Optional[Dict[str, Any]]:        \"\"\"Extract setup() arguments from an AST node\"\"\"        if not (isinstance(node, ast.Call) and                 isinstance(node.func, ast.Name) and                 node.func.id == 'setup'):            return None        args_dict: Dict[str, Any] = {}                for keyword in node.keywords:            if keyword.arg is None:  # Skip if no argument name                continue            if isinstance(keyword.value, ast.Constant):                value = SetupParser._extract_constant_value(keyword.value)                if value is not None:                    args_dict[keyword.arg] = value            elif isinstance(keyword.value, ast.List):                args_dict[keyword.arg] = SetupParser._extract_list_values(keyword.value)            elif isinstance(keyword.value, ast.Dict):                args_dict[keyword.arg] = SetupParser._extract_dict_values(keyword.value)                        return args_dict    @staticmethod    def _parse_entry_points(entry_points: Union[Dict, str, List]) -> Dict[str, str]:        \"\"\"Parse entry_points data into a standardized dictionary format\"\"\"        result = {}        if isinstance(entry_points, dict):            console_scripts = entry_points.get('console_scripts', [])            if isinstance(console_scripts, list):                for script in console_scripts:                    try:                        name, path = [part.strip() for part in script.split('=', 1)]                        result[name] = path                    except ValueError:                        logger.warning(f\"Invalid entry point format: {script}\")            elif isinstance(console_scripts, dict):                result.update(console_scripts)        elif isinstance(entry_points, str):            try:                # Standard Library                import configparser                config = configparser.ConfigParser()                if not entry_points.strip().startswith('['):                    entry_points = '[console_scripts]\\n' + entry_points                config.read_string(entry_points)                if 'console_scripts' in config:                    result.update(dict(config['console_scripts']))            except Exception as e:                logger.warning(f\"Failed to parse entry_points string: {str(e)}\")        elif isinstance(entry_points, list):            for entry in entry_points:                try:                    name, path = [part.strip() for part in entry.split('=', 1)]                    result[name] = path                except ValueError:                    logger.warning(f\"Invalid entry point format: {entry}\")        return result    @staticmethod    def _update_metadata_from_setup_args(        metadata: UnoplatPackageManagerMetadata,         setup_args: Dict[str, Any]    ) -> UnoplatPackageManagerMetadata:        \"\"\"Update metadata instance with setup arguments\"\"\"        if 'name' in setup_args:            metadata.package_name = setup_args['name']        if 'version' in setup_args:            metadata.project_version = setup_args['version']                    if 'description' in setup_args:            metadata.description = setup_args['description']                    if 'author' in setup_args:            authors = [setup_args['author']]            if 'author_email' in setup_args:                authors[0] = f\"{authors[0]} <{setup_args['author_email']}>\"            metadata.authors = authors                    if 'license' in setup_args:            metadata.license = setup_args['license']                    if 'python_requires' in setup_args:            version_str = setup_args['python_requires']            version_info = {}            if '>=' in version_str:                version_info['min'] = version_str            if '<=' in version_str:                version_info['max'] = version_str            metadata.programming_language_version = version_info                    if 'entry_points' in setup_args:            metadata.entry_points = SetupParser._parse_entry_points(setup_args['entry_points'])        return metadata    @staticmethod    def parse_setup_file(root_dir: str, metadata: UnoplatPackageManagerMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Parse a setup.py file and update the UnoplatPackageManagerMetadata instance                Args:            root_dir: Path to the directory containing setup.py            metadata: Existing UnoplatPackageManagerMetadata instance to update                    Returns:            Updated UnoplatPackageManagerMetadata instance        \"\"\"        try:            setup_file_path = os.path.join(root_dir, \"setup.py\")                    if not os.path.exists(setup_file_path):                raise FileNotFoundError(f\"setup.py not found at {setup_file_path}\")            with open(setup_file_path, 'r', encoding='utf-8') as file:                setup_content = file.read()                                # Parse the AST            tree = ast.parse(setup_content)                        # Find the setup() call            setup_args = None            for node in ast.walk(tree):                args = SetupParser._extract_setup_args_from_ast(node)                if args:                    setup_args = args                    break                                if setup_args:                metadata = SetupParser._update_metadata_from_setup_args(metadata, setup_args)                            return metadata                    except Exception as e:            logger.error(f\"Error parsing setup.py: {str(e)}\")            return metadata"},{"NodeName":"RequirementsUtils","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/package_manager/utils/requirements_utils.py","Functions":[{"Name":"parse_requirements_folder","Parameters":[{"TypeValue":"workspace_path","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":90,"StartLinePosition":22,"StopLine":90,"StopLinePosition":66}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":31,"StartLinePosition":4,"StopLine":32,"StopLinePosition":4}}],"Position":{"StartLine":32,"StartLinePosition":4,"StopLine":96,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"_SKIP_OPTIONS","TypeType":"{'-c','--constraint','-r','--requirement','--no-binary','--only-binary','--prefer-binary','--require-hashes','--pre','--trusted-host','--use-feature','-Z','--always-unzip'}"},{"TypeValue":"requirements_paths","TypeType":"[]"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"req_folder","TypeType":"os"},{"TypeValue":"default_path","TypeType":"os"},{"TypeValue":"req_txt_path","TypeType":"os"},{"TypeValue":"root_req_txt","TypeType":"os"},{"TypeValue":"tuple_dependency","TypeType":"RequirementsUtils"},{"TypeValue":"dependencies[tuple_dependency[0]]","TypeType":"tuple_dependency"}],"Content":"def parse_requirements_folder(workspace_path: str) -> Dict[str, UnoplatProjectDependency]:        \"\"\"Parse requirements files from the requirements folder.                Args:            workspace_path: Path to project workspace                    Returns:            Dict of parsed dependencies with package name as key                    Note:            Looks for requirements files in this order:            1. requirements/default.txt            2. requirements/requirements.txt              3. requirements/*.txt            4. requirements.txt (in workspace root)        \"\"\"        requirements_paths = []        dependencies: Dict[str, UnoplatProjectDependency] = {}                # Check requirements folder first        req_folder = os.path.join(workspace_path, \"requirements\")        if os.path.exists(req_folder):            # Priority 1: default.txt            default_path = os.path.join(req_folder, \"default.txt\")            if os.path.exists(default_path):                requirements_paths.append(default_path)                            # Priority 2: requirements.txt in requirements folder                req_txt_path = os.path.join(req_folder, \"requirements.txt\")            if os.path.exists(req_txt_path):                requirements_paths.append(req_txt_path)                            # Priority 3: All .txt files in requirements folder            if not requirements_paths:                requirements_paths.extend([                    os.path.join(req_folder, f)                     for f in os.listdir(req_folder)                     if f.endswith('.txt')                ])                # Priority 4: requirements.txt in workspace root        root_req_txt = os.path.join(workspace_path, \"requirements.txt\")        if os.path.exists(root_req_txt):            requirements_paths.append(root_req_txt)                    if not requirements_paths:            logger.warning(f\"No requirements files found in {workspace_path}\")            return {}                    # Parse all found requirement files using requirements-parser        for req_file in requirements_paths:            try:                with open(req_file, 'r') as f:                    for req in requirements.parse(f):                        tuple_dependency = RequirementsUtils._convert_requirement_to_dependency(req)                        if tuple_dependency:                            dependencies[tuple_dependency[0]] = tuple_dependency[1]            except Exception as e:                logger.error(f\"Error parsing {req_file}: {str(e)}\")                        return dependencies                   "},{"Name":"_convert_requirement_to_dependency","Parameters":[{"TypeValue":"req","TypeType":"Requirement"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":173,"StartLinePosition":18,"StopLine":173,"StopLinePosition":77}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":96,"StartLinePosition":4,"StopLine":97,"StopLinePosition":4}}],"Position":{"StartLine":97,"StartLinePosition":4,"StopLine":174,"StopLinePosition":23},"LocalVariables":[{"TypeValue":"_SKIP_OPTIONS","TypeType":"{'-c','--constraint','-r','--requirement','--no-binary','--only-binary','--prefer-binary','--require-hashes','--pre','--trusted-host','--use-feature','-Z','--always-unzip'}"},{"TypeValue":"requirements_paths","TypeType":"[]"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"req_folder","TypeType":"os"},{"TypeValue":"default_path","TypeType":"os"},{"TypeValue":"req_txt_path","TypeType":"os"},{"TypeValue":"root_req_txt","TypeType":"os"},{"TypeValue":"tuple_dependency","TypeType":"(name,UnoplatProjectDependency(version=version,extras=sorted(req.extras)ifreq.extraselseNone,source=source,source_url=source_url,source_reference=req.revisionifreq.vcselseNone,subdirectory=req.subdirectoryifhasattr(req,'subdirectory')elseNone,hash_info=hash_info))"},{"TypeValue":"dependencies[tuple_dependency[0]]","TypeType":"tuple_dependency"},{"TypeValue":"name","TypeType":"req"},{"TypeValue":"version","TypeType":"UnoplatVersion"},{"TypeValue":"sorted_specs","TypeType":"sorted"},{"TypeValue":"version.minimum_version","TypeType":"f\">={ver}\""},{"TypeValue":"version.maximum_version","TypeType":"f\"<{'.'.join(parts)}\""},{"TypeValue":"version.current_version","TypeType":"f\"=={ver}\""},{"TypeValue":"parts","TypeType":"ver"},{"TypeValue":"parts[0]","TypeType":"str"},{"TypeValue":"parts[-2]","TypeType":"str"},{"TypeValue":"parts[-1]","TypeType":"'0'"},{"TypeValue":"source","TypeType":"\"path\""},{"TypeValue":"source_url","TypeType":"req"},{"TypeValue":"hash_info","TypeType":"f\"{req.hash_name}:{req.hash}\""}],"Content":"def _convert_requirement_to_dependency(req: Requirement) -> Optional[Tuple[str, UnoplatProjectDependency]]:        \"\"\"Convert requirements-parser Requirement to UnoplatProjectDependency.\"\"\"        try:            # Get package name, handling both VCS and regular requirements            name = req.name            if not name and req.uri:                # Try to get name from URI fragment                if hasattr(req, 'fragment') and req.fragment:                    name = req.fragment.get('egg')                        if not name:                logger.warning(f\"Could not determine package name from: {req.line}\")                return None            # Handle version specs            version = UnoplatVersion()            if req.specs:                # Sort specs for consistent output                sorted_specs = sorted(req.specs, key=lambda x: (x[0], x[1]))                                # Parse version constraints into min/max versions                for op, ver in sorted_specs:                    if op == '>=':                        version.minimum_version = f\">={ver}\"                    elif op == '>':                        version.minimum_version = f\">{ver}\"                    elif op == '<=':                        version.maximum_version = f\"<={ver}\"                    elif op == '<':                        version.maximum_version = f\"<{ver}\"                    elif op == '==':                        version.current_version = f\"=={ver}\"                    elif op == '~=':  # Compatible release operator                        version.minimum_version = f\">={ver}\"                        # Handle compatible release based on version components                        parts = ver.split('.')                        if len(parts) >= 2:                            if len(parts) == 2:                                # For X.Y format, allow up to next major                                parts[0] = str(int(parts[0]) + 1)                                version.maximum_version = f\"<{'.'.join(parts)}\"                            else:                                # For X.Y.Z... format, allow up to next minor                                parts[-2] = str(int(parts[-2]) + 1)                                parts[-1] = '0'                                version.maximum_version = f\"<{'.'.join(parts)}\"                        # Determine source info            source = None            source_url = None            if req.vcs:                source = req.vcs                source_url = req.uri            elif req.uri:                source = \"url\"                 source_url = req.uri            elif req.local_file:                source = \"path\"                source_url = req.path            # Get hash info if present            hash_info = None            if hasattr(req, 'hash_name') and req.hash_name:                hash_info = f\"{req.hash_name}:{req.hash}\"            tuple_dependency = (name, UnoplatProjectDependency(                version=version,                extras=sorted(req.extras) if req.extras else None,                source=source,                source_url=source_url,                source_reference=req.revision if req.vcs else None,                subdirectory=req.subdirectory if hasattr(req, 'subdirectory') else None,                hash_info=hash_info            ))            return tuple_dependency        except Exception as e:            logger.error(f\"Error converting requirement {req.line}: {str(e)}\")            return None"}],"Imports":[{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_project_dependency","UsageName":["UnoplatProjectDependency"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_version","UsageName":["UnoplatVersion"]},{"Source":"os"},{"Source":"typing","UsageName":["Dict","Optional","Tuple"]},{"Source":"loguru","UsageName":["logger"]},{"Source":"requirements"},{"Source":"requirements.requirement","UsageName":["Requirement"]}],"Position":{"StartLine":16,"StopLine":174,"StopLinePosition":23},"Content":"class RequirementsUtils:    \"\"\"Utility class for parsing requirements files using requirements-parser library.\"\"\"        # Add constants for unsupported options that we want to skip    _SKIP_OPTIONS = {        '-c', '--constraint',  # Constraint files        '-r', '--requirement', # Recursive requirements         '--no-binary', '--only-binary', '--prefer-binary',        '--require-hashes',        '--pre',        '--trusted-host',        '--use-feature',        '-Z', '--always-unzip'    }        @staticmethod    def parse_requirements_folder(workspace_path: str) -> Dict[str, UnoplatProjectDependency]:        \"\"\"Parse requirements files from the requirements folder.                Args:            workspace_path: Path to project workspace                    Returns:            Dict of parsed dependencies with package name as key                    Note:            Looks for requirements files in this order:            1. requirements/default.txt            2. requirements/requirements.txt              3. requirements/*.txt            4. requirements.txt (in workspace root)        \"\"\"        requirements_paths = []        dependencies: Dict[str, UnoplatProjectDependency] = {}                # Check requirements folder first        req_folder = os.path.join(workspace_path, \"requirements\")        if os.path.exists(req_folder):            # Priority 1: default.txt            default_path = os.path.join(req_folder, \"default.txt\")            if os.path.exists(default_path):                requirements_paths.append(default_path)                            # Priority 2: requirements.txt in requirements folder                req_txt_path = os.path.join(req_folder, \"requirements.txt\")            if os.path.exists(req_txt_path):                requirements_paths.append(req_txt_path)                            # Priority 3: All .txt files in requirements folder            if not requirements_paths:                requirements_paths.extend([                    os.path.join(req_folder, f)                     for f in os.listdir(req_folder)                     if f.endswith('.txt')                ])                # Priority 4: requirements.txt in workspace root        root_req_txt = os.path.join(workspace_path, \"requirements.txt\")        if os.path.exists(root_req_txt):            requirements_paths.append(root_req_txt)                    if not requirements_paths:            logger.warning(f\"No requirements files found in {workspace_path}\")            return {}                    # Parse all found requirement files using requirements-parser        for req_file in requirements_paths:            try:                with open(req_file, 'r') as f:                    for req in requirements.parse(f):                        tuple_dependency = RequirementsUtils._convert_requirement_to_dependency(req)                        if tuple_dependency:                            dependencies[tuple_dependency[0]] = tuple_dependency[1]            except Exception as e:                logger.error(f\"Error parsing {req_file}: {str(e)}\")                        return dependencies                   @staticmethod    def _convert_requirement_to_dependency(req: Requirement) -> Optional[Tuple[str, UnoplatProjectDependency]]:        \"\"\"Convert requirements-parser Requirement to UnoplatProjectDependency.\"\"\"        try:            # Get package name, handling both VCS and regular requirements            name = req.name            if not name and req.uri:                # Try to get name from URI fragment                if hasattr(req, 'fragment') and req.fragment:                    name = req.fragment.get('egg')                        if not name:                logger.warning(f\"Could not determine package name from: {req.line}\")                return None            # Handle version specs            version = UnoplatVersion()            if req.specs:                # Sort specs for consistent output                sorted_specs = sorted(req.specs, key=lambda x: (x[0], x[1]))                                # Parse version constraints into min/max versions                for op, ver in sorted_specs:                    if op == '>=':                        version.minimum_version = f\">={ver}\"                    elif op == '>':                        version.minimum_version = f\">{ver}\"                    elif op == '<=':                        version.maximum_version = f\"<={ver}\"                    elif op == '<':                        version.maximum_version = f\"<{ver}\"                    elif op == '==':                        version.current_version = f\"=={ver}\"                    elif op == '~=':  # Compatible release operator                        version.minimum_version = f\">={ver}\"                        # Handle compatible release based on version components                        parts = ver.split('.')                        if len(parts) >= 2:                            if len(parts) == 2:                                # For X.Y format, allow up to next major                                parts[0] = str(int(parts[0]) + 1)                                version.maximum_version = f\"<{'.'.join(parts)}\"                            else:                                # For X.Y.Z... format, allow up to next minor                                parts[-2] = str(int(parts[-2]) + 1)                                parts[-1] = '0'                                version.maximum_version = f\"<{'.'.join(parts)}\"                        # Determine source info            source = None            source_url = None            if req.vcs:                source = req.vcs                source_url = req.uri            elif req.uri:                source = \"url\"                 source_url = req.uri            elif req.local_file:                source = \"path\"                source_url = req.path            # Get hash info if present            hash_info = None            if hasattr(req, 'hash_name') and req.hash_name:                hash_info = f\"{req.hash_name}:{req.hash}\"            tuple_dependency = (name, UnoplatProjectDependency(                version=version,                extras=sorted(req.extras) if req.extras else None,                source=source,                source_url=source_url,                source_reference=req.revision if req.vcs else None,                subdirectory=req.subdirectory if hasattr(req, 'subdirectory') else None,                hash_info=hash_info            ))            return tuple_dependency        except Exception as e:            logger.error(f\"Error converting requirement {req.line}: {str(e)}\")            return None"},{"NodeName":"PipStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/package_manager/pip/pip_strategy.py","MultipleExtend":["PackageManagerStrategy"],"Functions":[{"Name":"process_metadata","Parameters":[{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"metadata","TypeType":"ProgrammingLanguageMetadata"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":57,"StartLinePosition":18,"StopLine":57,"StopLinePosition":67}}],"Position":{"StartLine":18,"StartLinePosition":4,"StopLine":63,"StopLinePosition":13},"LocalVariables":[{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"package_metadata","TypeType":"SetupParser"},{"TypeValue":"package_metadata.programming_language_version","TypeType":"metadata"}],"Content":"def process_metadata(self, local_workspace_path: str, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Process pip specific metadata from requirements files and setup.py.                Args:            local_workspace_path: Path to the project workspace            metadata: Programming language metadata                    Returns:            UnoplatPackageManagerMetadata containing parsed project information        \"\"\"        try:            # First parse requirements to get dependencies            dependencies: Dict[str, UnoplatProjectDependency] = RequirementsUtils.parse_requirements_folder(local_workspace_path)                        # Create initial metadata with dependencies            package_metadata = UnoplatPackageManagerMetadata(                dependencies=dependencies,                programming_language=metadata.language.value,                package_manager=metadata.package_manager            )                        try:                # Try to parse setup.py for additional metadata                package_metadata = SetupParser.parse_setup_file(                    local_workspace_path,                     package_metadata                )                # if we do not get python version fall back to the one taken from configuration                           if package_metadata.programming_language_version is None:                    package_metadata.programming_language_version = metadata.language_version # type: ignore                                except FileNotFoundError:                logger.warning(\"setup.py not found, using only requirements data\")            except Exception as e:                logger.error(f\"Error parsing setup.py: {str(e)}\")                            return package_metadata                    except Exception as e:            logger.error(f\"Error processing pip metadata: {str(e)}\")            # Return basic metadata if parsing fails            return UnoplatPackageManagerMetadata(                dependencies={},                programming_language=metadata.language.value,                package_manager=metadata.package_manager            )"}],"Imports":[{"Source":"unoplat_code_confluence.configuration.settings","UsageName":["ProgrammingLanguageMetadata"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_project_dependency","UsageName":["UnoplatProjectDependency"]},{"Source":"unoplat_code_confluence.parser.python.package_manager.package_manager_strategy","UsageName":["PackageManagerStrategy"]},{"Source":"unoplat_code_confluence.parser.python.package_manager.utils.requirements_utils","UsageName":["RequirementsUtils"]},{"Source":"unoplat_code_confluence.parser.python.package_manager.utils.setup_parser","UsageName":["SetupParser"]},{"Source":"typing","UsageName":["Dict"]},{"Source":"loguru","UsageName":["logger"]}],"Position":{"StartLine":17,"StopLine":63,"StopLinePosition":13},"Content":"class PipStrategy(PackageManagerStrategy):    def process_metadata(self, local_workspace_path: str, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Process pip specific metadata from requirements files and setup.py.                Args:            local_workspace_path: Path to the project workspace            metadata: Programming language metadata                    Returns:            UnoplatPackageManagerMetadata containing parsed project information        \"\"\"        try:            # First parse requirements to get dependencies            dependencies: Dict[str, UnoplatProjectDependency] = RequirementsUtils.parse_requirements_folder(local_workspace_path)                        # Create initial metadata with dependencies            package_metadata = UnoplatPackageManagerMetadata(                dependencies=dependencies,                programming_language=metadata.language.value,                package_manager=metadata.package_manager            )                        try:                # Try to parse setup.py for additional metadata                package_metadata = SetupParser.parse_setup_file(                    local_workspace_path,                     package_metadata                )                # if we do not get python version fall back to the one taken from configuration                           if package_metadata.programming_language_version is None:                    package_metadata.programming_language_version = metadata.language_version # type: ignore                                except FileNotFoundError:                logger.warning(\"setup.py not found, using only requirements data\")            except Exception as e:                logger.error(f\"Error parsing setup.py: {str(e)}\")                            return package_metadata                    except Exception as e:            logger.error(f\"Error processing pip metadata: {str(e)}\")            # Return basic metadata if parsing fails            return UnoplatPackageManagerMetadata(                dependencies={},                programming_language=metadata.language.value,                package_manager=metadata.package_manager            )"},{"NodeName":"PythonPoetryStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/package_manager/poetry/poetry_strategy.py","MultipleExtend":["PackageManagerStrategy"],"Functions":[{"Name":"process_metadata","Parameters":[{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"metadata","TypeType":"ProgrammingLanguageMetadata"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":71,"StartLinePosition":18,"StopLine":71,"StopLinePosition":66}}],"Position":{"StartLine":21,"StartLinePosition":4,"StopLine":74,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":"os"},{"TypeValue":"pyproject_data","TypeType":"tomlkit"},{"TypeValue":"poetry_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"unoplatPackageManager","TypeType":"SetupParser"},{"TypeValue":"main_deps","TypeType":"poetry_data"},{"TypeValue":"programming_language_version","TypeType":"metadata"}],"Content":"def process_metadata(self, local_workspace_path: str, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Process poetry specific metadata from pyproject.toml\"\"\"        pyproject_path = os.path.join(local_workspace_path, \"pyproject.toml\")                if not os.path.exists(pyproject_path):            logger.warning(f\"pyproject.toml not found at {pyproject_path}\")            return self._create_empty_metadata(metadata)                    try:            with open(pyproject_path, \"r\") as f:                pyproject_data = tomlkit.parse(f.read())                        poetry_data = pyproject_data.get(\"tool\", {}).get(\"poetry\", {})            if not poetry_data:                logger.warning(\"No poetry configuration found in pyproject.toml, falling back to requirements\")                # Try parsing requirements folder using RequirementsUtils                dependencies: Dict[str, UnoplatProjectDependency] = RequirementsUtils.parse_requirements_folder(local_workspace_path)                unoplatPackageManager: UnoplatPackageManagerMetadata =  UnoplatPackageManagerMetadata(                    dependencies=dependencies,                    programming_language=metadata.language.value,                    package_manager=PackageManagerType.PIP.value                )                try:                    unoplatPackageManager = SetupParser.parse_setup_file(local_workspace_path, unoplatPackageManager)                except FileNotFoundError:                    logger.warning(\"setup.py not found, skipping setup.py parsing\")                return unoplatPackageManager                                            # Parse only main dependencies            main_deps = poetry_data.get(\"dependencies\", {})            dependencies: Dict[str, UnoplatProjectDependency] = self._parse_dependencies(main_deps) #type: ignore            programming_language_version=self._parse_python_version(main_deps.get(\"python\"))            # if we do not get python version fall back to the one taken from configuration                       if programming_language_version is None:                programming_language_version = metadata.language_version            # Create metadata object with entry_points instead of entry_point            return UnoplatPackageManagerMetadata(                dependencies=dependencies,                package_name=poetry_data.get(\"name\"),                programming_language=metadata.language.value,                package_manager=metadata.package_manager,                programming_language_version=programming_language_version,                project_version=poetry_data.get(\"version\"),                description=poetry_data.get(\"description\"),                authors=poetry_data.get(\"authors\"),                entry_points=self._get_entry_points(poetry_data.get(\"scripts\", {}))            )                    except Exception as e:            logger.error(f\"Error parsing pyproject.toml: {str(e)}\")            return self._create_empty_metadata(metadata)        "},{"Name":"_create_empty_metadata","Parameters":[{"TypeValue":"metadata","TypeType":"ProgrammingLanguageMetadata"}],"Position":{"StartLine":74,"StartLinePosition":4,"StopLine":82,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":"os"},{"TypeValue":"pyproject_data","TypeType":"tomlkit"},{"TypeValue":"poetry_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"unoplatPackageManager","TypeType":"SetupParser"},{"TypeValue":"main_deps","TypeType":"poetry_data"},{"TypeValue":"programming_language_version","TypeType":"metadata"}],"Content":"def _create_empty_metadata(self, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Create empty metadata with basic information\"\"\"        return UnoplatPackageManagerMetadata(            dependencies={},            programming_language=metadata.language.value,            package_manager=metadata.package_manager        )        "},{"Name":"_parse_version_constraint","Parameters":[{"TypeValue":"constraint","TypeType":"str"}],"Position":{"StartLine":82,"StartLinePosition":4,"StopLine":131,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":"os"},{"TypeValue":"pyproject_data","TypeType":"tomlkit"},{"TypeValue":"poetry_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"unoplatPackageManager","TypeType":"SetupParser"},{"TypeValue":"main_deps","TypeType":"poetry_data"},{"TypeValue":"programming_language_version","TypeType":"metadata"},{"TypeValue":"parts","TypeType":"[p.strip()forpinconstraint.split(\",\")]"},{"TypeValue":"min_ver","TypeType":"part"},{"TypeValue":"max_ver","TypeType":"part"},{"TypeValue":"current_ver","TypeType":"part"}],"Content":"def _parse_version_constraint(self, constraint: str) -> UnoplatVersion:        \"\"\"Parse version constraint into UnoplatVersion format.\"\"\"        if not constraint or constraint == \"*\":            return UnoplatVersion()                    # Handle comma-separated constraints (e.g., \">=1.0.0,<2.0.0\")        if \",\" in constraint:            parts = [p.strip() for p in constraint.split(\",\")]            min_ver = None            max_ver = None            current_ver = None                        for part in parts:                if part.startswith(\">=\") or part.startswith(\">\"):                    min_ver = part                elif part.startswith(\"<=\") or part.startswith(\"<\"):                    max_ver = part                elif part.startswith(\"==\"):                    current_ver = part                                return UnoplatVersion(                minimum_version=min_ver,                maximum_version=max_ver,                current_version=current_ver            )                # Handle caret constraints (e.g., \"^1.0.0\")        if constraint.startswith(\"^\"):            return UnoplatVersion(                minimum_version=constraint            )                # Handle tilde constraints (e.g., \"~1.0.0\")        if constraint.startswith(\"~\"):            return UnoplatVersion(                minimum_version=constraint            )                # Handle comparison operators        if constraint.startswith(\">=\") or constraint.startswith(\">\"):            return UnoplatVersion(minimum_version=constraint)        elif constraint.startswith(\"<=\") or constraint.startswith(\"<\"):            return UnoplatVersion(maximum_version=constraint)        elif constraint.startswith(\"==\"):            return UnoplatVersion(current_version=constraint)                # Plain version string treated as exact version        return UnoplatVersion(current_version=f\"=={constraint}\")        "},{"Name":"_parse_dependencies","Parameters":[{"TypeValue":"deps_dict","TypeType":"Dict"},{"DefaultValue":"None","TypeValue":"group","TypeType":"Optional[str]"}],"Position":{"StartLine":131,"StartLinePosition":4,"StopLine":201,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":"os"},{"TypeValue":"pyproject_data","TypeType":"tomlkit"},{"TypeValue":"poetry_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":"{}"},{"TypeValue":"unoplatPackageManager","TypeType":"SetupParser"},{"TypeValue":"main_deps","TypeType":"poetry_data"},{"TypeValue":"programming_language_version","TypeType":"metadata"},{"TypeValue":"parts","TypeType":"[p.strip()forpinconstraint.split(\",\")]"},{"TypeValue":"min_ver","TypeType":"part"},{"TypeValue":"max_ver","TypeType":"part"},{"TypeValue":"current_ver","TypeType":"part"},{"TypeValue":"version","TypeType":"UnoplatVersion"},{"TypeValue":"extras","TypeType":"constraint"},{"TypeValue":"source","TypeType":"\"url\""},{"TypeValue":"source_url","TypeType":"constraint"},{"TypeValue":"source_reference","TypeType":""},{"TypeValue":"subdirectory","TypeType":"constraint"},{"TypeValue":"tuple_dependency","TypeType":"UnoplatProjectDependency"},{"TypeValue":"dependencies[name]","TypeType":"tuple_dependency"}],"Content":"def _parse_dependencies(self, deps_dict: Dict, group: Optional[str] = None) -> Dict[str, UnoplatProjectDependency]:        dependencies = {}                for name, constraint in deps_dict.items():            if name == \"python\":  # Skip python version constraint                continue                            # Initialize dependency fields            version = UnoplatVersion()            extras = None            source = None            source_url = None            source_reference = None            subdirectory = None                        try:                # Parse different dependency specification formats                if isinstance(constraint, str):                    # Handle version constraints                    version = self._parse_version_constraint(constraint)                                    elif isinstance(constraint, dict):                    # Complex dependency specification                    if \"version\" in constraint:                        version = self._parse_version_constraint(constraint[\"version\"])                    extras = constraint.get(\"extras\")                                        # Handle git dependencies                    if \"git\" in constraint:                        source = \"git\"                        source_url = constraint[\"git\"]                        source_reference = constraint.get(\"rev\") or constraint.get(\"branch\") or constraint.get(\"tag\")                        subdirectory = constraint.get(\"subdirectory\")                        version = UnoplatVersion()  # Git dependencies don't have version constraints                                            # Handle path dependencies                    elif \"path\" in constraint:                        source = \"path\"                        source_url = constraint[\"path\"]                        version = UnoplatVersion()  # Path dependencies don't have version constraints                                            # Handle url dependencies                    elif \"url\" in constraint:                        source = \"url\"                        source_url = constraint[\"url\"]                        version = UnoplatVersion()  # URL dependencies don't have version constraints                else:                    logger.warning(f\"Skipping invalid dependency specification for {name}\")                    continue                                tuple_dependency = UnoplatProjectDependency(                    version=version,                    extras=extras,                    source=source,                    source_url=source_url,                    source_reference=source_reference,                    subdirectory=subdirectory                )                dependencies[name] = tuple_dependency                            except Exception as e:                logger.warning(f\"Error parsing dependency {name}: {str(e)}\")                # Add dependency with empty version constraint                tuple_dependency = UnoplatProjectDependency(                    version=UnoplatVersion()                )                dependencies[name] = tuple_dependency                        return dependencies        "},{"Name":"_parse_python_version","Parameters":[{"TypeValue":"version_constraint","TypeType":"Optional[str]"}],"Position":{"StartLine":201,"StartLinePosition":4,"StopLine":249,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":"os"},{"TypeValue":"pyproject_data","TypeType":"tomlkit"},{"TypeValue":"poetry_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":"{}"},{"TypeValue":"unoplatPackageManager","TypeType":"SetupParser"},{"TypeValue":"main_deps","TypeType":"poetry_data"},{"TypeValue":"programming_language_version","TypeType":"metadata"},{"TypeValue":"parts","TypeType":"[p.strip()forpinversion_constraint.split(\",\")]"},{"TypeValue":"min_ver","TypeType":"f\">={part[1:]}\""},{"TypeValue":"max_ver","TypeType":"part"},{"TypeValue":"current_ver","TypeType":"part"},{"TypeValue":"version","TypeType":"UnoplatVersion"},{"TypeValue":"extras","TypeType":"constraint"},{"TypeValue":"source","TypeType":"\"url\""},{"TypeValue":"source_url","TypeType":"constraint"},{"TypeValue":"source_reference","TypeType":""},{"TypeValue":"subdirectory","TypeType":"constraint"},{"TypeValue":"tuple_dependency","TypeType":"UnoplatProjectDependency"},{"TypeValue":"dependencies[name]","TypeType":"tuple_dependency"},{"TypeValue":"result","TypeType":"{}"},{"TypeValue":"result[\"min\"]","TypeType":"min_ver"},{"TypeValue":"result[\"max\"]","TypeType":"max_ver"}],"Content":"def _parse_python_version(self, version_constraint: Optional[str]) -> Dict[str, str]:        \"\"\"Parse Python version constraint into a standardized format.\"\"\"        if not version_constraint:            return {}  # Return empty dict if no constraint                    if \",\" in version_constraint:            parts = [p.strip() for p in version_constraint.split(\",\")]            min_ver = None            max_ver = None                        for part in parts:                if part.startswith(\">=\"):                    min_ver = part                elif part.startswith(\">\"):                    min_ver = part                elif part.startswith(\"<=\"):                    max_ver = part                elif part.startswith(\"<\"):                    max_ver = part                elif part.startswith(\"^\"):                    min_ver = f\">={part[1:]}\"                elif part.startswith(\"~\"):                    min_ver = f\">={part[1:]}\"                                result = {}            if min_ver:                result[\"min\"] = min_ver            if max_ver:                result[\"max\"] = max_ver            return result                # Handle single constraints        if version_constraint.startswith(\"^\"):            return {\"min\": f\">={version_constraint[1:]}\"}        elif version_constraint.startswith(\"~\"):            return {\"min\": f\">={version_constraint[1:]}\"}        elif version_constraint.startswith(\">=\"):            return {\"min\": version_constraint}        elif version_constraint.startswith(\">\"):            return {\"min\": version_constraint}        elif version_constraint.startswith(\"<=\"):            return {\"max\": version_constraint}        elif version_constraint.startswith(\"<\"):            return {\"max\": version_constraint}        else:            # Exact version            return {\"min\": f\">={version_constraint}\", \"max\": f\"<={version_constraint}\"}        "},{"Name":"_get_entry_points","Parameters":[{"TypeValue":"scripts","TypeType":"Dict[str,str]"}],"Position":{"StartLine":249,"StartLinePosition":4,"StopLine":281,"StopLinePosition":41},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":"os"},{"TypeValue":"pyproject_data","TypeType":"tomlkit"},{"TypeValue":"poetry_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":"{}"},{"TypeValue":"unoplatPackageManager","TypeType":"SetupParser"},{"TypeValue":"main_deps","TypeType":"poetry_data"},{"TypeValue":"programming_language_version","TypeType":"metadata"},{"TypeValue":"parts","TypeType":"[p.strip()forpinversion_constraint.split(\",\")]"},{"TypeValue":"min_ver","TypeType":"f\">={part[1:]}\""},{"TypeValue":"max_ver","TypeType":"part"},{"TypeValue":"current_ver","TypeType":"part"},{"TypeValue":"version","TypeType":"UnoplatVersion"},{"TypeValue":"extras","TypeType":"constraint"},{"TypeValue":"source","TypeType":"\"url\""},{"TypeValue":"source_url","TypeType":"constraint"},{"TypeValue":"source_reference","TypeType":""},{"TypeValue":"subdirectory","TypeType":"constraint"},{"TypeValue":"tuple_dependency","TypeType":"UnoplatProjectDependency"},{"TypeValue":"dependencies[name]","TypeType":"tuple_dependency"},{"TypeValue":"result","TypeType":"{}"},{"TypeValue":"result[\"min\"]","TypeType":"min_ver"},{"TypeValue":"result[\"max\"]","TypeType":"max_ver"}],"Content":"def _get_entry_points(self, scripts: Dict[str, str]) -> Dict[str, str]:        \"\"\"Get all entry points from Poetry scripts section.                Returns all scripts defined in pyproject.toml as a dictionary mapping        script names to their entry points.                Handles various script definitions:        1. Module format:            [tool.poetry.scripts]            cli = \"package_name.module:function\"                2. Command format:            [tool.poetry.scripts]            serve = \"uvicorn main:app --reload\"                Args:            scripts (Dict[str, str]): Dictionary of scripts from pyproject.toml                Returns:            Dict[str, str]: Dictionary mapping script names to their entry points                Example from pyproject.toml:            [tool.poetry.scripts]            unoplat-code-confluence = \"unoplat_code_confluence.__main__:main\"            serve = \"uvicorn api:app --reload\"                        Returns:            {                \"unoplat-code-confluence\": \"unoplat_code_confluence.__main__:main\",                \"serve\": \"uvicorn api:app --reload\"            }        \"\"\"        return scripts if scripts else {}"}],"Imports":[{"Source":"unoplat_code_confluence.configuration.settings","UsageName":["PackageManagerType","ProgrammingLanguageMetadata"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_project_dependency","UsageName":["UnoplatProjectDependency"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_version","UsageName":["UnoplatVersion"]},{"Source":"unoplat_code_confluence.parser.python.package_manager.package_manager_strategy","UsageName":["PackageManagerStrategy"]},{"Source":"unoplat_code_confluence.parser.python.package_manager.utils.requirements_utils","UsageName":["RequirementsUtils"]},{"Source":"unoplat_code_confluence.parser.python.package_manager.utils.setup_parser","UsageName":["SetupParser"]},{"Source":"os"},{"Source":"typing","UsageName":["Dict","Optional"]},{"Source":"loguru","UsageName":["logger"]},{"Source":"tomlkit"}],"Position":{"StartLine":20,"StopLine":281,"StopLinePosition":41},"Content":"class PythonPoetryStrategy(PackageManagerStrategy):    def process_metadata(self, local_workspace_path: str, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Process poetry specific metadata from pyproject.toml\"\"\"        pyproject_path = os.path.join(local_workspace_path, \"pyproject.toml\")                if not os.path.exists(pyproject_path):            logger.warning(f\"pyproject.toml not found at {pyproject_path}\")            return self._create_empty_metadata(metadata)                    try:            with open(pyproject_path, \"r\") as f:                pyproject_data = tomlkit.parse(f.read())                        poetry_data = pyproject_data.get(\"tool\", {}).get(\"poetry\", {})            if not poetry_data:                logger.warning(\"No poetry configuration found in pyproject.toml, falling back to requirements\")                # Try parsing requirements folder using RequirementsUtils                dependencies: Dict[str, UnoplatProjectDependency] = RequirementsUtils.parse_requirements_folder(local_workspace_path)                unoplatPackageManager: UnoplatPackageManagerMetadata =  UnoplatPackageManagerMetadata(                    dependencies=dependencies,                    programming_language=metadata.language.value,                    package_manager=PackageManagerType.PIP.value                )                try:                    unoplatPackageManager = SetupParser.parse_setup_file(local_workspace_path, unoplatPackageManager)                except FileNotFoundError:                    logger.warning(\"setup.py not found, skipping setup.py parsing\")                return unoplatPackageManager                                            # Parse only main dependencies            main_deps = poetry_data.get(\"dependencies\", {})            dependencies: Dict[str, UnoplatProjectDependency] = self._parse_dependencies(main_deps) #type: ignore            programming_language_version=self._parse_python_version(main_deps.get(\"python\"))            # if we do not get python version fall back to the one taken from configuration                       if programming_language_version is None:                programming_language_version = metadata.language_version            # Create metadata object with entry_points instead of entry_point            return UnoplatPackageManagerMetadata(                dependencies=dependencies,                package_name=poetry_data.get(\"name\"),                programming_language=metadata.language.value,                package_manager=metadata.package_manager,                programming_language_version=programming_language_version,                project_version=poetry_data.get(\"version\"),                description=poetry_data.get(\"description\"),                authors=poetry_data.get(\"authors\"),                entry_points=self._get_entry_points(poetry_data.get(\"scripts\", {}))            )                    except Exception as e:            logger.error(f\"Error parsing pyproject.toml: {str(e)}\")            return self._create_empty_metadata(metadata)        def _create_empty_metadata(self, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Create empty metadata with basic information\"\"\"        return UnoplatPackageManagerMetadata(            dependencies={},            programming_language=metadata.language.value,            package_manager=metadata.package_manager        )        def _parse_version_constraint(self, constraint: str) -> UnoplatVersion:        \"\"\"Parse version constraint into UnoplatVersion format.\"\"\"        if not constraint or constraint == \"*\":            return UnoplatVersion()                    # Handle comma-separated constraints (e.g., \">=1.0.0,<2.0.0\")        if \",\" in constraint:            parts = [p.strip() for p in constraint.split(\",\")]            min_ver = None            max_ver = None            current_ver = None                        for part in parts:                if part.startswith(\">=\") or part.startswith(\">\"):                    min_ver = part                elif part.startswith(\"<=\") or part.startswith(\"<\"):                    max_ver = part                elif part.startswith(\"==\"):                    current_ver = part                                return UnoplatVersion(                minimum_version=min_ver,                maximum_version=max_ver,                current_version=current_ver            )                # Handle caret constraints (e.g., \"^1.0.0\")        if constraint.startswith(\"^\"):            return UnoplatVersion(                minimum_version=constraint            )                # Handle tilde constraints (e.g., \"~1.0.0\")        if constraint.startswith(\"~\"):            return UnoplatVersion(                minimum_version=constraint            )                # Handle comparison operators        if constraint.startswith(\">=\") or constraint.startswith(\">\"):            return UnoplatVersion(minimum_version=constraint)        elif constraint.startswith(\"<=\") or constraint.startswith(\"<\"):            return UnoplatVersion(maximum_version=constraint)        elif constraint.startswith(\"==\"):            return UnoplatVersion(current_version=constraint)                # Plain version string treated as exact version        return UnoplatVersion(current_version=f\"=={constraint}\")        def _parse_dependencies(self, deps_dict: Dict, group: Optional[str] = None) -> Dict[str, UnoplatProjectDependency]:        dependencies = {}                for name, constraint in deps_dict.items():            if name == \"python\":  # Skip python version constraint                continue                            # Initialize dependency fields            version = UnoplatVersion()            extras = None            source = None            source_url = None            source_reference = None            subdirectory = None                        try:                # Parse different dependency specification formats                if isinstance(constraint, str):                    # Handle version constraints                    version = self._parse_version_constraint(constraint)                                    elif isinstance(constraint, dict):                    # Complex dependency specification                    if \"version\" in constraint:                        version = self._parse_version_constraint(constraint[\"version\"])                    extras = constraint.get(\"extras\")                                        # Handle git dependencies                    if \"git\" in constraint:                        source = \"git\"                        source_url = constraint[\"git\"]                        source_reference = constraint.get(\"rev\") or constraint.get(\"branch\") or constraint.get(\"tag\")                        subdirectory = constraint.get(\"subdirectory\")                        version = UnoplatVersion()  # Git dependencies don't have version constraints                                            # Handle path dependencies                    elif \"path\" in constraint:                        source = \"path\"                        source_url = constraint[\"path\"]                        version = UnoplatVersion()  # Path dependencies don't have version constraints                                            # Handle url dependencies                    elif \"url\" in constraint:                        source = \"url\"                        source_url = constraint[\"url\"]                        version = UnoplatVersion()  # URL dependencies don't have version constraints                else:                    logger.warning(f\"Skipping invalid dependency specification for {name}\")                    continue                                tuple_dependency = UnoplatProjectDependency(                    version=version,                    extras=extras,                    source=source,                    source_url=source_url,                    source_reference=source_reference,                    subdirectory=subdirectory                )                dependencies[name] = tuple_dependency                            except Exception as e:                logger.warning(f\"Error parsing dependency {name}: {str(e)}\")                # Add dependency with empty version constraint                tuple_dependency = UnoplatProjectDependency(                    version=UnoplatVersion()                )                dependencies[name] = tuple_dependency                        return dependencies        def _parse_python_version(self, version_constraint: Optional[str]) -> Dict[str, str]:        \"\"\"Parse Python version constraint into a standardized format.\"\"\"        if not version_constraint:            return {}  # Return empty dict if no constraint                    if \",\" in version_constraint:            parts = [p.strip() for p in version_constraint.split(\",\")]            min_ver = None            max_ver = None                        for part in parts:                if part.startswith(\">=\"):                    min_ver = part                elif part.startswith(\">\"):                    min_ver = part                elif part.startswith(\"<=\"):                    max_ver = part                elif part.startswith(\"<\"):                    max_ver = part                elif part.startswith(\"^\"):                    min_ver = f\">={part[1:]}\"                elif part.startswith(\"~\"):                    min_ver = f\">={part[1:]}\"                                result = {}            if min_ver:                result[\"min\"] = min_ver            if max_ver:                result[\"max\"] = max_ver            return result                # Handle single constraints        if version_constraint.startswith(\"^\"):            return {\"min\": f\">={version_constraint[1:]}\"}        elif version_constraint.startswith(\"~\"):            return {\"min\": f\">={version_constraint[1:]}\"}        elif version_constraint.startswith(\">=\"):            return {\"min\": version_constraint}        elif version_constraint.startswith(\">\"):            return {\"min\": version_constraint}        elif version_constraint.startswith(\"<=\"):            return {\"max\": version_constraint}        elif version_constraint.startswith(\"<\"):            return {\"max\": version_constraint}        else:            # Exact version            return {\"min\": f\">={version_constraint}\", \"max\": f\"<={version_constraint}\"}        def _get_entry_points(self, scripts: Dict[str, str]) -> Dict[str, str]:        \"\"\"Get all entry points from Poetry scripts section.                Returns all scripts defined in pyproject.toml as a dictionary mapping        script names to their entry points.                Handles various script definitions:        1. Module format:            [tool.poetry.scripts]            cli = \"package_name.module:function\"                2. Command format:            [tool.poetry.scripts]            serve = \"uvicorn main:app --reload\"                Args:            scripts (Dict[str, str]): Dictionary of scripts from pyproject.toml                Returns:            Dict[str, str]: Dictionary mapping script names to their entry points                Example from pyproject.toml:            [tool.poetry.scripts]            unoplat-code-confluence = \"unoplat_code_confluence.__main__:main\"            serve = \"uvicorn api:app --reload\"                        Returns:            {                \"unoplat-code-confluence\": \"unoplat_code_confluence.__main__:main\",                \"serve\": \"uvicorn api:app --reload\"            }        \"\"\"        return scripts if scripts else {}"},{"NodeName":"PackageManagerStrategyFactory","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/package_manager/package_manager_factory.py","Functions":[{"Name":"get_strategy","Parameters":[{"TypeValue":"cls","TypeType":""},{"TypeValue":"package_manager","TypeType":"str"}],"Annotations":[{"Name":"classmethod","Position":{"StartLine":16,"StartLinePosition":4,"StopLine":17,"StopLinePosition":4}}],"Position":{"StartLine":17,"StartLinePosition":4,"StopLine":26},"LocalVariables":[{"TypeValue":"_strategies","TypeType":""}],"Content":"def get_strategy(cls, package_manager: str) -> PackageManagerStrategy:                if package_manager not in cls._strategies:            raise UnsupportedPackageManagerError(                f\"Unsupported package manager {package_manager}\"            )                return cls._strategies[package_manager]()"}],"Imports":[{"Source":"unoplat_code_confluence.parser.python.package_manager.package_manager_strategy","UsageName":["PackageManagerStrategy"]},{"Source":"unoplat_code_confluence.parser.python.package_manager.pip.pip_strategy","UsageName":["PipStrategy"]},{"Source":"unoplat_code_confluence.parser.python.package_manager.poetry.poetry_strategy","UsageName":["PythonPoetryStrategy"]},{"Source":"typing","UsageName":["Dict"]}],"Position":{"StartLine":10,"StopLine":26},"Content":"class PackageManagerStrategyFactory:    _strategies: Dict[str, type[PackageManagerStrategy]] = {        \"poetry\": PythonPoetryStrategy,        \"pip\": PipStrategy       }    @classmethod    def get_strategy(cls, package_manager: str) -> PackageManagerStrategy:                if package_manager not in cls._strategies:            raise UnsupportedPackageManagerError(                f\"Unsupported package manager {package_manager}\"            )                return cls._strategies[package_manager]()"},{"NodeName":"UnsupportedPackageManagerError","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/package_manager/package_manager_factory.py","MultipleExtend":["Exception"],"Imports":[{"Source":"unoplat_code_confluence.parser.python.package_manager.package_manager_strategy","UsageName":["PackageManagerStrategy"]},{"Source":"unoplat_code_confluence.parser.python.package_manager.pip.pip_strategy","UsageName":["PipStrategy"]},{"Source":"unoplat_code_confluence.parser.python.package_manager.poetry.poetry_strategy","UsageName":["PythonPoetryStrategy"]},{"Source":"typing","UsageName":["Dict"]}],"Position":{"StartLine":26,"StopLine":27,"StopLinePosition":9},"Content":"class UnsupportedPackageManagerError(Exception):    pass "},{"NodeName":"ProgrammingFileReader","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/utils/read_programming_file.py","Functions":[{"Name":"read_file","Parameters":[{"TypeValue":"file_path","TypeType":"str|Path"}],"FunctionCalls":[{"FunctionName":"Path","Position":{"StartLine":21,"StartLinePosition":24,"StopLine":21,"StopLinePosition":34}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":6,"StartLinePosition":4,"StopLine":7,"StopLinePosition":4}}],"Position":{"StartLine":7,"StartLinePosition":4,"StopLine":30},"LocalVariables":[{"TypeValue":"file_path","TypeType":"Path"}],"Content":"def read_file(file_path: str | Path) -> str:        \"\"\"        Reads the content of a file and returns it as a string.                Args:            file_path (str | Path): Path to the file to be read                    Returns:            str: Content of the file as a string                    Raises:            FileNotFoundError: If the specified file does not exist            IOError: If there are issues reading the file        \"\"\"        file_path = Path(file_path)        if not file_path.exists():            raise FileNotFoundError(f\"File not found: {file_path}\")                    try:            with open(file_path, 'r', encoding='utf-8') as file:                return file.read()        except IOError as e:            raise IOError(f\"Error reading file {file_path}: {str(e)}\")"}],"Imports":[{"Source":"pathlib","UsageName":["Path"]}],"Position":{"StartLine":5,"StopLine":30},"Content":"class ProgrammingFileReader:    @staticmethod    def read_file(file_path: str | Path) -> str:        \"\"\"        Reads the content of a file and returns it as a string.                Args:            file_path (str | Path): Path to the file to be read                    Returns:            str: Content of the file as a string                    Raises:            FileNotFoundError: If the specified file does not exist            IOError: If there are issues reading the file        \"\"\"        file_path = Path(file_path)        if not file_path.exists():            raise FileNotFoundError(f\"File not found: {file_path}\")                    try:            with open(file_path, 'r', encoding='utf-8') as file:                return file.read()        except IOError as e:            raise IOError(f\"Error reading file {file_path}: {str(e)}\")"},{"NodeName":"PythonImportCommentParser","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/utils/python_import_comment_parser.py","Functions":[{"Name":"__init__","Position":{"StartLine":21,"StartLinePosition":4,"StopLine":29,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"SECTION_PATTERNS","TypeType":"{ImportType.STANDARD:[r\"#\\s*Standard\\s+Library\\s*\"],ImportType.EXTERNAL:[r\"#\\s*Third\\s+Party\\s*\"],ImportType.INTERNAL:[r\"#\\s*First\\s+Party\\s*\"],ImportType.LOCAL:[r\"#\\s*Local\\s*\"]}"},{"TypeValue":"self.compiled_patterns","TypeType":"{import_type:[re.compile(pattern,re.IGNORECASE)forpatterninpatterns]forimport_type,patternsinself.SECTION_PATTERNS.items()}"}],"Content":"def __init__(self):        # Compile all patterns for better performance        self.compiled_patterns = {            import_type: [re.compile(pattern, re.IGNORECASE)                          for pattern in patterns]            for import_type, patterns in self.SECTION_PATTERNS.items()        }        "},{"Name":"identify_section","Parameters":[{"TypeValue":"comment_line","TypeType":"str"}],"Position":{"StartLine":29,"StartLinePosition":4,"StopLine":54,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"SECTION_PATTERNS","TypeType":"{ImportType.STANDARD:[r\"#\\s*Standard\\s+Library\\s*\"],ImportType.EXTERNAL:[r\"#\\s*Third\\s+Party\\s*\"],ImportType.INTERNAL:[r\"#\\s*First\\s+Party\\s*\"],ImportType.LOCAL:[r\"#\\s*Local\\s*\"]}"},{"TypeValue":"self.compiled_patterns","TypeType":"{import_type:[re.compile(pattern,re.IGNORECASE)forpatterninpatterns]forimport_type,patternsinself.SECTION_PATTERNS.items()}"}],"Content":"def identify_section(self, comment_line: str) -> Optional[ImportType]:        \"\"\"        Identifies the import section type from a comment line.                Args:            comment_line: The comment line to parse                    Returns:            ImportType if the comment matches a section pattern, None otherwise                    Examples:            \"# Standard Library\" -> ImportType.STANDARD            \"# Third Party\" -> ImportType.EXTERNAL            \"# First Party\" -> ImportType.INTERNAL            \"# Local\" -> ImportType.LOCAL        \"\"\"        if not comment_line.strip():            return None                    for import_type, patterns in self.compiled_patterns.items():            for pattern in patterns:                if pattern.match(comment_line.strip()):                    return import_type        return None        "},{"Name":"_clean_import_line","Parameters":[{"TypeValue":"import_str","TypeType":"str"}],"FunctionCalls":[{"NodeName":"re","FunctionName":"sub","Position":{"StartLine":76,"StartLinePosition":20,"StopLine":76,"StopLinePosition":53}}],"Position":{"StartLine":54,"StartLinePosition":4,"StopLine":80,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"SECTION_PATTERNS","TypeType":"{ImportType.STANDARD:[r\"#\\s*Standard\\s+Library\\s*\"],ImportType.EXTERNAL:[r\"#\\s*Third\\s+Party\\s*\"],ImportType.INTERNAL:[r\"#\\s*First\\s+Party\\s*\"],ImportType.LOCAL:[r\"#\\s*Local\\s*\"]}"},{"TypeValue":"self.compiled_patterns","TypeType":"{import_type:[re.compile(pattern,re.IGNORECASE)forpatterninpatterns]forimport_type,patternsinself.SECTION_PATTERNS.items()}"},{"TypeValue":"cleaned","TypeType":"re"}],"Content":"def _clean_import_line(self, import_str: str) -> str:        \"\"\"Clean up an import line by standardizing spaces and removing unnecessary characters.                Args:            import_str: Raw import string to clean                    Returns:            Cleaned import string with standardized spacing        \"\"\"        # Remove parentheses and any trailing/leading whitespace        cleaned = import_str.replace('(', '').replace(')', '').strip()                # Standardize spaces around commas        cleaned = re.sub(r'\\s*,\\s*', ', ', cleaned)                # Standardize spaces around 'as' keyword        cleaned = re.sub(r'\\s+as\\s+', ' as ', cleaned)                # Standardize spaces after 'import' keyword        cleaned = re.sub(r'import\\s+', 'import ', cleaned)                # Standardize spaces after 'from' keyword        cleaned = re.sub(r'from\\s+', 'from ', cleaned)                return cleaned        "},{"Name":"parse_import_sections","Parameters":[{"TypeValue":"file_content","TypeType":"str"}],"FunctionCalls":[{"FunctionName":"sections","Position":{"StartLine":160,"StartLinePosition":20,"StopLine":160,"StopLinePosition":36}},{"NodeName":"sections","FunctionName":"append","Position":{"StartLine":160,"StartLinePosition":37,"StopLine":160,"StopLinePosition":55}}],"Position":{"StartLine":80,"StartLinePosition":4,"StopLine":164,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"SECTION_PATTERNS","TypeType":"{ImportType.STANDARD:[r\"#\\s*Standard\\s+Library\\s*\"],ImportType.EXTERNAL:[r\"#\\s*Third\\s+Party\\s*\"],ImportType.INTERNAL:[r\"#\\s*First\\s+Party\\s*\"],ImportType.LOCAL:[r\"#\\s*Local\\s*\"]}"},{"TypeValue":"self.compiled_patterns","TypeType":"{import_type:[re.compile(pattern,re.IGNORECASE)forpatterninpatterns]forimport_type,patternsinself.SECTION_PATTERNS.items()}"},{"TypeValue":"cleaned","TypeType":"re"},{"TypeValue":"sections","TypeType":""},{"TypeValue":"current_section","TypeType":"section_type"},{"TypeValue":"current_import","TypeType":"[]"},{"TypeValue":"in_multiline","TypeType":"False"},{"TypeValue":"line","TypeType":"line"},{"TypeValue":"import_str","TypeType":"self"},{"TypeValue":"section_type","TypeType":"self"},{"TypeValue":"cleaned_import","TypeType":"self"},{"TypeValue":"cleaned_line","TypeType":"line"}],"Content":"def parse_import_sections(self, file_content: str) -> Dict[ImportType, List[str]]:        \"\"\"        Parses Python code lines into sections based on import comments.                Args:            file_content: String containing the Python file content                    Returns:            Dictionary mapping ImportType to list of import lines in that section        \"\"\"        sections: Dict[ImportType, List[str]] = {            ImportType.STANDARD: [],            ImportType.EXTERNAL: [],            ImportType.INTERNAL: [],            ImportType.LOCAL: []        }                current_section: ImportType | None = None        current_import: List[str] = []        in_multiline: bool = False                # Split file content into lines and process each line        for line in file_content.splitlines():            line = line.strip()                        # Skip empty lines            if not line:                continue                            # Check if this is a section comment            if line.startswith('#'):                # If we were in a multiline import, save it before changing section                if in_multiline and current_import and current_section is not None:                    import_str = self._join_multiline_import(current_import)                    sections[current_section].append(import_str)                    current_import = []                    in_multiline = False                                section_type = self.identify_section(line)                if section_type:                    current_section = section_type                continue                        # Handle import lines            if current_section is not None:                if line.startswith('import ') or line.startswith('from '):                    # If we were in a multiline import, save the previous one                    if in_multiline and current_import:                        import_str = self._join_multiline_import(current_import)                        sections[current_section].append(import_str)                        current_import = []                                        # Start new import                    current_import = [line]                    # Check if this is start of a multiline import                    in_multiline = '(' in line and ')' not in line                                        # If it's a single line import, save it immediately                    if not in_multiline:                        cleaned_import = self._clean_import_line(line)                        sections[current_section].append(cleaned_import)                        current_import = []                                # Continue multiline import                elif in_multiline:                    # Remove trailing comma if present                    cleaned_line = line.rstrip(',').strip()                    if cleaned_line:                        current_import.append(cleaned_line)                                        # Check if multiline import ends                    if ')' in line:                        import_str = self._join_multiline_import(current_import)                        sections[current_section].append(import_str)                        current_import = []                        in_multiline = False                # Handle any remaining multiline import        if in_multiline and current_import and current_section is not None:            import_str = self._join_multiline_import(current_import)            sections[current_section].append(import_str)                return sections        "},{"Name":"_join_multiline_import","Parameters":[{"TypeValue":"import_lines","TypeType":"List[str]"}],"Position":{"StartLine":164,"StartLinePosition":4,"StopLine":214,"StopLinePosition":3},"LocalVariables":[{"TypeValue":"SECTION_PATTERNS","TypeType":"{ImportType.STANDARD:[r\"#\\s*Standard\\s+Library\\s*\"],ImportType.EXTERNAL:[r\"#\\s*Third\\s+Party\\s*\"],ImportType.INTERNAL:[r\"#\\s*First\\s+Party\\s*\"],ImportType.LOCAL:[r\"#\\s*Local\\s*\"]}"},{"TypeValue":"self.compiled_patterns","TypeType":"{import_type:[re.compile(pattern,re.IGNORECASE)forpatterninpatterns]forimport_type,patternsinself.SECTION_PATTERNS.items()}"},{"TypeValue":"cleaned","TypeType":"re"},{"TypeValue":"sections","TypeType":""},{"TypeValue":"current_section","TypeType":"section_type"},{"TypeValue":"current_import","TypeType":"[]"},{"TypeValue":"in_multiline","TypeType":"False"},{"TypeValue":"line","TypeType":"line"},{"TypeValue":"import_str","TypeType":"self"},{"TypeValue":"section_type","TypeType":"self"},{"TypeValue":"cleaned_import","TypeType":"self"},{"TypeValue":"cleaned_line","TypeType":"line"},{"TypeValue":"prefix","TypeType":"import_lines"},{"TypeValue":"items","TypeType":"[]"},{"TypeValue":"joined_items","TypeType":"', '"},{"TypeValue":"result","TypeType":"f\"{prefix} {joined_items}\""}],"Content":"def _join_multiline_import(self, import_lines: List[str]) -> str:        \"\"\"Join multiline import statements while preserving commas.                Args:            import_lines: List of import statement lines                    Returns:            Single line import statement with proper formatting                    Example:            Input: [                'from sqlalchemy import (',                '    Column,',                '    Integer as Int,',                '    String as Str,',                '    ForeignKey',                ')'            ]            Output: 'from sqlalchemy import Column, Integer as Int, String as Str, ForeignKey'        \"\"\"        # Get the import statement prefix (everything before the first parenthesis)        prefix = import_lines[0].split('(')[0].strip()                # Process the imported items        items = []        for line in import_lines[1:]:  # Skip the first line as it's the prefix            # Remove parentheses and extra whitespace            line = line.replace('(', '').replace(')', '').strip()                        # Skip empty lines            if not line:                continue                            # If line ends with comma, remove it but remember we need one            if line.endswith(','):                line = line.rstrip(',')                if line:  # Only add non-empty items                    items.append(line)            else:                if line:  # Only add non-empty items                    items.append(line)                # Join items with commas        joined_items = ', '.join(items)                # Combine prefix with items        result = f\"{prefix} {joined_items}\"                return result       "}],"Imports":[{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_import_type","UsageName":["ImportType"]},{"Source":"re"},{"Source":"typing","UsageName":["Dict","List","Optional"]}],"Position":{"StartLine":10,"StopLine":214,"StopLinePosition":3},"Content":"class PythonImportCommentParser:    \"\"\"Parser for Python import section comments\"\"\"        # Standard comment patterns for each import type    SECTION_PATTERNS = {        ImportType.STANDARD: [r\"#\\s*Standard\\s+Library\\s*\"],        ImportType.EXTERNAL: [r\"#\\s*Third\\s+Party\\s*\"],        ImportType.INTERNAL: [r\"#\\s*First\\s+Party\\s*\"],        ImportType.LOCAL: [r\"#\\s*Local\\s*\"]    }        def __init__(self):        # Compile all patterns for better performance        self.compiled_patterns = {            import_type: [re.compile(pattern, re.IGNORECASE)                          for pattern in patterns]            for import_type, patterns in self.SECTION_PATTERNS.items()        }        def identify_section(self, comment_line: str) -> Optional[ImportType]:        \"\"\"        Identifies the import section type from a comment line.                Args:            comment_line: The comment line to parse                    Returns:            ImportType if the comment matches a section pattern, None otherwise                    Examples:            \"# Standard Library\" -> ImportType.STANDARD            \"# Third Party\" -> ImportType.EXTERNAL            \"# First Party\" -> ImportType.INTERNAL            \"# Local\" -> ImportType.LOCAL        \"\"\"        if not comment_line.strip():            return None                    for import_type, patterns in self.compiled_patterns.items():            for pattern in patterns:                if pattern.match(comment_line.strip()):                    return import_type        return None        def _clean_import_line(self, import_str: str) -> str:        \"\"\"Clean up an import line by standardizing spaces and removing unnecessary characters.                Args:            import_str: Raw import string to clean                    Returns:            Cleaned import string with standardized spacing        \"\"\"        # Remove parentheses and any trailing/leading whitespace        cleaned = import_str.replace('(', '').replace(')', '').strip()                # Standardize spaces around commas        cleaned = re.sub(r'\\s*,\\s*', ', ', cleaned)                # Standardize spaces around 'as' keyword        cleaned = re.sub(r'\\s+as\\s+', ' as ', cleaned)                # Standardize spaces after 'import' keyword        cleaned = re.sub(r'import\\s+', 'import ', cleaned)                # Standardize spaces after 'from' keyword        cleaned = re.sub(r'from\\s+', 'from ', cleaned)                return cleaned        def parse_import_sections(self, file_content: str) -> Dict[ImportType, List[str]]:        \"\"\"        Parses Python code lines into sections based on import comments.                Args:            file_content: String containing the Python file content                    Returns:            Dictionary mapping ImportType to list of import lines in that section        \"\"\"        sections: Dict[ImportType, List[str]] = {            ImportType.STANDARD: [],            ImportType.EXTERNAL: [],            ImportType.INTERNAL: [],            ImportType.LOCAL: []        }                current_section: ImportType | None = None        current_import: List[str] = []        in_multiline: bool = False                # Split file content into lines and process each line        for line in file_content.splitlines():            line = line.strip()                        # Skip empty lines            if not line:                continue                            # Check if this is a section comment            if line.startswith('#'):                # If we were in a multiline import, save it before changing section                if in_multiline and current_import and current_section is not None:                    import_str = self._join_multiline_import(current_import)                    sections[current_section].append(import_str)                    current_import = []                    in_multiline = False                                section_type = self.identify_section(line)                if section_type:                    current_section = section_type                continue                        # Handle import lines            if current_section is not None:                if line.startswith('import ') or line.startswith('from '):                    # If we were in a multiline import, save the previous one                    if in_multiline and current_import:                        import_str = self._join_multiline_import(current_import)                        sections[current_section].append(import_str)                        current_import = []                                        # Start new import                    current_import = [line]                    # Check if this is start of a multiline import                    in_multiline = '(' in line and ')' not in line                                        # If it's a single line import, save it immediately                    if not in_multiline:                        cleaned_import = self._clean_import_line(line)                        sections[current_section].append(cleaned_import)                        current_import = []                                # Continue multiline import                elif in_multiline:                    # Remove trailing comma if present                    cleaned_line = line.rstrip(',').strip()                    if cleaned_line:                        current_import.append(cleaned_line)                                        # Check if multiline import ends                    if ')' in line:                        import_str = self._join_multiline_import(current_import)                        sections[current_section].append(import_str)                        current_import = []                        in_multiline = False                # Handle any remaining multiline import        if in_multiline and current_import and current_section is not None:            import_str = self._join_multiline_import(current_import)            sections[current_section].append(import_str)                return sections        def _join_multiline_import(self, import_lines: List[str]) -> str:        \"\"\"Join multiline import statements while preserving commas.                Args:            import_lines: List of import statement lines                    Returns:            Single line import statement with proper formatting                    Example:            Input: [                'from sqlalchemy import (',                '    Column,',                '    Integer as Int,',                '    String as Str,',                '    ForeignKey',                ')'            ]            Output: 'from sqlalchemy import Column, Integer as Int, String as Str, ForeignKey'        \"\"\"        # Get the import statement prefix (everything before the first parenthesis)        prefix = import_lines[0].split('(')[0].strip()                # Process the imported items        items = []        for line in import_lines[1:]:  # Skip the first line as it's the prefix            # Remove parentheses and extra whitespace            line = line.replace('(', '').replace(')', '').strip()                        # Skip empty lines            if not line:                continue                            # If line ends with comma, remove it but remember we need one            if line.endswith(','):                line = line.rstrip(',')                if line:  # Only add non-empty items                    items.append(line)            else:                if line:  # Only add non-empty items                    items.append(line)                # Join items with commas        joined_items = ', '.join(items)                # Combine prefix with items        result = f\"{prefix} {joined_items}\"                return result       "},{"NodeName":"PythonCodebaseParser","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/python_codebase_parser.py","MultipleExtend":["CodebaseParserStrategy"],"Functions":[{"Name":"__init__","FunctionCalls":[{"FunctionName":"NodeVariablesParser","Position":{"StartLine":43,"StartLinePosition":56,"StopLine":43,"StopLinePosition":117}}],"Position":{"StartLine":34,"StartLinePosition":4,"StopLine":46,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.python_extract_inheritance","TypeType":"PythonExtractInheritance"},{"TypeValue":"self.package_naming_strategy","TypeType":"PythonPackageNamingStrategy"},{"TypeValue":"self.qualified_name_strategy","TypeType":"PythonQualifiedNameStrategy"},{"TypeValue":"self.python_import_segregation_strategy","TypeType":"PythonImportSegregationStrategy"},{"TypeValue":"self.code_confluence_tree_sitter","TypeType":"CodeConfluenceTreeSitter"},{"TypeValue":"self.python_function_calls","TypeType":"FunctionMetadataParser"},{"TypeValue":"self.sort_function_dependencies","TypeType":"SortFunctionDependencies"},{"TypeValue":"self.python_node_dependency_processor","TypeType":"PythonNodeDependencyProcessor"},{"TypeValue":"self.node_variables_parser","TypeType":"NodeVariablesParser"}],"Content":"def __init__(self):        self.python_extract_inheritance = PythonExtractInheritance()        self.package_naming_strategy = PythonPackageNamingStrategy()        self.qualified_name_strategy = PythonQualifiedNameStrategy()        self.python_import_segregation_strategy = PythonImportSegregationStrategy()        self.code_confluence_tree_sitter = CodeConfluenceTreeSitter(language=ProgrammingLanguage.PYTHON)        self.python_function_calls = FunctionMetadataParser(tree_sitter=self.code_confluence_tree_sitter)        self.sort_function_dependencies = SortFunctionDependencies()        self.python_node_dependency_processor = PythonNodeDependencyProcessor()        self.node_variables_parser = NodeVariablesParser(code_confluence_tree_sitter=self.code_confluence_tree_sitter)            # we handle procedural , class and mix of procedural and class nodes.    "},{"Name":"__preprocess_nodes","Parameters":[{"TypeValue":"json_data","TypeType":"dict"},{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"codebase_name","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"debug","Position":{"StartLine":75,"StartLinePosition":18,"StopLine":75,"StopLinePosition":37}}],"Position":{"StartLine":46,"StartLinePosition":4,"StopLine":80,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.python_extract_inheritance","TypeType":"PythonExtractInheritance"},{"TypeValue":"self.package_naming_strategy","TypeType":"PythonPackageNamingStrategy"},{"TypeValue":"self.qualified_name_strategy","TypeType":"PythonQualifiedNameStrategy"},{"TypeValue":"self.python_import_segregation_strategy","TypeType":"PythonImportSegregationStrategy"},{"TypeValue":"self.code_confluence_tree_sitter","TypeType":"CodeConfluenceTreeSitter"},{"TypeValue":"self.python_function_calls","TypeType":"FunctionMetadataParser"},{"TypeValue":"self.sort_function_dependencies","TypeType":"SortFunctionDependencies"},{"TypeValue":"self.python_node_dependency_processor","TypeType":"PythonNodeDependencyProcessor"},{"TypeValue":"self.node_variables_parser","TypeType":"NodeVariablesParser"},{"TypeValue":"file_path_nodes","TypeType":""},{"TypeValue":"qualified_names_dict","TypeType":""},{"TypeValue":"node","TypeType":""},{"TypeValue":"unoplat_node","TypeType":""},{"TypeValue":"file_path_nodes[unoplat_node.file_path]","TypeType":"[unoplat_node]"},{"TypeValue":"qualified_names_dict[unoplat_node.qualified_name]","TypeType":"unoplat_node"}],"Content":"def __preprocess_nodes(        self, json_data: dict, local_workspace_path: str,codebase_name: str    ) -> Tuple[Dict[str, List[UnoplatChapiForgeNode]],Dict[str,UnoplatChapiForgeNode]]:        \"\"\"Preprocess nodes to extract qualified names and segregate imports.\"\"\"        file_path_nodes: Dict[str, List[UnoplatChapiForgeNode]] = {}          qualified_names_dict: Dict[str,UnoplatChapiForgeNode] = {}                for item in json_data:            try:                node: ChapiNode = ChapiNode.model_validate(item)                unoplat_node: UnoplatChapiForgeNode = self.__common_node_processing(node, local_workspace_path)                                # Add debug logging                logger.debug(f\"Processing node: {node.node_name}\")                logger.debug(f\"Qualified name: {unoplat_node.qualified_name}\")                                if unoplat_node.file_path not in file_path_nodes:                    file_path_nodes[unoplat_node.file_path] = [unoplat_node] #type: ignore                else:                    file_path_nodes[unoplat_node.file_path].append(unoplat_node)                                qualified_names_dict[unoplat_node.qualified_name] = unoplat_node                                            except Exception as e:                logger.error(f\"Error building qualified name map: {e}\")                # Add debug logging for final map        logger.debug(\"Qualified names in dict:\")        for qname in qualified_names_dict.keys():            logger.debug(f\"  {qname}\")                    return file_path_nodes, qualified_names_dict                "},{"Name":"__common_node_processing","Parameters":[{"TypeValue":"node","TypeType":"ChapiNode"},{"TypeValue":"local_workspace_path","TypeType":"str"}],"FunctionCalls":[{"NodeName":"self","FunctionName":"package_naming_strategy","Position":{"StartLine":110,"StartLinePosition":31,"StopLine":110,"StopLinePosition":32}},{"NodeName":"self","FunctionName":"get_package_name","Position":{"StartLine":110,"StartLinePosition":55,"StopLine":113,"StopLinePosition":12}}],"Position":{"StartLine":80,"StartLinePosition":4,"StopLine":128,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.python_extract_inheritance","TypeType":"PythonExtractInheritance"},{"TypeValue":"self.package_naming_strategy","TypeType":"PythonPackageNamingStrategy"},{"TypeValue":"self.qualified_name_strategy","TypeType":"PythonQualifiedNameStrategy"},{"TypeValue":"self.python_import_segregation_strategy","TypeType":"PythonImportSegregationStrategy"},{"TypeValue":"self.code_confluence_tree_sitter","TypeType":"CodeConfluenceTreeSitter"},{"TypeValue":"self.python_function_calls","TypeType":"FunctionMetadataParser"},{"TypeValue":"self.sort_function_dependencies","TypeType":"SortFunctionDependencies"},{"TypeValue":"self.python_node_dependency_processor","TypeType":"PythonNodeDependencyProcessor"},{"TypeValue":"self.node_variables_parser","TypeType":"NodeVariablesParser"},{"TypeValue":"file_path_nodes","TypeType":""},{"TypeValue":"qualified_names_dict","TypeType":""},{"TypeValue":"node","TypeType":""},{"TypeValue":"unoplat_node","TypeType":""},{"TypeValue":"file_path_nodes[unoplat_node.file_path]","TypeType":"[unoplat_node]"},{"TypeValue":"qualified_names_dict[unoplat_node.qualified_name]","TypeType":"unoplat_node"},{"TypeValue":"node.node_name","TypeType":"(os.path.basename(node.file_path).split('.')[0]ifnode.file_pathelse\"unknown\")"},{"TypeValue":"qualified_name","TypeType":"self"},{"TypeValue":"imports_dict","TypeType":""},{"TypeValue":"final_internal_imports","TypeType":"self"},{"TypeValue":"imports_dict[ImportType.INTERNAL]","TypeType":"final_internal_imports"},{"TypeValue":"node.package","TypeType":"self"}],"Content":"def __common_node_processing(self, node: ChapiNode, local_workspace_path: str):        if node.node_name == \"default\":            node.node_name = (                os.path.basename(node.file_path).split('.')[0]                if node.file_path                else \"unknown\"            )                    if node.node_name and node.file_path:  # Type guard for linter            qualified_name = self.qualified_name_strategy.get_qualified_name(                node_name=node.node_name,                node_file_path=node.file_path,                local_workspace_path=local_workspace_path,                node_type=node.type            )                # segregating imports        imports_dict: Dict[ImportType, List[UnoplatImport]] = self.python_import_segregation_strategy.process_imports(node)                    # Extracting inheritance                    if imports_dict and ImportType.INTERNAL in imports_dict:            final_internal_imports = self.python_extract_inheritance.extract_inheritance(                node,                 imports_dict[ImportType.INTERNAL]            )            imports_dict[ImportType.INTERNAL] = final_internal_imports                # Todo: Add dependent nodes        if node.file_path:  # Type guard for linter            node.package = self.package_naming_strategy.get_package_name(                node.file_path,                local_workspace_path            )                # TODO: enable below when archguard fixes the formatting issues of code content - be it class or function        # node.fields = []        # if node is of type class do parse class variables and instance variables and add them to node.class_variables        # if node.type == \"CLASS\":        #     node.fields = self.node_variables_parser.parse_class_variables(node.content, node.functions)                          # if node.functions:        #     self.python_function_calls.process_functions(node.functions)                        return UnoplatChapiForgeNode.from_chapi_node(chapi_node=node, qualified_name=qualified_name,segregated_imports=imports_dict if imports_dict is not None else {})                        "},{"Name":"parse_codebase","Parameters":[{"TypeValue":"codebase_name","TypeType":"str"},{"TypeValue":"json_data","TypeType":"dict"},{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"programming_language_metadata","TypeType":"ProgrammingLanguageMetadata"}],"Position":{"StartLine":128,"StartLinePosition":4,"StopLine":217,"StopLinePosition":8},"LocalVariables":[{"TypeValue":"self.python_extract_inheritance","TypeType":"PythonExtractInheritance"},{"TypeValue":"self.package_naming_strategy","TypeType":"PythonPackageNamingStrategy"},{"TypeValue":"self.qualified_name_strategy","TypeType":"PythonQualifiedNameStrategy"},{"TypeValue":"self.python_import_segregation_strategy","TypeType":"PythonImportSegregationStrategy"},{"TypeValue":"self.code_confluence_tree_sitter","TypeType":"CodeConfluenceTreeSitter"},{"TypeValue":"self.python_function_calls","TypeType":"FunctionMetadataParser"},{"TypeValue":"self.sort_function_dependencies","TypeType":"SortFunctionDependencies"},{"TypeValue":"self.python_node_dependency_processor","TypeType":"PythonNodeDependencyProcessor"},{"TypeValue":"self.node_variables_parser","TypeType":"NodeVariablesParser"},{"TypeValue":"file_path_nodes","TypeType":""},{"TypeValue":"qualified_names_dict","TypeType":""},{"TypeValue":"node","TypeType":""},{"TypeValue":"unoplat_node","TypeType":""},{"TypeValue":"file_path_nodes[unoplat_node.file_path]","TypeType":"[unoplat_node]"},{"TypeValue":"qualified_names_dict[unoplat_node.qualified_name]","TypeType":"unoplat_node"},{"TypeValue":"node.node_name","TypeType":"(os.path.basename(node.file_path).split('.')[0]ifnode.file_pathelse\"unknown\")"},{"TypeValue":"qualified_name","TypeType":"self"},{"TypeValue":"imports_dict","TypeType":""},{"TypeValue":"final_internal_imports","TypeType":"self"},{"TypeValue":"imports_dict[ImportType.INTERNAL]","TypeType":"final_internal_imports"},{"TypeValue":"node.package","TypeType":"self"},{"TypeValue":"","TypeType":"self"},{"TypeValue":"package_manager_strategy","TypeType":"PackageManagerStrategyFactory"},{"TypeValue":"processed_metadata","TypeType":"None"},{"TypeValue":"unoplat_package_dict","TypeType":""},{"TypeValue":"content_of_file","TypeType":"ProgrammingFileReader"},{"TypeValue":"global_variables","TypeType":""},{"TypeValue":"dependent_classes","TypeType":""},{"TypeValue":"sorted_functions","TypeType":""},{"TypeValue":"node.global_variables","TypeType":"global_variables"},{"TypeValue":"node.functions","TypeType":"sorted_functions"},{"TypeValue":"node.dependent_internal_classes","TypeType":"dependent_classes"},{"TypeValue":"package_parts","TypeType":"node"},{"TypeValue":"current_package","TypeType":"current_package"},{"TypeValue":"full_package_name","TypeType":"part"},{"TypeValue":"current_package[full_package_name]","TypeType":"UnoplatPackage"},{"TypeValue":"current_package[full_package_name].nodes[file_path]","TypeType":"[]"},{"TypeValue":"unoplat_codebase","TypeType":""}],"Content":"def parse_codebase(        self, codebase_name: str, json_data: dict, local_workspace_path: str,         programming_language_metadata: ProgrammingLanguageMetadata    ) -> UnoplatCodebase:        \"\"\"Parse the entire codebase.                First preprocesses nodes to extract qualified names and segregate imports,        then processes dependencies for each node using that map.        \"\"\"        # Phase 1: Preprocess nodes        preprocessed_file_path_nodes, preprocessed_qualified_name_dict = self.__preprocess_nodes(json_data, local_workspace_path, codebase_name)                # Get package manager metadata        try:            package_manager_strategy = PackageManagerStrategyFactory.get_strategy(                programming_language_metadata.package_manager            )            processed_metadata = package_manager_strategy.process_metadata(                local_workspace_path,                 programming_language_metadata            )        except Exception as e:            logger.warning(f\"Error processing package manager metadata: {e}\")            processed_metadata = None                # Phase 2: Process dependencies using the map        unoplat_package_dict: Dict[str, UnoplatPackage] = {}                for file_path, nodes in preprocessed_file_path_nodes.items():            try:                                #TODO: enable when archguard fixes the formatting issues of code content - be it class or function                # The operation of generating global variables should be done once per file even if there are multiple nodes in the file                content_of_file = ProgrammingFileReader.read_file(file_path)                global_variables: List[ClassGlobalFieldModel] = self.node_variables_parser.parse_global_variables(content_of_file)                                # The operation of figuring out dependent class should be done once per file even if there are multiple nodes in the file                dependent_classes: List[UnoplatChapiForgeNode] = self.python_node_dependency_processor.process_dependencies(nodes[0], preprocessed_qualified_name_dict)                                # Process all nodes from file                for node in nodes:                    # TODO: check for interface and abstract class - what are types from chapi                    sorted_functions: List[UnoplatChapiForgeFunction] = self.sort_function_dependencies.sort_function_dependencies(                        functions=node.functions,                        node_type=node.type                    )                                        #TODO: enable when archguard fixes the formatting issues of code content - be it class or function                    node.global_variables = global_variables                                        if sorted_functions:                        node.functions = sorted_functions                                        # Generate dependent classes                    # skip first node since it is already processed                    if node.node_name != nodes[0].node_name:                        node.dependent_internal_classes = dependent_classes                                                                # Build package structure                    if node.package:                        package_parts = node.package.split('.')                        current_package = unoplat_package_dict                        full_package_name = \"\"                                                for i, part in enumerate(package_parts):                            full_package_name = part if i == 0 else f\"{full_package_name}.{part}\"                            if full_package_name not in current_package:                                current_package[full_package_name] = UnoplatPackage(name=full_package_name)                            if i == len(package_parts) - 1:                                # Add nodes to dict by file path                                if file_path not in current_package[full_package_name].nodes:                                    current_package[full_package_name].nodes[file_path] = []                                current_package[full_package_name].nodes[file_path].append(node)                            else:                                current_package = current_package[full_package_name].sub_packages #type: ignore            except Exception as e:                logger.error(f\"Error processing node dependencies: {e}\")               unoplat_codebase: UnoplatCodebase = UnoplatCodebase(            name=codebase_name,            packages=list(unoplat_package_dict.values())[0] if unoplat_package_dict else None,            package_manager_metadata=processed_metadata, #type: ignore            local_path=local_workspace_path        ) #type: ignore        return unoplat_codebase                        "}],"Imports":[{"Source":"unoplat_code_confluence.configuration.settings","UsageName":["ProgrammingLanguage","ProgrammingLanguageMetadata"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_class_global_fieldmodel","UsageName":["ClassGlobalFieldModel"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_node","UsageName":["ChapiNode"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_chapi_forge_function","UsageName":["UnoplatChapiForgeFunction"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_chapi_forge_node","UsageName":["UnoplatChapiForgeNode"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_codebase","UsageName":["UnoplatCodebase"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_import","UsageName":["UnoplatImport"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_import_type","UsageName":["ImportType"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_package","UsageName":["UnoplatPackage"]},{"Source":"unoplat_code_confluence.parser.codebase_parser_strategy","UsageName":["CodebaseParserStrategy"]},{"Source":"unoplat_code_confluence.parser.python.function_metadata.function_metadata_parser","UsageName":["FunctionMetadataParser"]},{"Source":"unoplat_code_confluence.parser.python.in_class_dependency.sort_function_dependencies","UsageName":["SortFunctionDependencies"]},{"Source":"unoplat_code_confluence.parser.python.node_variables.node_variables_parser","UsageName":["NodeVariablesParser"]},{"Source":"unoplat_code_confluence.parser.python.package_manager.package_manager_factory","UsageName":["PackageManagerStrategyFactory"]},{"Source":"unoplat_code_confluence.parser.python.python_extract_inheritance","UsageName":["PythonExtractInheritance"]},{"Source":"unoplat_code_confluence.parser.python.python_import_segregation_strategy","UsageName":["PythonImportSegregationStrategy"]},{"Source":"unoplat_code_confluence.parser.python.python_node_dependency_processor","UsageName":["PythonNodeDependencyProcessor"]},{"Source":"unoplat_code_confluence.parser.python.python_package_naming_strategy","UsageName":["PythonPackageNamingStrategy"]},{"Source":"unoplat_code_confluence.parser.python.python_qualified_name_strategy","UsageName":["PythonQualifiedNameStrategy"]},{"Source":"unoplat_code_confluence.parser.python.utils.read_programming_file","UsageName":["ProgrammingFileReader"]},{"Source":"unoplat_code_confluence.parser.tree_sitter.code_confluence_tree_sitter","UsageName":["CodeConfluenceTreeSitter"]},{"Source":"os"},{"Source":"typing","UsageName":["Dict","List","Tuple"]},{"Source":"loguru","UsageName":["logger"]}],"Position":{"StartLine":32,"StopLine":217,"StopLinePosition":8},"Content":"class PythonCodebaseParser(CodebaseParserStrategy):        def __init__(self):        self.python_extract_inheritance = PythonExtractInheritance()        self.package_naming_strategy = PythonPackageNamingStrategy()        self.qualified_name_strategy = PythonQualifiedNameStrategy()        self.python_import_segregation_strategy = PythonImportSegregationStrategy()        self.code_confluence_tree_sitter = CodeConfluenceTreeSitter(language=ProgrammingLanguage.PYTHON)        self.python_function_calls = FunctionMetadataParser(tree_sitter=self.code_confluence_tree_sitter)        self.sort_function_dependencies = SortFunctionDependencies()        self.python_node_dependency_processor = PythonNodeDependencyProcessor()        self.node_variables_parser = NodeVariablesParser(code_confluence_tree_sitter=self.code_confluence_tree_sitter)            # we handle procedural , class and mix of procedural and class nodes.    def __preprocess_nodes(        self, json_data: dict, local_workspace_path: str,codebase_name: str    ) -> Tuple[Dict[str, List[UnoplatChapiForgeNode]],Dict[str,UnoplatChapiForgeNode]]:        \"\"\"Preprocess nodes to extract qualified names and segregate imports.\"\"\"        file_path_nodes: Dict[str, List[UnoplatChapiForgeNode]] = {}          qualified_names_dict: Dict[str,UnoplatChapiForgeNode] = {}                for item in json_data:            try:                node: ChapiNode = ChapiNode.model_validate(item)                unoplat_node: UnoplatChapiForgeNode = self.__common_node_processing(node, local_workspace_path)                                # Add debug logging                logger.debug(f\"Processing node: {node.node_name}\")                logger.debug(f\"Qualified name: {unoplat_node.qualified_name}\")                                if unoplat_node.file_path not in file_path_nodes:                    file_path_nodes[unoplat_node.file_path] = [unoplat_node] #type: ignore                else:                    file_path_nodes[unoplat_node.file_path].append(unoplat_node)                                qualified_names_dict[unoplat_node.qualified_name] = unoplat_node                                            except Exception as e:                logger.error(f\"Error building qualified name map: {e}\")                # Add debug logging for final map        logger.debug(\"Qualified names in dict:\")        for qname in qualified_names_dict.keys():            logger.debug(f\"  {qname}\")                    return file_path_nodes, qualified_names_dict                def __common_node_processing(self, node: ChapiNode, local_workspace_path: str):        if node.node_name == \"default\":            node.node_name = (                os.path.basename(node.file_path).split('.')[0]                if node.file_path                else \"unknown\"            )                    if node.node_name and node.file_path:  # Type guard for linter            qualified_name = self.qualified_name_strategy.get_qualified_name(                node_name=node.node_name,                node_file_path=node.file_path,                local_workspace_path=local_workspace_path,                node_type=node.type            )                # segregating imports        imports_dict: Dict[ImportType, List[UnoplatImport]] = self.python_import_segregation_strategy.process_imports(node)                    # Extracting inheritance                    if imports_dict and ImportType.INTERNAL in imports_dict:            final_internal_imports = self.python_extract_inheritance.extract_inheritance(                node,                 imports_dict[ImportType.INTERNAL]            )            imports_dict[ImportType.INTERNAL] = final_internal_imports                # Todo: Add dependent nodes        if node.file_path:  # Type guard for linter            node.package = self.package_naming_strategy.get_package_name(                node.file_path,                local_workspace_path            )                # TODO: enable below when archguard fixes the formatting issues of code content - be it class or function        # node.fields = []        # if node is of type class do parse class variables and instance variables and add them to node.class_variables        # if node.type == \"CLASS\":        #     node.fields = self.node_variables_parser.parse_class_variables(node.content, node.functions)                          # if node.functions:        #     self.python_function_calls.process_functions(node.functions)                        return UnoplatChapiForgeNode.from_chapi_node(chapi_node=node, qualified_name=qualified_name,segregated_imports=imports_dict if imports_dict is not None else {})                        def parse_codebase(        self, codebase_name: str, json_data: dict, local_workspace_path: str,         programming_language_metadata: ProgrammingLanguageMetadata    ) -> UnoplatCodebase:        \"\"\"Parse the entire codebase.                First preprocesses nodes to extract qualified names and segregate imports,        then processes dependencies for each node using that map.        \"\"\"        # Phase 1: Preprocess nodes        preprocessed_file_path_nodes, preprocessed_qualified_name_dict = self.__preprocess_nodes(json_data, local_workspace_path, codebase_name)                # Get package manager metadata        try:            package_manager_strategy = PackageManagerStrategyFactory.get_strategy(                programming_language_metadata.package_manager            )            processed_metadata = package_manager_strategy.process_metadata(                local_workspace_path,                 programming_language_metadata            )        except Exception as e:            logger.warning(f\"Error processing package manager metadata: {e}\")            processed_metadata = None                # Phase 2: Process dependencies using the map        unoplat_package_dict: Dict[str, UnoplatPackage] = {}                for file_path, nodes in preprocessed_file_path_nodes.items():            try:                                #TODO: enable when archguard fixes the formatting issues of code content - be it class or function                # The operation of generating global variables should be done once per file even if there are multiple nodes in the file                content_of_file = ProgrammingFileReader.read_file(file_path)                global_variables: List[ClassGlobalFieldModel] = self.node_variables_parser.parse_global_variables(content_of_file)                                # The operation of figuring out dependent class should be done once per file even if there are multiple nodes in the file                dependent_classes: List[UnoplatChapiForgeNode] = self.python_node_dependency_processor.process_dependencies(nodes[0], preprocessed_qualified_name_dict)                                # Process all nodes from file                for node in nodes:                    # TODO: check for interface and abstract class - what are types from chapi                    sorted_functions: List[UnoplatChapiForgeFunction] = self.sort_function_dependencies.sort_function_dependencies(                        functions=node.functions,                        node_type=node.type                    )                                        #TODO: enable when archguard fixes the formatting issues of code content - be it class or function                    node.global_variables = global_variables                                        if sorted_functions:                        node.functions = sorted_functions                                        # Generate dependent classes                    # skip first node since it is already processed                    if node.node_name != nodes[0].node_name:                        node.dependent_internal_classes = dependent_classes                                                                # Build package structure                    if node.package:                        package_parts = node.package.split('.')                        current_package = unoplat_package_dict                        full_package_name = \"\"                                                for i, part in enumerate(package_parts):                            full_package_name = part if i == 0 else f\"{full_package_name}.{part}\"                            if full_package_name not in current_package:                                current_package[full_package_name] = UnoplatPackage(name=full_package_name)                            if i == len(package_parts) - 1:                                # Add nodes to dict by file path                                if file_path not in current_package[full_package_name].nodes:                                    current_package[full_package_name].nodes[file_path] = []                                current_package[full_package_name].nodes[file_path].append(node)                            else:                                current_package = current_package[full_package_name].sub_packages #type: ignore            except Exception as e:                logger.error(f\"Error processing node dependencies: {e}\")               unoplat_codebase: UnoplatCodebase = UnoplatCodebase(            name=codebase_name,            packages=list(unoplat_package_dict.values())[0] if unoplat_package_dict else None,            package_manager_metadata=processed_metadata, #type: ignore            local_path=local_workspace_path        ) #type: ignore        return unoplat_codebase                        "},{"NodeName":"NodeVariablesParser","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/node_variables/node_variables_parser.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"code_confluence_tree_sitter","TypeType":"CodeConfluenceTreeSitter"}],"FunctionCalls":[{"NodeName":"self","FunctionName":"variables_dict","Position":{"StartLine":22,"StartLinePosition":12,"StopLine":22,"StopLinePosition":13}}],"Position":{"StartLine":19,"StartLinePosition":4,"StopLine":25,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":""}],"Content":"def __init__(self, code_confluence_tree_sitter: CodeConfluenceTreeSitter):        self.parser = code_confluence_tree_sitter.get_parser()        # Change to simple string key since we no longer track scope        self.variables_dict: Dict[str, ClassGlobalFieldModel] = {}# Here the content refers to content of a file where it can contain multiple classes/procedural functions and variables        "},{"Name":"parse_global_variables","Parameters":[{"TypeValue":"content_of_file","TypeType":"str"}],"FunctionCalls":[{"NodeName":"self","FunctionName":"__traverse_global_variables","Position":{"StartLine":32,"StartLinePosition":12,"StopLine":32,"StopLinePosition":47}}],"Position":{"StartLine":25,"StartLinePosition":4,"StopLine":38,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":"{}"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"cursor","TypeType":"tree"}],"Content":"def parse_global_variables(self, content_of_file: str) -> List[ClassGlobalFieldModel]:        \"\"\"Parse variables from Python content.\"\"\"        tree = self.parser.parse(bytes(content_of_file, \"utf8\"))                self.variables_dict = {}        cursor = tree.walk()        #this is for class and global variables        self.__traverse_global_variables(cursor)            return list(self.variables_dict.values())        # It should parse class variables and instance variables defined in a class but outside of any functions based on content of class    # and then should use list of functions to check if there are any instance variables or class run variables defined in the functions and add them. Make sure there is no duplication.    "},{"Name":"parse_class_variables","Parameters":[{"TypeValue":"content_of_class_code","TypeType":"str"},{"TypeValue":"list_of_functions","TypeType":"List[ChapiFunction]"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"debug","Position":{"StartLine":82,"StartLinePosition":18,"StopLine":82,"StopLinePosition":81}}],"Position":{"StartLine":38,"StartLinePosition":4,"StopLine":86,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":"{}"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"cursor","TypeType":"tree"},{"TypeValue":"seen_variables","TypeType":"{v.class_field_nameforvinself.variables_dict.values()}"},{"TypeValue":"func_tree","TypeType":"self"},{"TypeValue":"func_cursor","TypeType":"func_tree"}],"Content":"def parse_class_variables(self, content_of_class_code: str,                              list_of_functions: List[ChapiFunction]) -> List[ClassGlobalFieldModel]:        \"\"\"Parse class-level and instance variables from class content.                First parses variables defined at class level (outside functions),        then processes function contents for instance/runtime variables.                Args:            content_of_class_code: Content of just the class definition            list_of_functions: List of functions to check for instance/runtime variables                                      Returns:            List[ClassGlobalFieldModel]: All unique class variables including:                - ClassVar variables                - Instance variables defined at class level                - Instance variables from methods (self.x)                - Runtime class variables from @classmethod (cls.x)        \"\"\"        # Phase 1: Parse class-level variables        tree = self.parser.parse(bytes(content_of_class_code, \"utf8\"))        self.variables_dict = {}  # Reset for new parsing        cursor = tree.walk()        logger.debug(\"Before class variables: {}\", len(self.variables_dict))        self.__traverse_class_variables(cursor)        logger.debug(\"After class variables: {}\", len(self.variables_dict))                # Track seen variables to avoid duplicates        seen_variables = {v.class_field_name for v in self.variables_dict.values()}        logger.debug(\"Seen variables: {}\", seen_variables)                # Phase 2: Parse variables from function contents        for function in list_of_functions:            if not function.content:                continue                        logger.debug(\"Processing function: {}\", function.name)            logger.debug(\"Function content: {}\", function.content)                        func_tree = self.parser.parse(bytes(function.content, \"utf8\"))            func_cursor = func_tree.walk()            self.__traverse_function_variables(                cursor=func_cursor,                seen_variables=seen_variables            )            logger.debug(\"Variables after function: {}\", len(self.variables_dict))                return list(self.variables_dict.values())    "},{"Name":"__traverse_global_variables","Parameters":[{"TypeValue":"cursor","TypeType":"TreeCursor"}],"Position":{"StartLine":86,"StartLinePosition":4,"StopLine":134,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":"{}"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"cursor","TypeType":"tree"},{"TypeValue":"seen_variables","TypeType":"{v.class_field_nameforvinself.variables_dict.values()}"},{"TypeValue":"func_tree","TypeType":"self"},{"TypeValue":"func_cursor","TypeType":"func_tree"},{"TypeValue":"inside_function","TypeType":"False"},{"TypeValue":"inside_class","TypeType":"False"},{"TypeValue":"current_decorators","TypeType":"[]"},{"TypeValue":"processed_decorators","TypeType":""},{"TypeValue":"should_traverse_children","TypeType":"cursor"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"assignment_node","TypeType":"node"}],"Content":"def __traverse_global_variables(self, cursor: TreeCursor) -> None:        \"\"\"Traverse AST to find only global variables (outside any class/function).\"\"\"        inside_function = False        inside_class = False        current_decorators: List[ChapiAnnotation] = []        processed_decorators: set[tuple[int, int]] = set()        # Move cursor to first child if exists        should_traverse_children = cursor.goto_first_child()                while should_traverse_children:            node = cursor.node                        # Track scope            if node.type == \"function_definition\":                inside_function = True            elif node.type == \"class_definition\":                inside_class = True                        # Process global variables            if node.type == \"expression_statement\":                if (not inside_function and not inside_class and                     node.children and node.children[0].type == \"assignment\"):                    assignment_node = node.children[0]                    self.__process_assignment(assignment_node, current_decorators)                    current_decorators = []                    processed_decorators.clear()                        # Try to move to next sibling            if cursor.goto_next_sibling():                # If we moved to sibling, check if we're exiting a scope                if inside_function and node.type == \"function_definition\":                    inside_function = False                elif inside_class and node.type == \"class_definition\":                    inside_class = False                continue                        # No more siblings, go back to parent            if not cursor.goto_parent():                # We've reached the root, stop traversal                break                        # Reset scope flags when exiting their definitions            if inside_function and node.type == \"function_definition\":                inside_function = False            elif inside_class and node.type == \"class_definition\":                inside_class = False            "},{"Name":"__traverse_class_variables","Parameters":[{"TypeValue":"cursor","TypeType":"TreeCursor"}],"Position":{"StartLine":134,"StartLinePosition":4,"StopLine":188,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":"{}"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"cursor","TypeType":"tree"},{"TypeValue":"seen_variables","TypeType":"{v.class_field_nameforvinself.variables_dict.values()}"},{"TypeValue":"func_tree","TypeType":"self"},{"TypeValue":"func_cursor","TypeType":"func_tree"},{"TypeValue":"inside_function","TypeType":"True"},{"TypeValue":"inside_class","TypeType":"False"},{"TypeValue":"current_decorators","TypeType":"[]"},{"TypeValue":"processed_decorators","TypeType":""},{"TypeValue":"should_traverse_children","TypeType":"cursor"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"assignment_node","TypeType":"node"},{"TypeValue":"annotation","TypeType":"self"}],"Content":"def __traverse_class_variables(self, cursor: TreeCursor) -> None:        \"\"\"Parse variables defined at class level (outside any function).\"\"\"        inside_function = False        current_decorators: List[ChapiAnnotation] = []        # Move down the tree until we find a class_definition node        while cursor.node.type != \"class_definition\":            if not cursor.goto_first_child():                return        # Enter class_definition internals        if not cursor.goto_first_child():            return        # Skip until we hit 'block' (class body)        while cursor.node.type != \"block\":            if not cursor.goto_next_sibling():                return        # Enter the block        if not cursor.goto_first_child():            return        while True:            # Update node at the start of each iteration            node = cursor.node            # If we were inside a function previously, check if we've left it            if inside_function and node.type != \"function_definition\":                # Moved on from function definition node, reset inside_function                inside_function = False            if node.type == \"function_definition\":                inside_function = True            elif node.type == \"decorator\" and not inside_function:                annotation = self.__get_annotation(node)                if annotation:                    current_decorators.append(annotation)            elif not inside_function and node.type == \"expression_statement\":                if node.children and node.children[0].type == \"assignment\":                    self.__process_assignment(node.children[0], current_decorators)                    current_decorators = []            # Move to next sibling if possible            if cursor.goto_next_sibling():                # Don't update node here, rely on next iteration's node = cursor.node                continue            break  # No more siblings           "},{"Name":"__traverse_function_variables","Parameters":[{"TypeValue":"cursor","TypeType":"TreeCursor"},{"TypeValue":"seen_variables","TypeType":"set[str]"}],"FunctionCalls":[{"NodeName":"self","FunctionName":"__process_assignment","Position":{"StartLine":238,"StartLinePosition":36,"StopLine":238,"StopLinePosition":73}}],"Position":{"StartLine":188,"StartLinePosition":4,"StopLine":245,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":"{}"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"cursor","TypeType":"tree"},{"TypeValue":"seen_variables","TypeType":"{v.class_field_nameforvinself.variables_dict.values()}"},{"TypeValue":"func_tree","TypeType":"self"},{"TypeValue":"func_cursor","TypeType":"func_tree"},{"TypeValue":"inside_function","TypeType":"True"},{"TypeValue":"inside_class","TypeType":"False"},{"TypeValue":"current_decorators","TypeType":"[]"},{"TypeValue":"processed_decorators","TypeType":""},{"TypeValue":"should_traverse_children","TypeType":"cursor"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"assignment_node","TypeType":"node"},{"TypeValue":"annotation","TypeType":"self"},{"TypeValue":"first_child","TypeType":"node"},{"TypeValue":"lhs","TypeType":"first_child"},{"TypeValue":"attr_text","TypeType":"lhs"},{"TypeValue":"var_name","TypeType":"attr_text"}],"Content":"def __traverse_function_variables(self, cursor: TreeCursor, seen_variables: set[str]) -> None:        \"\"\"Parse variables from function body.\"\"\"        # Step 1: Navigate to function_definition or decorated_definition        while cursor.node.type not in [\"function_definition\", \"decorated_definition\"]:            if not cursor.goto_first_child():                logger.error(\"Failed to find function/decorated definition\")                return        # If decorated, move to the function_definition within        if cursor.node.type == \"decorated_definition\":            if not cursor.goto_first_child():                return            while cursor.node.type != \"function_definition\":                if not cursor.goto_next_sibling():                    return        # Step 2: Move down into the function_definition's children        if not cursor.goto_first_child():            logger.error(\"Failed to enter function internals\")            return        # Step 3: Iterate siblings until we find the block node        # The block node is the actual function body.        while cursor.node.type != \"block\":            if not cursor.goto_next_sibling():                logger.error(\"Failed to find block\")                return        # Now cursor.node is 'block', enter it        if not cursor.goto_first_child():            logger.error(\"Failed to enter block contents\")            return        # Step 4: We are now inside the function body. Iterate over statements.        while True:            node = cursor.node            if node.type == \"expression_statement\" and node.children:                first_child = node.children[0]                if first_child.type == \"assignment\":                    lhs = first_child.children[0] if first_child.children else None                    if lhs and lhs.type == \"attribute\":                        attr_text = lhs.text.decode('utf8')                        # Check for instance/class variable assignments (self.x or cls.x)                        if attr_text.startswith((\"self.\", \"cls.\")):                            var_name = attr_text.split('.', 1)[1]                            if var_name not in seen_variables:                                seen_variables.add(var_name)                                # Process the assignment as needed                                self.__process_assignment(first_child, [])                                # If needed, rename the variable in self.variables_dict here            if not cursor.goto_next_sibling():                break  # No more siblings in block                        "},{"Name":"__get_annotation","Parameters":[{"TypeValue":"node","TypeType":"Node"}],"FunctionCalls":[{"NodeName":"self","FunctionName":"__extract_annotation_arguments","Position":{"StartLine":261,"StartLinePosition":37,"StopLine":261,"StopLinePosition":74}}],"Position":{"StartLine":245,"StartLinePosition":4,"StopLine":268,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":"{}"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"cursor","TypeType":"tree"},{"TypeValue":"seen_variables","TypeType":"{v.class_field_nameforvinself.variables_dict.values()}"},{"TypeValue":"func_tree","TypeType":"self"},{"TypeValue":"func_cursor","TypeType":"func_tree"},{"TypeValue":"inside_function","TypeType":"True"},{"TypeValue":"inside_class","TypeType":"False"},{"TypeValue":"current_decorators","TypeType":"[]"},{"TypeValue":"processed_decorators","TypeType":""},{"TypeValue":"should_traverse_children","TypeType":"cursor"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"assignment_node","TypeType":"node"},{"TypeValue":"annotation","TypeType":"self"},{"TypeValue":"first_child","TypeType":"node"},{"TypeValue":"lhs","TypeType":"first_child"},{"TypeValue":"attr_text","TypeType":"lhs"},{"TypeValue":"var_name","TypeType":"attr_text"},{"TypeValue":"func_name","TypeType":"call_child"},{"TypeValue":"key_values","TypeType":"self"}],"Content":"def __get_annotation(self, node: Node) -> Optional[ChapiAnnotation]:        \"\"\"Extract annotation from decorator node.\"\"\"        # Skip @ symbol        for child in node.children:            if child.type == \"identifier\":                # Simple decorator without arguments                return ChapiAnnotation(Name=child.text.decode('utf8'))            elif child.type == \"call\":                # Decorator with arguments                func_name = None                for call_child in child.children:                    if call_child.type == \"identifier\":                        func_name = call_child.text.decode('utf8')                        break                                if func_name:                    key_values = self.__extract_annotation_arguments(child)                    return ChapiAnnotation(                        Name=func_name,                        KeyValues=key_values if key_values else None                    )        return None    "},{"Name":"__extract_annotation_arguments","Parameters":[{"TypeValue":"call_node","TypeType":"Node"}],"Position":{"StartLine":268,"StartLinePosition":4,"StopLine":309,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":"{}"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"cursor","TypeType":"tree"},{"TypeValue":"seen_variables","TypeType":"{v.class_field_nameforvinself.variables_dict.values()}"},{"TypeValue":"func_tree","TypeType":"self"},{"TypeValue":"func_cursor","TypeType":"func_tree"},{"TypeValue":"inside_function","TypeType":"True"},{"TypeValue":"inside_class","TypeType":"False"},{"TypeValue":"current_decorators","TypeType":"[]"},{"TypeValue":"processed_decorators","TypeType":""},{"TypeValue":"should_traverse_children","TypeType":"cursor"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"assignment_node","TypeType":"node"},{"TypeValue":"annotation","TypeType":"self"},{"TypeValue":"first_child","TypeType":"node"},{"TypeValue":"lhs","TypeType":"first_child"},{"TypeValue":"attr_text","TypeType":"lhs"},{"TypeValue":"var_name","TypeType":"attr_text"},{"TypeValue":"func_name","TypeType":"call_child"},{"TypeValue":"key_values","TypeType":""},{"TypeValue":"pos_arg_index","TypeType":""},{"TypeValue":"key","TypeType":"arg"},{"TypeValue":"value","TypeType":"arg"}],"Content":"def __extract_annotation_arguments(self, call_node: Node) -> List[ChapiAnnotationKeyVal]:        \"\"\"Extract arguments from a decorator call node.                Examples:            @decorator(1, x=2)           -> [(\"0\", \"1\"), (\"x\", \"2\")]            @decorator(\"str\", y=\"test\")  -> [(\"0\", \"\\\"str\\\"\"), (\"y\", \"\\\"test\\\"\")]            @decorator(key=\"value\")      -> [(\"key\", \"\\\"value\\\"\")]            @decorator(1, 2, 3)          -> [(\"0\", \"1\"), (\"1\", \"2\"), (\"2\", \"3\")]        \"\"\"        key_values: List[ChapiAnnotationKeyVal] = []                # Find argument_list node        for child in call_node.children:            if child.type == \"argument_list\":                pos_arg_index = 0                                # Process each argument                for arg in child.children:                    if arg.type == \"keyword_argument\":                        # Handle key=value style arguments                        for i, kw_child in enumerate(arg.children):                            if kw_child.type == \"=\":                                # Get key (everything before =)                                key = arg.children[i-1].text.decode('utf8')                                # Get value (everything after =)                                value = arg.children[i+1].text.decode('utf8')                                key_values.append(ChapiAnnotationKeyVal(                                    Key=key,                                    Value=value                                ))                                break                    elif arg.type not in [\"(\", \")\", \",\"]:                        # Handle positional arguments                        key_values.append(ChapiAnnotationKeyVal(                            Key=str(pos_arg_index),                            Value=arg.text.decode('utf8')                        ))                        pos_arg_index += 1                                return key_values    "},{"Name":"__process_assignment","Parameters":[{"TypeValue":"node","TypeType":"Node"},{"TypeValue":"decorators","TypeType":"List[ChapiAnnotation]"}],"Position":{"StartLine":309,"StartLinePosition":4,"StopLine":392,"StopLinePosition":47},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":"{}"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"cursor","TypeType":"tree"},{"TypeValue":"seen_variables","TypeType":"{v.class_field_nameforvinself.variables_dict.values()}"},{"TypeValue":"func_tree","TypeType":"self"},{"TypeValue":"func_cursor","TypeType":"func_tree"},{"TypeValue":"inside_function","TypeType":"True"},{"TypeValue":"inside_class","TypeType":"False"},{"TypeValue":"current_decorators","TypeType":"[]"},{"TypeValue":"processed_decorators","TypeType":""},{"TypeValue":"should_traverse_children","TypeType":"cursor"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"assignment_node","TypeType":"node"},{"TypeValue":"annotation","TypeType":"self"},{"TypeValue":"first_child","TypeType":"node"},{"TypeValue":"lhs","TypeType":"node"},{"TypeValue":"attr_text","TypeType":"lhs"},{"TypeValue":"var_name","TypeType":"attr_text"},{"TypeValue":"func_name","TypeType":"call_child"},{"TypeValue":"key_values","TypeType":""},{"TypeValue":"pos_arg_index","TypeType":""},{"TypeValue":"key","TypeType":"arg"},{"TypeValue":"value","TypeType":"values"},{"TypeValue":"var_names","TypeType":"[]"},{"TypeValue":"start_point","TypeType":"var_name_node"},{"TypeValue":"end_point","TypeType":"var_name_node"},{"TypeValue":"prefix","TypeType":"lhs"},{"TypeValue":"var_name_node","TypeType":"lhs"},{"TypeValue":"type_hint","TypeType":"type_node"},{"TypeValue":"values","TypeType":"[]"},{"TypeValue":"type_node","TypeType":"node"},{"TypeValue":"value_node","TypeType":"node"},{"TypeValue":"position","TypeType":"Position"},{"TypeValue":"var","TypeType":"ClassGlobalFieldModel"},{"TypeValue":"self.variables_dict[var_name]","TypeType":"var"}],"Content":"def __process_assignment(self, node: Node, decorators: List[ChapiAnnotation]) -> None:        \"\"\"Process an assignment node to extract variable information.        Only captures variables on their first occurrence.\"\"\"        if not node.children:            return                lhs = node.children[0]  # Left-hand side of assignment                # Get variable names based on node type        var_names = []        if lhs.type == \"identifier\":            var_names.append(lhs.text.decode('utf8'))            start_point = lhs.start_point            end_point = lhs.end_point        elif lhs.type == \"pattern_list\":            # Handle tuple unpacking (x, y = ...)            for pattern_child in lhs.children:                if pattern_child.type == \"identifier\":                    var_names.append(pattern_child.text.decode('utf8'))                    # For tuple unpacking, use each identifier's position                    start_point = pattern_child.start_point                    end_point = pattern_child.end_point        elif lhs.type == \"attribute\":            # Handle self.x or cls.x attributes            if len(lhs.children) >= 3:  # Need at least 3 parts: self/cls, ., var_name                prefix = lhs.children[0].text.decode('utf8')                if prefix in (\"self\", \"cls\"):                    var_name_node = lhs.children[2]                    var_names.append(var_name_node.text.decode('utf8'))                    # For attributes, use the variable part's position                    start_point = var_name_node.start_point                    end_point = var_name_node.end_point                if not var_names:            return                # Get type hint if present        type_hint = None        values = []                # Look for type hint and values        for i, child in enumerate(node.children):            if child.type == \":\" and i + 1 < len(node.children):                type_node = node.children[i + 1]                type_hint = type_node.text.decode('utf8')            elif child.type == \"=\" and i + 1 < len(node.children):                value_node = node.children[i + 1]                if value_node.type == \"expression_list\":                    # Handle tuple values (1, 2)                    for value_child in value_node.children:                        if value_child.type not in [\",\", \"(\", \")\"]:                            values.append(value_child.text.decode('utf8').strip())                else:                    # Single value - normalize dictionary/object literals                    value = value_node.text.decode('utf8')                    if value_node.type == \"dictionary\":                        # Remove newlines and extra whitespace                        value = ''.join(value.split())                    values.append(value)                # Create variables for each name-value pair, but only if not already captured        for i, var_name in enumerate(var_names):            # Skip if variable already exists            if var_name in self.variables_dict:                continue                        value = values[i] if i < len(values) else None                        # Create position object            position = Position(                StartLine=start_point[0],  # Convert to 1-based line numbers                StartLinePosition=start_point[1],                StopLine=end_point[0],                StopLinePosition=end_point[1]            )                        var = ClassGlobalFieldModel(                TypeValue=var_name,                TypeType=type_hint,                DefaultValue=value,                Annotations=decorators if decorators else None,                Position=position  # Add position information            )            self.variables_dict[var_name] = var"}],"Imports":[{"Source":"unoplat_code_confluence.data_models.chapi.chapi_annotation","UsageName":["ChapiAnnotation","ChapiAnnotationKeyVal"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_class_global_fieldmodel","UsageName":["ClassGlobalFieldModel"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_function","UsageName":["ChapiFunction"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_position","UsageName":["Position"]},{"Source":"unoplat_code_confluence.parser.tree_sitter.code_confluence_tree_sitter","UsageName":["CodeConfluenceTreeSitter"]},{"Source":"typing","UsageName":["Dict","List","Optional"]},{"Source":"loguru","UsageName":["logger"]},{"Source":"tree_sitter","UsageName":["Node","TreeCursor"]}],"Position":{"StartLine":17,"StopLine":392,"StopLinePosition":47},"Content":"class NodeVariablesParser:        def __init__(self, code_confluence_tree_sitter: CodeConfluenceTreeSitter):        self.parser = code_confluence_tree_sitter.get_parser()        # Change to simple string key since we no longer track scope        self.variables_dict: Dict[str, ClassGlobalFieldModel] = {}# Here the content refers to content of a file where it can contain multiple classes/procedural functions and variables        def parse_global_variables(self, content_of_file: str) -> List[ClassGlobalFieldModel]:        \"\"\"Parse variables from Python content.\"\"\"        tree = self.parser.parse(bytes(content_of_file, \"utf8\"))                self.variables_dict = {}        cursor = tree.walk()        #this is for class and global variables        self.__traverse_global_variables(cursor)            return list(self.variables_dict.values())        # It should parse class variables and instance variables defined in a class but outside of any functions based on content of class    # and then should use list of functions to check if there are any instance variables or class run variables defined in the functions and add them. Make sure there is no duplication.    def parse_class_variables(self, content_of_class_code: str,                              list_of_functions: List[ChapiFunction]) -> List[ClassGlobalFieldModel]:        \"\"\"Parse class-level and instance variables from class content.                First parses variables defined at class level (outside functions),        then processes function contents for instance/runtime variables.                Args:            content_of_class_code: Content of just the class definition            list_of_functions: List of functions to check for instance/runtime variables                                      Returns:            List[ClassGlobalFieldModel]: All unique class variables including:                - ClassVar variables                - Instance variables defined at class level                - Instance variables from methods (self.x)                - Runtime class variables from @classmethod (cls.x)        \"\"\"        # Phase 1: Parse class-level variables        tree = self.parser.parse(bytes(content_of_class_code, \"utf8\"))        self.variables_dict = {}  # Reset for new parsing        cursor = tree.walk()        logger.debug(\"Before class variables: {}\", len(self.variables_dict))        self.__traverse_class_variables(cursor)        logger.debug(\"After class variables: {}\", len(self.variables_dict))                # Track seen variables to avoid duplicates        seen_variables = {v.class_field_name for v in self.variables_dict.values()}        logger.debug(\"Seen variables: {}\", seen_variables)                # Phase 2: Parse variables from function contents        for function in list_of_functions:            if not function.content:                continue                        logger.debug(\"Processing function: {}\", function.name)            logger.debug(\"Function content: {}\", function.content)                        func_tree = self.parser.parse(bytes(function.content, \"utf8\"))            func_cursor = func_tree.walk()            self.__traverse_function_variables(                cursor=func_cursor,                seen_variables=seen_variables            )            logger.debug(\"Variables after function: {}\", len(self.variables_dict))                return list(self.variables_dict.values())    def __traverse_global_variables(self, cursor: TreeCursor) -> None:        \"\"\"Traverse AST to find only global variables (outside any class/function).\"\"\"        inside_function = False        inside_class = False        current_decorators: List[ChapiAnnotation] = []        processed_decorators: set[tuple[int, int]] = set()        # Move cursor to first child if exists        should_traverse_children = cursor.goto_first_child()                while should_traverse_children:            node = cursor.node                        # Track scope            if node.type == \"function_definition\":                inside_function = True            elif node.type == \"class_definition\":                inside_class = True                        # Process global variables            if node.type == \"expression_statement\":                if (not inside_function and not inside_class and                     node.children and node.children[0].type == \"assignment\"):                    assignment_node = node.children[0]                    self.__process_assignment(assignment_node, current_decorators)                    current_decorators = []                    processed_decorators.clear()                        # Try to move to next sibling            if cursor.goto_next_sibling():                # If we moved to sibling, check if we're exiting a scope                if inside_function and node.type == \"function_definition\":                    inside_function = False                elif inside_class and node.type == \"class_definition\":                    inside_class = False                continue                        # No more siblings, go back to parent            if not cursor.goto_parent():                # We've reached the root, stop traversal                break                        # Reset scope flags when exiting their definitions            if inside_function and node.type == \"function_definition\":                inside_function = False            elif inside_class and node.type == \"class_definition\":                inside_class = False            def __traverse_class_variables(self, cursor: TreeCursor) -> None:        \"\"\"Parse variables defined at class level (outside any function).\"\"\"        inside_function = False        current_decorators: List[ChapiAnnotation] = []        # Move down the tree until we find a class_definition node        while cursor.node.type != \"class_definition\":            if not cursor.goto_first_child():                return        # Enter class_definition internals        if not cursor.goto_first_child():            return        # Skip until we hit 'block' (class body)        while cursor.node.type != \"block\":            if not cursor.goto_next_sibling():                return        # Enter the block        if not cursor.goto_first_child():            return        while True:            # Update node at the start of each iteration            node = cursor.node            # If we were inside a function previously, check if we've left it            if inside_function and node.type != \"function_definition\":                # Moved on from function definition node, reset inside_function                inside_function = False            if node.type == \"function_definition\":                inside_function = True            elif node.type == \"decorator\" and not inside_function:                annotation = self.__get_annotation(node)                if annotation:                    current_decorators.append(annotation)            elif not inside_function and node.type == \"expression_statement\":                if node.children and node.children[0].type == \"assignment\":                    self.__process_assignment(node.children[0], current_decorators)                    current_decorators = []            # Move to next sibling if possible            if cursor.goto_next_sibling():                # Don't update node here, rely on next iteration's node = cursor.node                continue            break  # No more siblings           def __traverse_function_variables(self, cursor: TreeCursor, seen_variables: set[str]) -> None:        \"\"\"Parse variables from function body.\"\"\"        # Step 1: Navigate to function_definition or decorated_definition        while cursor.node.type not in [\"function_definition\", \"decorated_definition\"]:            if not cursor.goto_first_child():                logger.error(\"Failed to find function/decorated definition\")                return        # If decorated, move to the function_definition within        if cursor.node.type == \"decorated_definition\":            if not cursor.goto_first_child():                return            while cursor.node.type != \"function_definition\":                if not cursor.goto_next_sibling():                    return        # Step 2: Move down into the function_definition's children        if not cursor.goto_first_child():            logger.error(\"Failed to enter function internals\")            return        # Step 3: Iterate siblings until we find the block node        # The block node is the actual function body.        while cursor.node.type != \"block\":            if not cursor.goto_next_sibling():                logger.error(\"Failed to find block\")                return        # Now cursor.node is 'block', enter it        if not cursor.goto_first_child():            logger.error(\"Failed to enter block contents\")            return        # Step 4: We are now inside the function body. Iterate over statements.        while True:            node = cursor.node            if node.type == \"expression_statement\" and node.children:                first_child = node.children[0]                if first_child.type == \"assignment\":                    lhs = first_child.children[0] if first_child.children else None                    if lhs and lhs.type == \"attribute\":                        attr_text = lhs.text.decode('utf8')                        # Check for instance/class variable assignments (self.x or cls.x)                        if attr_text.startswith((\"self.\", \"cls.\")):                            var_name = attr_text.split('.', 1)[1]                            if var_name not in seen_variables:                                seen_variables.add(var_name)                                # Process the assignment as needed                                self.__process_assignment(first_child, [])                                # If needed, rename the variable in self.variables_dict here            if not cursor.goto_next_sibling():                break  # No more siblings in block                        def __get_annotation(self, node: Node) -> Optional[ChapiAnnotation]:        \"\"\"Extract annotation from decorator node.\"\"\"        # Skip @ symbol        for child in node.children:            if child.type == \"identifier\":                # Simple decorator without arguments                return ChapiAnnotation(Name=child.text.decode('utf8'))            elif child.type == \"call\":                # Decorator with arguments                func_name = None                for call_child in child.children:                    if call_child.type == \"identifier\":                        func_name = call_child.text.decode('utf8')                        break                                if func_name:                    key_values = self.__extract_annotation_arguments(child)                    return ChapiAnnotation(                        Name=func_name,                        KeyValues=key_values if key_values else None                    )        return None    def __extract_annotation_arguments(self, call_node: Node) -> List[ChapiAnnotationKeyVal]:        \"\"\"Extract arguments from a decorator call node.                Examples:            @decorator(1, x=2)           -> [(\"0\", \"1\"), (\"x\", \"2\")]            @decorator(\"str\", y=\"test\")  -> [(\"0\", \"\\\"str\\\"\"), (\"y\", \"\\\"test\\\"\")]            @decorator(key=\"value\")      -> [(\"key\", \"\\\"value\\\"\")]            @decorator(1, 2, 3)          -> [(\"0\", \"1\"), (\"1\", \"2\"), (\"2\", \"3\")]        \"\"\"        key_values: List[ChapiAnnotationKeyVal] = []                # Find argument_list node        for child in call_node.children:            if child.type == \"argument_list\":                pos_arg_index = 0                                # Process each argument                for arg in child.children:                    if arg.type == \"keyword_argument\":                        # Handle key=value style arguments                        for i, kw_child in enumerate(arg.children):                            if kw_child.type == \"=\":                                # Get key (everything before =)                                key = arg.children[i-1].text.decode('utf8')                                # Get value (everything after =)                                value = arg.children[i+1].text.decode('utf8')                                key_values.append(ChapiAnnotationKeyVal(                                    Key=key,                                    Value=value                                ))                                break                    elif arg.type not in [\"(\", \")\", \",\"]:                        # Handle positional arguments                        key_values.append(ChapiAnnotationKeyVal(                            Key=str(pos_arg_index),                            Value=arg.text.decode('utf8')                        ))                        pos_arg_index += 1                                return key_values    def __process_assignment(self, node: Node, decorators: List[ChapiAnnotation]) -> None:        \"\"\"Process an assignment node to extract variable information.        Only captures variables on their first occurrence.\"\"\"        if not node.children:            return                lhs = node.children[0]  # Left-hand side of assignment                # Get variable names based on node type        var_names = []        if lhs.type == \"identifier\":            var_names.append(lhs.text.decode('utf8'))            start_point = lhs.start_point            end_point = lhs.end_point        elif lhs.type == \"pattern_list\":            # Handle tuple unpacking (x, y = ...)            for pattern_child in lhs.children:                if pattern_child.type == \"identifier\":                    var_names.append(pattern_child.text.decode('utf8'))                    # For tuple unpacking, use each identifier's position                    start_point = pattern_child.start_point                    end_point = pattern_child.end_point        elif lhs.type == \"attribute\":            # Handle self.x or cls.x attributes            if len(lhs.children) >= 3:  # Need at least 3 parts: self/cls, ., var_name                prefix = lhs.children[0].text.decode('utf8')                if prefix in (\"self\", \"cls\"):                    var_name_node = lhs.children[2]                    var_names.append(var_name_node.text.decode('utf8'))                    # For attributes, use the variable part's position                    start_point = var_name_node.start_point                    end_point = var_name_node.end_point                if not var_names:            return                # Get type hint if present        type_hint = None        values = []                # Look for type hint and values        for i, child in enumerate(node.children):            if child.type == \":\" and i + 1 < len(node.children):                type_node = node.children[i + 1]                type_hint = type_node.text.decode('utf8')            elif child.type == \"=\" and i + 1 < len(node.children):                value_node = node.children[i + 1]                if value_node.type == \"expression_list\":                    # Handle tuple values (1, 2)                    for value_child in value_node.children:                        if value_child.type not in [\",\", \"(\", \")\"]:                            values.append(value_child.text.decode('utf8').strip())                else:                    # Single value - normalize dictionary/object literals                    value = value_node.text.decode('utf8')                    if value_node.type == \"dictionary\":                        # Remove newlines and extra whitespace                        value = ''.join(value.split())                    values.append(value)                # Create variables for each name-value pair, but only if not already captured        for i, var_name in enumerate(var_names):            # Skip if variable already exists            if var_name in self.variables_dict:                continue                        value = values[i] if i < len(values) else None                        # Create position object            position = Position(                StartLine=start_point[0],  # Convert to 1-based line numbers                StartLinePosition=start_point[1],                StopLine=end_point[0],                StopLinePosition=end_point[1]            )                        var = ClassGlobalFieldModel(                TypeValue=var_name,                TypeType=type_hint,                DefaultValue=value,                Annotations=decorators if decorators else None,                Position=position  # Add position information            )            self.variables_dict[var_name] = var"},{"NodeName":"PythonQualifiedNameStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/python_qualified_name_strategy.py","Functions":[{"Name":"get_qualified_name","Parameters":[{"TypeValue":"node_name","TypeType":"str"},{"TypeValue":"node_file_path","TypeType":"str"},{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"node_type","TypeType":"str"}],"FunctionCalls":[{"NodeName":"'.'","FunctionName":"join","Position":{"StartLine":33,"StartLinePosition":28,"StopLine":33,"StopLinePosition":44}}],"Position":{"StartLine":6,"StartLinePosition":4,"StopLine":35,"StopLinePosition":30},"LocalVariables":[{"TypeValue":"relative_path","TypeType":"os"},{"TypeValue":"path_without_ext","TypeType":"os"},{"TypeValue":"path_parts","TypeType":"path_without_ext"},{"TypeValue":"qualified_name","TypeType":"'.'"}],"Content":"def get_qualified_name(self, node_name: str, node_file_path: str, local_workspace_path: str,node_type: str) -> str:        \"\"\"Get qualified name for Python nodes.                For Python, the qualified name follows the format:        package.subpackage.classname        where classname is the actual node_name rather than the filename                Args:            node_name: The actual name of the class/node            node_file_path: Path to the node's file            local_workspace_path: Base workspace path                    Returns:            str: Qualified name in format package.subpackage.node_name        \"\"\"        # Get relative path and remove .py extension        relative_path = os.path.relpath(node_file_path, local_workspace_path)        path_without_ext = os.path.splitext(relative_path)[0]                # Split the path into parts        path_parts = path_without_ext.replace('/', '.').replace('\\\\', '.').split('.')                # Replace the last part (filename) with the actual node name        if node_type == \"CLASS\":            path_parts.append(node_name)                # Join the path parts without adding codebase_name since it's already in the path        qualified_name = '.'.join(path_parts)                return qualified_name "}],"Imports":[{"Source":"os"}],"Position":{"StartLine":5,"StopLine":35,"StopLinePosition":30},"Content":"class PythonQualifiedNameStrategy:    def get_qualified_name(self, node_name: str, node_file_path: str, local_workspace_path: str,node_type: str) -> str:        \"\"\"Get qualified name for Python nodes.                For Python, the qualified name follows the format:        package.subpackage.classname        where classname is the actual node_name rather than the filename                Args:            node_name: The actual name of the class/node            node_file_path: Path to the node's file            local_workspace_path: Base workspace path                    Returns:            str: Qualified name in format package.subpackage.node_name        \"\"\"        # Get relative path and remove .py extension        relative_path = os.path.relpath(node_file_path, local_workspace_path)        path_without_ext = os.path.splitext(relative_path)[0]                # Split the path into parts        path_parts = path_without_ext.replace('/', '.').replace('\\\\', '.').split('.')                # Replace the last part (filename) with the actual node name        if node_type == \"CLASS\":            path_parts.append(node_name)                # Join the path parts without adding codebase_name since it's already in the path        qualified_name = '.'.join(path_parts)                return qualified_name "},{"NodeName":"SortFunctionDependencies","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/in_class_dependency/sort_function_dependencies.py","Functions":[{"Name":"sort_function_dependencies","Parameters":[{"TypeValue":"functions","TypeType":"List[UnoplatChapiForgeFunction]"},{"TypeValue":"node_type","TypeType":"str"}],"Position":{"StartLine":11,"StartLinePosition":4,"StopLine":18,"StopLinePosition":4},"Content":"def sort_function_dependencies(self,functions: List[UnoplatChapiForgeFunction],node_type: str) -> List[UnoplatChapiForgeFunction]:        if node_type and node_type == \"CLASS\":            return self.__sort_function_dependencies_for_class(functions=functions)        else:            return self.__sort_function_dependencies_for_procedural(functions=functions)                    "},{"Name":"__build_dependency_graph_for_class","Parameters":[{"TypeValue":"functions","TypeType":"List[UnoplatChapiForgeFunction]"}],"FunctionCalls":[{"FunctionName":"in_degree","Position":{"StartLine":47,"StartLinePosition":29,"StopLine":47,"StopLinePosition":39}}],"Position":{"StartLine":18,"StartLinePosition":4,"StopLine":52,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"graph","TypeType":""},{"TypeValue":"in_degree","TypeType":""},{"TypeValue":"function_name_map","TypeType":"{func.name:funcforfuncinfunctionsiffunc.nameisnotNone}"},{"TypeValue":"in_degree[func.name]","TypeType":""}],"Content":"def __build_dependency_graph_for_class(self, functions: List[UnoplatChapiForgeFunction]) -> tuple[Dict[str, Set[str]], Dict[str, int]]:        # Create adjacency list and in-degree count        graph: Dict[str, Set[str]] = defaultdict(set)        in_degree: Dict[str, int] = defaultdict(int)        function_name_map = {func.name: func for func in functions if func.name is not None}                # Initialize in-degree for all functions        for func in functions:            if func.name is not None:                in_degree[func.name] = 0                # Build the graph        for func in functions:            if not func.function_calls or func.name is None:                continue                            for call in func.function_calls:                # Skip self-dependencies                if call.function_name == func.name:                    continue  # Skip adding an edge from a function to itself                                # Check if it's an internal class method call                if call.function_name is not None and (                    call.node_name in (\"self\", \"cls\") or                     not call.node_name                ) and call.function_name in function_name_map:                    # Add edge from called function to calling function                    # This means: called_function must be processed before calling_function                    graph[call.function_name].add(func.name)                    in_degree[func.name] += 1                # Convert defaultdict to regular dict before returning        return dict(graph), dict(in_degree)            "},{"Name":"__sort_function_dependencies_for_class","Parameters":[{"TypeValue":"functions","TypeType":"List[UnoplatChapiForgeFunction]"}],"FunctionCalls":[{"NodeName":"queue","FunctionName":"append","Position":{"StartLine":78,"StartLinePosition":25,"StopLine":78,"StopLinePosition":41}}],"Position":{"StartLine":52,"StartLinePosition":4,"StopLine":88,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"graph","TypeType":""},{"TypeValue":"in_degree","TypeType":""},{"TypeValue":"function_name_map","TypeType":"{func.name:funcforfuncinfunctionsiffunc.nameisnotNone}"},{"TypeValue":"in_degree[func.name]","TypeType":""},{"TypeValue":"","TypeType":"self"},{"TypeValue":"queue","TypeType":""},{"TypeValue":"sorted_names","TypeType":""},{"TypeValue":"function_map","TypeType":"{func.name:funcforfuncinfunctionsiffunc.nameisnotNone}"},{"TypeValue":"current","TypeType":"queue"},{"TypeValue":"in_degree[neighbor]","TypeType":""}],"Content":"def __sort_function_dependencies_for_class(self, functions: List[UnoplatChapiForgeFunction]) -> List[UnoplatChapiForgeFunction]:        if not functions:            return []                    # Build dependency graph        graph, in_degree = self.__build_dependency_graph_for_class(functions)                # Initialize queue with functions that have no incoming edges (in-degree == 0)        queue: Deque[str] = deque()        for func_name, degree in in_degree.items():            if degree == 0:                queue.append(func_name)                # Store function names in sorted order        sorted_names: List[str] = []        function_map = {func.name: func for func in functions if func.name is not None}                # Process queue (Kahn's algorithm)        while queue:            current = queue.popleft()            sorted_names.append(current)                        # Reduce in-degree for all functions that the current function depends on            for neighbor in graph.get(current, []):                in_degree[neighbor] -= 1                if in_degree[neighbor] == 0:                    queue.append(neighbor)                # Check for cycles        if len(sorted_names) != len(function_map):            # If there's a cycle, return original list as fallback            return functions                # Convert sorted names back to functions        return [function_map[name] for name in sorted_names]        "},{"Name":"__build_dependency_graph_for_procedural","Parameters":[{"TypeValue":"functions","TypeType":"List[UnoplatChapiForgeFunction]"}],"FunctionCalls":[{"FunctionName":"in_degree","Position":{"StartLine":113,"StartLinePosition":29,"StopLine":113,"StopLinePosition":39}}],"Position":{"StartLine":88,"StartLinePosition":4,"StopLine":119,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"graph","TypeType":""},{"TypeValue":"in_degree","TypeType":""},{"TypeValue":"function_name_map","TypeType":"{func.name:funcforfuncinfunctionsiffunc.nameisnotNone}"},{"TypeValue":"in_degree[func.name]","TypeType":""},{"TypeValue":"","TypeType":"self"},{"TypeValue":"queue","TypeType":""},{"TypeValue":"sorted_names","TypeType":""},{"TypeValue":"function_map","TypeType":"{func.name:funcforfuncinfunctionsiffunc.nameisnotNone}"},{"TypeValue":"current","TypeType":"queue"},{"TypeValue":"in_degree[neighbor]","TypeType":""}],"Content":"def __build_dependency_graph_for_procedural(self, functions: List[UnoplatChapiForgeFunction]) -> tuple[Dict[str, Set[str]], Dict[str, int]]:        # Create adjacency list and in-degree count        graph: Dict[str, Set[str]] = defaultdict(set)        in_degree: Dict[str, int] = defaultdict(int)        function_name_map = {func.name: func for func in functions if func.name is not None}                # Initialize in_degree for all functions        for func in functions:            if func.name is not None:                in_degree[func.name] = 0                # Build the graph        for func in functions:            if not func.function_calls or func.name is None:                continue                            for call in func.function_calls:                # Skip self-dependencies                if call.function_name == func.name:                    continue  # Skip adding an edge from a function to itself                                # Check if it's a call to another procedural function                if call.function_name is not None and call.function_name in function_name_map:                    # Add edge from current function to called function                    graph[call.function_name].add(func.name)                    in_degree[func.name] += 1                # Convert defaultdict to regular dict before returning        return dict(graph), dict(in_degree)            "},{"Name":"__sort_function_dependencies_for_procedural","Parameters":[{"TypeValue":"functions","TypeType":"List[UnoplatChapiForgeFunction]"}],"FunctionCalls":[{"NodeName":"queue","FunctionName":"append","Position":{"StartLine":145,"StartLinePosition":25,"StopLine":145,"StopLinePosition":41}}],"Position":{"StartLine":119,"StartLinePosition":4,"StopLine":155,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"graph","TypeType":""},{"TypeValue":"in_degree","TypeType":""},{"TypeValue":"function_name_map","TypeType":"{func.name:funcforfuncinfunctionsiffunc.nameisnotNone}"},{"TypeValue":"in_degree[func.name]","TypeType":""},{"TypeValue":"","TypeType":"self"},{"TypeValue":"queue","TypeType":""},{"TypeValue":"sorted_names","TypeType":""},{"TypeValue":"function_map","TypeType":"{func.name:funcforfuncinfunctionsiffunc.nameisnotNone}"},{"TypeValue":"current","TypeType":"queue"},{"TypeValue":"in_degree[neighbor]","TypeType":""}],"Content":"def __sort_function_dependencies_for_procedural(self, functions: List[UnoplatChapiForgeFunction]) -> List[UnoplatChapiForgeFunction]:        if not functions:            return []                    # Build dependency graph        graph, in_degree = self.__build_dependency_graph_for_procedural(functions)                # Initialize queue with functions that have no incoming edges (in-degree == 0)        queue: Deque[str] = deque()        for func_name, degree in in_degree.items():            if degree == 0:                queue.append(func_name)                # Store function names in sorted order        sorted_names: List[str] = []        function_map = {func.name: func for func in functions if func.name is not None}                # Process queue (Kahn's algorithm)        while queue:            current = queue.popleft()            sorted_names.append(current)                        # Reduce in-degree for all functions that the current function depends on            for neighbor in graph.get(current, []):                in_degree[neighbor] -= 1                if in_degree[neighbor] == 0:                    queue.append(neighbor)                # Check for cycles        if len(sorted_names) != len(function_map):            # If there's a cycle, return original list as fallback            return functions                # Convert sorted names back to functions        return [function_map[name] for name in sorted_names]        "}],"Imports":[{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_chapi_forge_function","UsageName":["UnoplatChapiForgeFunction"]},{"Source":"collections","UsageName":["defaultdict","deque"]},{"Source":"typing","UsageName":["Deque","Dict","List","Set"]}],"Position":{"StartLine":9,"StopLine":155,"StopLinePosition":4},"Content":"class SortFunctionDependencies:        def sort_function_dependencies(self,functions: List[UnoplatChapiForgeFunction],node_type: str) -> List[UnoplatChapiForgeFunction]:        if node_type and node_type == \"CLASS\":            return self.__sort_function_dependencies_for_class(functions=functions)        else:            return self.__sort_function_dependencies_for_procedural(functions=functions)                    def __build_dependency_graph_for_class(self, functions: List[UnoplatChapiForgeFunction]) -> tuple[Dict[str, Set[str]], Dict[str, int]]:        # Create adjacency list and in-degree count        graph: Dict[str, Set[str]] = defaultdict(set)        in_degree: Dict[str, int] = defaultdict(int)        function_name_map = {func.name: func for func in functions if func.name is not None}                # Initialize in-degree for all functions        for func in functions:            if func.name is not None:                in_degree[func.name] = 0                # Build the graph        for func in functions:            if not func.function_calls or func.name is None:                continue                            for call in func.function_calls:                # Skip self-dependencies                if call.function_name == func.name:                    continue  # Skip adding an edge from a function to itself                                # Check if it's an internal class method call                if call.function_name is not None and (                    call.node_name in (\"self\", \"cls\") or                     not call.node_name                ) and call.function_name in function_name_map:                    # Add edge from called function to calling function                    # This means: called_function must be processed before calling_function                    graph[call.function_name].add(func.name)                    in_degree[func.name] += 1                # Convert defaultdict to regular dict before returning        return dict(graph), dict(in_degree)            def __sort_function_dependencies_for_class(self, functions: List[UnoplatChapiForgeFunction]) -> List[UnoplatChapiForgeFunction]:        if not functions:            return []                    # Build dependency graph        graph, in_degree = self.__build_dependency_graph_for_class(functions)                # Initialize queue with functions that have no incoming edges (in-degree == 0)        queue: Deque[str] = deque()        for func_name, degree in in_degree.items():            if degree == 0:                queue.append(func_name)                # Store function names in sorted order        sorted_names: List[str] = []        function_map = {func.name: func for func in functions if func.name is not None}                # Process queue (Kahn's algorithm)        while queue:            current = queue.popleft()            sorted_names.append(current)                        # Reduce in-degree for all functions that the current function depends on            for neighbor in graph.get(current, []):                in_degree[neighbor] -= 1                if in_degree[neighbor] == 0:                    queue.append(neighbor)                # Check for cycles        if len(sorted_names) != len(function_map):            # If there's a cycle, return original list as fallback            return functions                # Convert sorted names back to functions        return [function_map[name] for name in sorted_names]        def __build_dependency_graph_for_procedural(self, functions: List[UnoplatChapiForgeFunction]) -> tuple[Dict[str, Set[str]], Dict[str, int]]:        # Create adjacency list and in-degree count        graph: Dict[str, Set[str]] = defaultdict(set)        in_degree: Dict[str, int] = defaultdict(int)        function_name_map = {func.name: func for func in functions if func.name is not None}                # Initialize in_degree for all functions        for func in functions:            if func.name is not None:                in_degree[func.name] = 0                # Build the graph        for func in functions:            if not func.function_calls or func.name is None:                continue                            for call in func.function_calls:                # Skip self-dependencies                if call.function_name == func.name:                    continue  # Skip adding an edge from a function to itself                                # Check if it's a call to another procedural function                if call.function_name is not None and call.function_name in function_name_map:                    # Add edge from current function to called function                    graph[call.function_name].add(func.name)                    in_degree[func.name] += 1                # Convert defaultdict to regular dict before returning        return dict(graph), dict(in_degree)            def __sort_function_dependencies_for_procedural(self, functions: List[UnoplatChapiForgeFunction]) -> List[UnoplatChapiForgeFunction]:        if not functions:            return []                    # Build dependency graph        graph, in_degree = self.__build_dependency_graph_for_procedural(functions)                # Initialize queue with functions that have no incoming edges (in-degree == 0)        queue: Deque[str] = deque()        for func_name, degree in in_degree.items():            if degree == 0:                queue.append(func_name)                # Store function names in sorted order        sorted_names: List[str] = []        function_map = {func.name: func for func in functions if func.name is not None}                # Process queue (Kahn's algorithm)        while queue:            current = queue.popleft()            sorted_names.append(current)                        # Reduce in-degree for all functions that the current function depends on            for neighbor in graph.get(current, []):                in_degree[neighbor] -= 1                if in_degree[neighbor] == 0:                    queue.append(neighbor)                # Check for cycles        if len(sorted_names) != len(function_map):            # If there's a cycle, return original list as fallback            return functions                # Convert sorted names back to functions        return [function_map[name] for name in sorted_names]        "},{"NodeName":"FunctionMetadataParser","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/function_metadata/function_metadata_parser.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"tree_sitter","TypeType":"CodeConfluenceTreeSitter"}],"FunctionCalls":[{"NodeName":"self","FunctionName":"seen_variables","Position":{"StartLine":18,"StartLinePosition":12,"StopLine":18,"StopLinePosition":13}}],"Position":{"StartLine":16,"StartLinePosition":4,"StopLine":20,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"tree_sitter"},{"TypeValue":"self.seen_variables","TypeType":""}],"Content":"def __init__(self, tree_sitter: CodeConfluenceTreeSitter):        self.parser = tree_sitter.get_parser()        self.seen_variables: set[str] = set()  # Track variables we've seen    "},{"Name":"process_functions","Parameters":[{"TypeValue":"functions","TypeType":"List[ChapiFunction]"}],"FunctionCalls":[{"NodeName":"updated_functions","FunctionName":"append","Position":{"StartLine":24,"StartLinePosition":29,"StopLine":24,"StopLinePosition":71}}],"Position":{"StartLine":20,"StartLinePosition":4,"StopLine":27,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"tree_sitter"},{"TypeValue":"self.seen_variables","TypeType":""},{"TypeValue":"updated_functions","TypeType":"[]"},{"TypeValue":"func.local_variables","TypeType":"[]"}],"Content":"def process_functions(self, functions: List[ChapiFunction]) -> List[ChapiFunction]:        updated_functions = []        for func in functions:            func.local_variables = []            updated_functions.append(self.__get_function_metadata(func))        return updated_functions    "},{"Name":"__get_function_metadata","Parameters":[{"TypeValue":"chapi_function","TypeType":"ChapiFunction"}],"Position":{"StartLine":27,"StartLinePosition":4,"StopLine":49,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"tree_sitter"},{"TypeValue":"self.seen_variables","TypeType":""},{"TypeValue":"updated_functions","TypeType":"[]"},{"TypeValue":"func.local_variables","TypeType":"[]"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"function_calls","TypeType":""},{"TypeValue":"local_vars","TypeType":""},{"TypeValue":"return_type","TypeType":"self"},{"TypeValue":"chapi_function.return_type","TypeType":"return_type"},{"TypeValue":"chapi_function.local_variables","TypeType":"list"},{"TypeValue":"chapi_function.function_calls","TypeType":"function_calls"}],"Content":"def __get_function_metadata(self, chapi_function: ChapiFunction) -> ChapiFunction:        if not chapi_function.content:            return chapi_function        self.seen_variables.clear()        tree = self.parser.parse(bytes(chapi_function.content, \"utf8\"))        function_calls: List[ChapiFunctionCall] = []        local_vars: Dict[str, ChapiFunctionFieldModel] = {}        self.__traverse_tree(tree.walk(), function_calls, local_vars)        # Extract return type if any        return_type = self.__get_return_type(tree)        if return_type:            chapi_function.return_type = return_type        chapi_function.local_variables = list(local_vars.values())        chapi_function.function_calls = function_calls        return chapi_function        "},{"Name":"__get_return_type","Parameters":[{"TypeValue":"tree","TypeType":""}],"FunctionCalls":[{"NodeName":"stack","FunctionName":"append","Position":{"StartLine":63,"StartLinePosition":21,"StopLine":63,"StopLinePosition":34}}],"Position":{"StartLine":49,"StartLinePosition":4,"StopLine":67,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"tree_sitter"},{"TypeValue":"self.seen_variables","TypeType":""},{"TypeValue":"updated_functions","TypeType":"[]"},{"TypeValue":"func.local_variables","TypeType":"[]"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"function_calls","TypeType":""},{"TypeValue":"local_vars","TypeType":""},{"TypeValue":"return_type","TypeType":"self"},{"TypeValue":"chapi_function.return_type","TypeType":"return_type"},{"TypeValue":"chapi_function.local_variables","TypeType":"list"},{"TypeValue":"chapi_function.function_calls","TypeType":"function_calls"},{"TypeValue":"root","TypeType":"tree"},{"TypeValue":"stack","TypeType":"[root]"},{"TypeValue":"node","TypeType":"stack"}],"Content":"def __get_return_type(self, tree) -> Optional[str]:    # Instead of manually navigating cursor, we can do a DFS using the tree's children.        root = tree.root_node        stack = [root]        while stack:            node = stack.pop()            if node.type == \"function_definition\":                # Check children for a 'type' node                for child in node.children:                    if child.type == \"type\":                        return child.text.decode('utf8').strip(\": \")            # Push children into stack to continue traversal            for child in reversed(node.children):                stack.append(child)        return None    "},{"Name":"__traverse_tree","Parameters":[{"TypeValue":"cursor","TypeType":"TreeCursor"},{"TypeValue":"function_calls","TypeType":"List[ChapiFunctionCall]"},{"TypeValue":"local_vars","TypeType":"Dict[str,ChapiFunctionFieldModel]"}],"Position":{"StartLine":67,"StartLinePosition":4,"StopLine":111,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"tree_sitter"},{"TypeValue":"self.seen_variables","TypeType":""},{"TypeValue":"updated_functions","TypeType":"[]"},{"TypeValue":"func.local_variables","TypeType":"[]"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"function_calls","TypeType":""},{"TypeValue":"local_vars","TypeType":""},{"TypeValue":"return_type","TypeType":"self"},{"TypeValue":"chapi_function.return_type","TypeType":"return_type"},{"TypeValue":"chapi_function.local_variables","TypeType":"list"},{"TypeValue":"chapi_function.function_calls","TypeType":"function_calls"},{"TypeValue":"root","TypeType":"tree"},{"TypeValue":"stack","TypeType":"[root]"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"reached_root","TypeType":"True"},{"TypeValue":"call_obj","TypeType":"self"},{"TypeValue":"retracing","TypeType":"False"}],"Content":"def __traverse_tree(self, cursor: TreeCursor, function_calls: List[ChapiFunctionCall], local_vars: Dict[str, ChapiFunctionFieldModel]) -> None:        \"\"\"        Traverse the AST and:        - Capture function calls        - Capture local variables from 'assignment' and 'named_expression'        \"\"\"        reached_root = False        while not reached_root:            node = cursor.node            # Identify function calls            if node.type == \"call\":                call_obj = self.__extract_function_call(node)                if call_obj:                    function_calls.append(call_obj)            # Identify local variables from assignment: `x = ...`            if node.type == \"expression_statement\":                for child in node.children:                    if child.type == \"assignment\":                        self.__process_local_variable(child, local_vars)                    # Check if named_expressions appear top-level in an expression_statement                    # Usually they might appear inside conditions or comprehensions, but let's also scan                    # children for named_expression                    self.__extract_named_expressions(child, local_vars)            # Also extract named expressions outside `expression_statement`            # because they can appear in conditions or comprehensions (e.g. in 'if', 'while')            if node.type not in [\"expression_statement\", \"function_definition\"]:                self.__extract_named_expressions(node, local_vars)            if cursor.goto_first_child():                continue            if cursor.goto_next_sibling():                continue            # Retrace upwards            retracing = True            while retracing:                if not cursor.goto_parent():                    retracing = False                    reached_root = True                elif cursor.goto_next_sibling():                    retracing = False    "},{"Name":"__extract_named_expressions","Parameters":[{"TypeValue":"node","TypeType":"Node"},{"TypeValue":"local_vars","TypeType":"Dict[str,ChapiFunctionFieldModel]"}],"FunctionCalls":[{"NodeName":"self","FunctionName":"__extract_named_expressions","Position":{"StartLine":120,"StartLinePosition":16,"StopLine":120,"StopLinePosition":62}}],"Position":{"StartLine":111,"StartLinePosition":4,"StopLine":122,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"tree_sitter"},{"TypeValue":"self.seen_variables","TypeType":""},{"TypeValue":"updated_functions","TypeType":"[]"},{"TypeValue":"func.local_variables","TypeType":"[]"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"function_calls","TypeType":""},{"TypeValue":"local_vars","TypeType":""},{"TypeValue":"return_type","TypeType":"self"},{"TypeValue":"chapi_function.return_type","TypeType":"return_type"},{"TypeValue":"chapi_function.local_variables","TypeType":"list"},{"TypeValue":"chapi_function.function_calls","TypeType":"function_calls"},{"TypeValue":"root","TypeType":"tree"},{"TypeValue":"stack","TypeType":"[root]"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"reached_root","TypeType":"True"},{"TypeValue":"call_obj","TypeType":"self"},{"TypeValue":"retracing","TypeType":"False"}],"Content":"def __extract_named_expressions(self, node: Node, local_vars: Dict[str, ChapiFunctionFieldModel]) -> None:        \"\"\"        Recursively look for named_expression nodes inside given node to capture walrus-operator introduced variables.        \"\"\"        for child in node.children:            if child.type == \"named_expression\":                # Named expression looks like: identifier := value                self.__process_named_expression(child, local_vars)            # Recurse            self.__extract_named_expressions(child, local_vars)    "},{"Name":"__process_named_expression","Parameters":[{"TypeValue":"node","TypeType":"Node"},{"TypeValue":"local_vars","TypeType":"Dict[str,ChapiFunctionFieldModel]"}],"FunctionCalls":[{"FunctionName":"ChapiFunctionFieldModel","Position":{"StartLine":148,"StartLinePosition":58,"StopLine":152,"StopLinePosition":12}}],"Position":{"StartLine":122,"StartLinePosition":4,"StopLine":154,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"tree_sitter"},{"TypeValue":"self.seen_variables","TypeType":""},{"TypeValue":"updated_functions","TypeType":"[]"},{"TypeValue":"func.local_variables","TypeType":"[]"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"function_calls","TypeType":""},{"TypeValue":"local_vars","TypeType":""},{"TypeValue":"return_type","TypeType":"self"},{"TypeValue":"chapi_function.return_type","TypeType":"return_type"},{"TypeValue":"chapi_function.local_variables","TypeType":"list"},{"TypeValue":"chapi_function.function_calls","TypeType":"function_calls"},{"TypeValue":"root","TypeType":"tree"},{"TypeValue":"stack","TypeType":"[root]"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"reached_root","TypeType":"True"},{"TypeValue":"call_obj","TypeType":"self"},{"TypeValue":"retracing","TypeType":"False"},{"TypeValue":"var_name","TypeType":"c"},{"TypeValue":"value","TypeType":"c"},{"TypeValue":"found_assign","TypeType":"True"},{"TypeValue":"local_vars[var_name]","TypeType":"ChapiFunctionFieldModel"}],"Content":"def __process_named_expression(self, node: Node, local_vars: Dict[str, ChapiFunctionFieldModel]) -> None:        \"\"\"        Handle named_expressions like (matched := regex.match(...))        The AST: named_expression -> identifier, \":=\", value        We treat this like a local variable assignment.        \"\"\"        # Example structure: matched := regex.match(...)        # Children: [identifier('matched'), ':=', call(...)]        var_name = None        value = None        found_assign = False        for c in node.children:            if c.type == \"identifier\" and var_name is None:                # Potential variable name                var_name = c.text.decode('utf8')            elif c.type in {\"=\", \":=\"}:                found_assign = True            elif found_assign:                # Everything after := is the value                # We'll take the entire text of this node as value from here onward                # For named_expression, let's just grab the entire right-hand side node's text                value = c.text.decode('utf8')                break        if var_name and var_name not in local_vars:            local_vars[var_name] = ChapiFunctionFieldModel(                TypeValue=var_name,                TypeType=None,                DefaultValue=value            )    "},{"Name":"__process_local_variable","Parameters":[{"TypeValue":"node","TypeType":"Node"},{"TypeValue":"local_vars","TypeType":"Dict[str,ChapiFunctionFieldModel]"}],"FunctionCalls":[{"FunctionName":"ChapiFunctionFieldModel","Position":{"StartLine":168,"StartLinePosition":54,"StopLine":172,"StopLinePosition":8}}],"Position":{"StartLine":154,"StartLinePosition":4,"StopLine":174,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"tree_sitter"},{"TypeValue":"self.seen_variables","TypeType":""},{"TypeValue":"updated_functions","TypeType":"[]"},{"TypeValue":"func.local_variables","TypeType":"[]"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"function_calls","TypeType":""},{"TypeValue":"local_vars","TypeType":""},{"TypeValue":"return_type","TypeType":"self"},{"TypeValue":"chapi_function.return_type","TypeType":"return_type"},{"TypeValue":"chapi_function.local_variables","TypeType":"list"},{"TypeValue":"chapi_function.function_calls","TypeType":"function_calls"},{"TypeValue":"root","TypeType":"tree"},{"TypeValue":"stack","TypeType":"[root]"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"reached_root","TypeType":"True"},{"TypeValue":"call_obj","TypeType":"self"},{"TypeValue":"retracing","TypeType":"False"},{"TypeValue":"var_name","TypeType":"self"},{"TypeValue":"value","TypeType":"self"},{"TypeValue":"found_assign","TypeType":"True"},{"TypeValue":"local_vars[var_name]","TypeType":"ChapiFunctionFieldModel"},{"TypeValue":"var_type","TypeType":"self"}],"Content":"def __process_local_variable(self, node: Node, local_vars: Dict[str, ChapiFunctionFieldModel]) -> None:        var_name = self.__get_variable_names_from_assignment(node)        if not var_name:            return        # Skip if we've seen this variable before (only keep initial assignment)        if var_name in self.seen_variables:            return                    self.seen_variables.add(var_name)  # Mark as seen                var_type = self.__get_type_hint(node)        value = self.__get_value(node)                local_vars[var_name] = ChapiFunctionFieldModel(            TypeValue=var_name,            TypeType=var_type if var_type else None,            DefaultValue=value if value else None        )    "},{"Name":"__get_variable_names_from_assignment","Parameters":[{"TypeValue":"node","TypeType":"Node"}],"FunctionCalls":[{"NodeName":"\"\"","FunctionName":"join","Position":{"StartLine":195,"StartLinePosition":16,"StopLine":195,"StopLinePosition":31}},{"NodeName":"\"\"","FunctionName":"strip","Position":{"StartLine":195,"StartLinePosition":32,"StopLine":195,"StopLinePosition":39}}],"Position":{"StartLine":174,"StartLinePosition":4,"StopLine":198,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"tree_sitter"},{"TypeValue":"self.seen_variables","TypeType":""},{"TypeValue":"updated_functions","TypeType":"[]"},{"TypeValue":"func.local_variables","TypeType":"[]"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"function_calls","TypeType":""},{"TypeValue":"local_vars","TypeType":""},{"TypeValue":"return_type","TypeType":"self"},{"TypeValue":"chapi_function.return_type","TypeType":"return_type"},{"TypeValue":"chapi_function.local_variables","TypeType":"list"},{"TypeValue":"chapi_function.function_calls","TypeType":"function_calls"},{"TypeValue":"root","TypeType":"tree"},{"TypeValue":"stack","TypeType":"[root]"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"reached_root","TypeType":"True"},{"TypeValue":"call_obj","TypeType":"self"},{"TypeValue":"retracing","TypeType":"False"},{"TypeValue":"var_name","TypeType":"self"},{"TypeValue":"value","TypeType":"self"},{"TypeValue":"found_assign","TypeType":"True"},{"TypeValue":"local_vars[var_name]","TypeType":"ChapiFunctionFieldModel"},{"TypeValue":"var_type","TypeType":"self"},{"TypeValue":"lhs_parts","TypeType":"[]"},{"TypeValue":"lhs","TypeType":"\"\""}],"Content":"def __get_variable_names_from_assignment(self, node: Node) -> str:        \"\"\"        Extract the entire LHS of the assignment as a single 'variable name'.        For example:        - \"x = 1\"  -> \"x\"        - \"x: int = 1\" -> \"x\"        - \"(c, (d, e)) = (3, (4, 5))\" -> \"(c, (d, e))\"        We skip 'type' nodes here because __get_type_hint handles them separately.        \"\"\"        lhs_parts = []        for c in node.children:            if c.type in {\"=\", \":=\"}:                break            elif c.type == \":\":                continue            elif c.type == \"type\":                continue            lhs_parts.append(c.text.decode('utf8'))        lhs = \"\".join(lhs_parts).strip()        return lhs if lhs else ''    "},{"Name":"__get_type_hint","Parameters":[{"TypeValue":"node","TypeType":"Node"}],"Position":{"StartLine":198,"StartLinePosition":4,"StopLine":204,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"tree_sitter"},{"TypeValue":"self.seen_variables","TypeType":""},{"TypeValue":"updated_functions","TypeType":"[]"},{"TypeValue":"func.local_variables","TypeType":"[]"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"function_calls","TypeType":""},{"TypeValue":"local_vars","TypeType":""},{"TypeValue":"return_type","TypeType":"self"},{"TypeValue":"chapi_function.return_type","TypeType":"return_type"},{"TypeValue":"chapi_function.local_variables","TypeType":"list"},{"TypeValue":"chapi_function.function_calls","TypeType":"function_calls"},{"TypeValue":"root","TypeType":"tree"},{"TypeValue":"stack","TypeType":"[root]"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"reached_root","TypeType":"True"},{"TypeValue":"call_obj","TypeType":"self"},{"TypeValue":"retracing","TypeType":"False"},{"TypeValue":"var_name","TypeType":"self"},{"TypeValue":"value","TypeType":"self"},{"TypeValue":"found_assign","TypeType":"True"},{"TypeValue":"local_vars[var_name]","TypeType":"ChapiFunctionFieldModel"},{"TypeValue":"var_type","TypeType":"self"},{"TypeValue":"lhs_parts","TypeType":"[]"},{"TypeValue":"lhs","TypeType":"\"\""}],"Content":"def __get_type_hint(self, node: Node) -> Optional[str]:        for child in node.children:            if child.type == \"type\":                return child.text.decode('utf8').strip(\": \")        return None    "},{"Name":"__get_value","Parameters":[{"TypeValue":"node","TypeType":"Node"}],"FunctionCalls":[{"NodeName":"\"\"","FunctionName":"join","Position":{"StartLine":215,"StartLinePosition":16,"StopLine":215,"StopLinePosition":31}},{"NodeName":"\"\"","FunctionName":"strip","Position":{"StartLine":215,"StartLinePosition":32,"StopLine":215,"StopLinePosition":39}}],"Position":{"StartLine":204,"StartLinePosition":4,"StopLine":218,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"tree_sitter"},{"TypeValue":"self.seen_variables","TypeType":""},{"TypeValue":"updated_functions","TypeType":"[]"},{"TypeValue":"func.local_variables","TypeType":"[]"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"function_calls","TypeType":""},{"TypeValue":"local_vars","TypeType":""},{"TypeValue":"return_type","TypeType":"self"},{"TypeValue":"chapi_function.return_type","TypeType":"return_type"},{"TypeValue":"chapi_function.local_variables","TypeType":"list"},{"TypeValue":"chapi_function.function_calls","TypeType":"function_calls"},{"TypeValue":"root","TypeType":"tree"},{"TypeValue":"stack","TypeType":"[root]"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"reached_root","TypeType":"True"},{"TypeValue":"call_obj","TypeType":"self"},{"TypeValue":"retracing","TypeType":"False"},{"TypeValue":"var_name","TypeType":"self"},{"TypeValue":"value","TypeType":"self"},{"TypeValue":"found_assign","TypeType":"True"},{"TypeValue":"local_vars[var_name]","TypeType":"ChapiFunctionFieldModel"},{"TypeValue":"var_type","TypeType":"self"},{"TypeValue":"lhs_parts","TypeType":"[]"},{"TypeValue":"lhs","TypeType":"\"\""},{"TypeValue":"found_equal","TypeType":"True"},{"TypeValue":"rhs_parts","TypeType":"[]"},{"TypeValue":"rhs","TypeType":"\"\""}],"Content":"def __get_value(self, node: Node) -> str:        found_equal = False        rhs_parts = []        for child in node.children:            if child.type in {\"=\", \":=\"}:                found_equal = True                continue            if found_equal:                # Append all parts of the RHS into one string                rhs_parts.append(child.text.decode('utf8'))        rhs = \"\".join(rhs_parts).strip()        return rhs    "},{"Name":"__extract_function_call","Parameters":[{"TypeValue":"call_node","TypeType":"Node"}],"FunctionCalls":[{"NodeName":"self","FunctionName":"__extract_call_parameters","Position":{"StartLine":233,"StartLinePosition":23,"StopLine":233,"StopLinePosition":58}}],"Position":{"StartLine":218,"StartLinePosition":4,"StopLine":241,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"tree_sitter"},{"TypeValue":"self.seen_variables","TypeType":""},{"TypeValue":"updated_functions","TypeType":"[]"},{"TypeValue":"func.local_variables","TypeType":"[]"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"function_calls","TypeType":""},{"TypeValue":"local_vars","TypeType":""},{"TypeValue":"return_type","TypeType":"self"},{"TypeValue":"chapi_function.return_type","TypeType":"return_type"},{"TypeValue":"chapi_function.local_variables","TypeType":"list"},{"TypeValue":"chapi_function.function_calls","TypeType":"function_calls"},{"TypeValue":"root","TypeType":"tree"},{"TypeValue":"stack","TypeType":"[root]"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"reached_root","TypeType":"True"},{"TypeValue":"call_obj","TypeType":"self"},{"TypeValue":"retracing","TypeType":"False"},{"TypeValue":"var_name","TypeType":"self"},{"TypeValue":"value","TypeType":"self"},{"TypeValue":"found_assign","TypeType":"True"},{"TypeValue":"local_vars[var_name]","TypeType":"ChapiFunctionFieldModel"},{"TypeValue":"var_type","TypeType":"self"},{"TypeValue":"lhs_parts","TypeType":"[]"},{"TypeValue":"lhs","TypeType":"\"\""},{"TypeValue":"found_equal","TypeType":"True"},{"TypeValue":"rhs_parts","TypeType":"[]"},{"TypeValue":"rhs","TypeType":"\"\""},{"TypeValue":"","TypeType":"self"},{"TypeValue":"args","TypeType":"self"},{"TypeValue":"arg_list","TypeType":"child"},{"TypeValue":"func_name","TypeType":"child"}],"Content":"def __extract_function_call(self, call_node: Node) -> Optional[ChapiFunctionCall]:        func_name, node_name = None, None        args = []        arg_list = None        # Identify function name and arguments        for child in call_node.children:            if child.type == \"identifier\":                func_name = child.text.decode('utf8')            elif child.type == \"attribute\":                func_name, node_name = self.__extract_attribute_name(child)            elif child.type == \"argument_list\":                arg_list = child        if arg_list:            args = self.__extract_call_parameters(arg_list)        return ChapiFunctionCall(            NodeName=node_name,            FunctionName=func_name,            Parameters=args        )    "},{"Name":"__extract_attribute_name","Parameters":[{"TypeValue":"attribute_node","TypeType":"Node"}],"FunctionCalls":[{"NodeName":"parts","FunctionName":"append","Position":{"StartLine":246,"StartLinePosition":21,"StopLine":246,"StopLinePosition":50}}],"Position":{"StartLine":241,"StartLinePosition":4,"StopLine":253,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"tree_sitter"},{"TypeValue":"self.seen_variables","TypeType":""},{"TypeValue":"updated_functions","TypeType":"[]"},{"TypeValue":"func.local_variables","TypeType":"[]"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"function_calls","TypeType":""},{"TypeValue":"local_vars","TypeType":""},{"TypeValue":"return_type","TypeType":"self"},{"TypeValue":"chapi_function.return_type","TypeType":"return_type"},{"TypeValue":"chapi_function.local_variables","TypeType":"list"},{"TypeValue":"chapi_function.function_calls","TypeType":"function_calls"},{"TypeValue":"root","TypeType":"tree"},{"TypeValue":"stack","TypeType":"[root]"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"reached_root","TypeType":"True"},{"TypeValue":"call_obj","TypeType":"self"},{"TypeValue":"retracing","TypeType":"False"},{"TypeValue":"var_name","TypeType":"self"},{"TypeValue":"value","TypeType":"self"},{"TypeValue":"found_assign","TypeType":"True"},{"TypeValue":"local_vars[var_name]","TypeType":"ChapiFunctionFieldModel"},{"TypeValue":"var_type","TypeType":"self"},{"TypeValue":"lhs_parts","TypeType":"[]"},{"TypeValue":"lhs","TypeType":"\"\""},{"TypeValue":"found_equal","TypeType":"True"},{"TypeValue":"rhs_parts","TypeType":"[]"},{"TypeValue":"rhs","TypeType":"\"\""},{"TypeValue":"","TypeType":"self"},{"TypeValue":"args","TypeType":"self"},{"TypeValue":"arg_list","TypeType":"child"},{"TypeValue":"func_name","TypeType":"child"},{"TypeValue":"parts","TypeType":"[]"}],"Content":"def __extract_attribute_name(self, attribute_node: Node) -> Tuple[str, Optional[str]]:        # attribute: identifier '.' identifier        parts = []        for c in attribute_node.children:            if c.type == \"identifier\":                parts.append(c.text.decode('utf8'))        if len(parts) == 2:            # obj.method            return parts[1], parts[0]        # fallback if unusual structure        return parts[-1] if parts else None, None #type: ignore    "},{"Name":"__extract_call_parameters","Parameters":[{"TypeValue":"arg_list_node","TypeType":"Node"}],"Position":{"StartLine":253,"StartLinePosition":4,"StopLine":271,"StopLinePosition":21},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"tree_sitter"},{"TypeValue":"self.seen_variables","TypeType":""},{"TypeValue":"updated_functions","TypeType":"[]"},{"TypeValue":"func.local_variables","TypeType":"[]"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"function_calls","TypeType":""},{"TypeValue":"local_vars","TypeType":""},{"TypeValue":"return_type","TypeType":"self"},{"TypeValue":"chapi_function.return_type","TypeType":"return_type"},{"TypeValue":"chapi_function.local_variables","TypeType":"list"},{"TypeValue":"chapi_function.function_calls","TypeType":"function_calls"},{"TypeValue":"root","TypeType":"tree"},{"TypeValue":"stack","TypeType":"[root]"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"reached_root","TypeType":"True"},{"TypeValue":"call_obj","TypeType":"self"},{"TypeValue":"retracing","TypeType":"False"},{"TypeValue":"var_name","TypeType":"self"},{"TypeValue":"value","TypeType":"self"},{"TypeValue":"found_assign","TypeType":"True"},{"TypeValue":"local_vars[var_name]","TypeType":"ChapiFunctionFieldModel"},{"TypeValue":"var_type","TypeType":"self"},{"TypeValue":"lhs_parts","TypeType":"[]"},{"TypeValue":"lhs","TypeType":"\"\""},{"TypeValue":"found_equal","TypeType":"True"},{"TypeValue":"rhs_parts","TypeType":"[]"},{"TypeValue":"rhs","TypeType":"\"\""},{"TypeValue":"","TypeType":""},{"TypeValue":"args","TypeType":"self"},{"TypeValue":"arg_list","TypeType":"child"},{"TypeValue":"func_name","TypeType":"child"},{"TypeValue":"parts","TypeType":"[]"},{"TypeValue":"params","TypeType":"[]"},{"TypeValue":"pos_arg_index","TypeType":""},{"TypeValue":"key","TypeType":"arg"},{"TypeValue":"val","TypeType":"arg"}],"Content":"def __extract_call_parameters(self, arg_list_node: Node) -> List[ChapiParameter]:        params = []        pos_arg_index = 0        for arg in arg_list_node.children:            if arg.type == \"keyword_argument\":                key, val = None, None                for i, kw_child in enumerate(arg.children):                    if kw_child.type == \"=\":                        key = arg.children[i-1].text.decode('utf8')                        val = arg.children[i+1].text.decode('utf8')                        break                if key and val:                    params.append(ChapiParameter(TypeValue=val, TypeType=key))            elif arg.type not in [\"(\", \")\", \",\", \"**\", \"*\"]:                # positional argument                val = arg.text.decode('utf8')                params.append(ChapiParameter(TypeValue=val, TypeType=str(pos_arg_index)))                pos_arg_index += 1        return params"}],"Imports":[{"Source":"unoplat_code_confluence.data_models.chapi.chapi_function","UsageName":["ChapiFunction"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_function_field_model","UsageName":["ChapiFunctionFieldModel"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_functioncall","UsageName":["ChapiFunctionCall"]},{"Source":"unoplat_code_confluence.data_models.chapi.chapi_parameter","UsageName":["ChapiParameter"]},{"Source":"unoplat_code_confluence.parser.tree_sitter.code_confluence_tree_sitter","UsageName":["CodeConfluenceTreeSitter"]},{"Source":"typing","UsageName":["Dict","List","Optional","Tuple"]},{"Source":"tree_sitter","UsageName":["Node","TreeCursor"]}],"Position":{"StartLine":15,"StopLine":271,"StopLinePosition":21},"Content":"class FunctionMetadataParser:    def __init__(self, tree_sitter: CodeConfluenceTreeSitter):        self.parser = tree_sitter.get_parser()        self.seen_variables: set[str] = set()  # Track variables we've seen    def process_functions(self, functions: List[ChapiFunction]) -> List[ChapiFunction]:        updated_functions = []        for func in functions:            func.local_variables = []            updated_functions.append(self.__get_function_metadata(func))        return updated_functions    def __get_function_metadata(self, chapi_function: ChapiFunction) -> ChapiFunction:        if not chapi_function.content:            return chapi_function        self.seen_variables.clear()        tree = self.parser.parse(bytes(chapi_function.content, \"utf8\"))        function_calls: List[ChapiFunctionCall] = []        local_vars: Dict[str, ChapiFunctionFieldModel] = {}        self.__traverse_tree(tree.walk(), function_calls, local_vars)        # Extract return type if any        return_type = self.__get_return_type(tree)        if return_type:            chapi_function.return_type = return_type        chapi_function.local_variables = list(local_vars.values())        chapi_function.function_calls = function_calls        return chapi_function        def __get_return_type(self, tree) -> Optional[str]:    # Instead of manually navigating cursor, we can do a DFS using the tree's children.        root = tree.root_node        stack = [root]        while stack:            node = stack.pop()            if node.type == \"function_definition\":                # Check children for a 'type' node                for child in node.children:                    if child.type == \"type\":                        return child.text.decode('utf8').strip(\": \")            # Push children into stack to continue traversal            for child in reversed(node.children):                stack.append(child)        return None    def __traverse_tree(self, cursor: TreeCursor, function_calls: List[ChapiFunctionCall], local_vars: Dict[str, ChapiFunctionFieldModel]) -> None:        \"\"\"        Traverse the AST and:        - Capture function calls        - Capture local variables from 'assignment' and 'named_expression'        \"\"\"        reached_root = False        while not reached_root:            node = cursor.node            # Identify function calls            if node.type == \"call\":                call_obj = self.__extract_function_call(node)                if call_obj:                    function_calls.append(call_obj)            # Identify local variables from assignment: `x = ...`            if node.type == \"expression_statement\":                for child in node.children:                    if child.type == \"assignment\":                        self.__process_local_variable(child, local_vars)                    # Check if named_expressions appear top-level in an expression_statement                    # Usually they might appear inside conditions or comprehensions, but let's also scan                    # children for named_expression                    self.__extract_named_expressions(child, local_vars)            # Also extract named expressions outside `expression_statement`            # because they can appear in conditions or comprehensions (e.g. in 'if', 'while')            if node.type not in [\"expression_statement\", \"function_definition\"]:                self.__extract_named_expressions(node, local_vars)            if cursor.goto_first_child():                continue            if cursor.goto_next_sibling():                continue            # Retrace upwards            retracing = True            while retracing:                if not cursor.goto_parent():                    retracing = False                    reached_root = True                elif cursor.goto_next_sibling():                    retracing = False    def __extract_named_expressions(self, node: Node, local_vars: Dict[str, ChapiFunctionFieldModel]) -> None:        \"\"\"        Recursively look for named_expression nodes inside given node to capture walrus-operator introduced variables.        \"\"\"        for child in node.children:            if child.type == \"named_expression\":                # Named expression looks like: identifier := value                self.__process_named_expression(child, local_vars)            # Recurse            self.__extract_named_expressions(child, local_vars)    def __process_named_expression(self, node: Node, local_vars: Dict[str, ChapiFunctionFieldModel]) -> None:        \"\"\"        Handle named_expressions like (matched := regex.match(...))        The AST: named_expression -> identifier, \":=\", value        We treat this like a local variable assignment.        \"\"\"        # Example structure: matched := regex.match(...)        # Children: [identifier('matched'), ':=', call(...)]        var_name = None        value = None        found_assign = False        for c in node.children:            if c.type == \"identifier\" and var_name is None:                # Potential variable name                var_name = c.text.decode('utf8')            elif c.type in {\"=\", \":=\"}:                found_assign = True            elif found_assign:                # Everything after := is the value                # We'll take the entire text of this node as value from here onward                # For named_expression, let's just grab the entire right-hand side node's text                value = c.text.decode('utf8')                break        if var_name and var_name not in local_vars:            local_vars[var_name] = ChapiFunctionFieldModel(                TypeValue=var_name,                TypeType=None,                DefaultValue=value            )    def __process_local_variable(self, node: Node, local_vars: Dict[str, ChapiFunctionFieldModel]) -> None:        var_name = self.__get_variable_names_from_assignment(node)        if not var_name:            return        # Skip if we've seen this variable before (only keep initial assignment)        if var_name in self.seen_variables:            return                    self.seen_variables.add(var_name)  # Mark as seen                var_type = self.__get_type_hint(node)        value = self.__get_value(node)                local_vars[var_name] = ChapiFunctionFieldModel(            TypeValue=var_name,            TypeType=var_type if var_type else None,            DefaultValue=value if value else None        )    def __get_variable_names_from_assignment(self, node: Node) -> str:        \"\"\"        Extract the entire LHS of the assignment as a single 'variable name'.        For example:        - \"x = 1\"  -> \"x\"        - \"x: int = 1\" -> \"x\"        - \"(c, (d, e)) = (3, (4, 5))\" -> \"(c, (d, e))\"        We skip 'type' nodes here because __get_type_hint handles them separately.        \"\"\"        lhs_parts = []        for c in node.children:            if c.type in {\"=\", \":=\"}:                break            elif c.type == \":\":                continue            elif c.type == \"type\":                continue            lhs_parts.append(c.text.decode('utf8'))        lhs = \"\".join(lhs_parts).strip()        return lhs if lhs else ''    def __get_type_hint(self, node: Node) -> Optional[str]:        for child in node.children:            if child.type == \"type\":                return child.text.decode('utf8').strip(\": \")        return None    def __get_value(self, node: Node) -> str:        found_equal = False        rhs_parts = []        for child in node.children:            if child.type in {\"=\", \":=\"}:                found_equal = True                continue            if found_equal:                # Append all parts of the RHS into one string                rhs_parts.append(child.text.decode('utf8'))        rhs = \"\".join(rhs_parts).strip()        return rhs    def __extract_function_call(self, call_node: Node) -> Optional[ChapiFunctionCall]:        func_name, node_name = None, None        args = []        arg_list = None        # Identify function name and arguments        for child in call_node.children:            if child.type == \"identifier\":                func_name = child.text.decode('utf8')            elif child.type == \"attribute\":                func_name, node_name = self.__extract_attribute_name(child)            elif child.type == \"argument_list\":                arg_list = child        if arg_list:            args = self.__extract_call_parameters(arg_list)        return ChapiFunctionCall(            NodeName=node_name,            FunctionName=func_name,            Parameters=args        )    def __extract_attribute_name(self, attribute_node: Node) -> Tuple[str, Optional[str]]:        # attribute: identifier '.' identifier        parts = []        for c in attribute_node.children:            if c.type == \"identifier\":                parts.append(c.text.decode('utf8'))        if len(parts) == 2:            # obj.method            return parts[1], parts[0]        # fallback if unusual structure        return parts[-1] if parts else None, None #type: ignore    def __extract_call_parameters(self, arg_list_node: Node) -> List[ChapiParameter]:        params = []        pos_arg_index = 0        for arg in arg_list_node.children:            if arg.type == \"keyword_argument\":                key, val = None, None                for i, kw_child in enumerate(arg.children):                    if kw_child.type == \"=\":                        key = arg.children[i-1].text.decode('utf8')                        val = arg.children[i+1].text.decode('utf8')                        break                if key and val:                    params.append(ChapiParameter(TypeValue=val, TypeType=key))            elif arg.type not in [\"(\", \")\", \",\", \"**\", \"*\"]:                # positional argument                val = arg.text.decode('utf8')                params.append(ChapiParameter(TypeValue=val, TypeType=str(pos_arg_index)))                pos_arg_index += 1        return params"},{"NodeName":"PythonNodeDependencyProcessor","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/python/python_node_dependency_processor.py","Functions":[{"Name":"process_dependencies","Parameters":[{"TypeValue":"node","TypeType":"UnoplatChapiForgeNode"},{"TypeValue":"qualified_dict","TypeType":"Dict[str,UnoplatChapiForgeNode]"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"opt","Position":{"StartLine":104,"StartLinePosition":18,"StopLine":104,"StopLinePosition":37}},{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":104,"StartLinePosition":38,"StopLine":117,"StopLinePosition":12}}],"Position":{"StartLine":14,"StartLinePosition":4,"StopLine":125,"StopLinePosition":8},"LocalVariables":[{"TypeValue":"node.dependent_internal_classes","TypeType":"[]"},{"TypeValue":"procedural_nodes","TypeType":""},{"TypeValue":"internal_imports","TypeType":"node"},{"TypeValue":"qualified_name","TypeType":"f\"{imp.source}.{usage.original_name}\""},{"TypeValue":"procedural_dependent_node","TypeType":"qualified_dict"}],"Content":"def process_dependencies(self, node: UnoplatChapiForgeNode, qualified_dict: Dict[str, UnoplatChapiForgeNode]) -> List[str]:        \"\"\"Process dependencies for a given node.                We have internal imports with source and usage - original and alias both.        What we have to do is go through each import and identify class dependencies        based on import type. Classes use CamelCase naming convention.                For example:        from myproject.models import UserModel, BaseClass, helper_func        from myproject.utils import helper_function, HelperClass as helper                We identify:        - UserModel, BaseClass as class dependencies -> myproject.models.UserModel, myproject.models.BaseClass        - helper_func as function -> myproject.models        - HelperClass as class dependency -> myproject.utils.HelperClass        - helper_function as function -> myproject.utils                Args:            node: The ChapiUnoplatNode to process dependencies for                    Note:            Modifies node.dependent_internal_classes in place:            - For classes: adds fully qualified name (source.original_name)            - For non-classes: adds only the source module path        \"\"\"        try:            logger.debug(\"Processing dependencies for node: {}\", node.node_name)                        if not node.segregated_imports or ImportType.INTERNAL not in node.segregated_imports:                logger.debug(\"No internal imports found for node: {}\", node.node_name)                return []                        node.dependent_internal_classes = []            procedural_nodes: Set[str] = set()                internal_imports = node.segregated_imports[ImportType.INTERNAL]                        logger.debug(\"Found {} internal imports for node: {}\", len(internal_imports), node.node_name)                        for imp in internal_imports:                if imp.usage_names:                    for usage in imp.usage_names:                        if usage.original_name:                            try:                                if IsClassName.is_python_class_name(usage.original_name):                                    # For classes, add the fully qualified name                                    qualified_name = f\"{imp.source}.{usage.original_name}\"                                    logger.debug(\"Found class dependency: {} -> {}\", node.node_name, qualified_name)                                    node.dependent_internal_classes.append(qualified_name)                                                                else:                                    # For non-classes, add only the source module path                                    try:                                        procedural_dependent_node = qualified_dict[imp.source] #type: ignore                                    except KeyError:                                        logger.error(                                            \"Missing source module in dictionary: {}\\n\"                                            \"Available modules: {}\\n\"                                            \"Current node: {}\",                                             imp.source, list(qualified_dict.keys()), node.node_name                                        )                                        continue                                                                            if procedural_dependent_node.node_name not in procedural_nodes:                                        logger.debug(                                            \"Found procedural dependency: {} -> {}\",                                             node.node_name, procedural_dependent_node.node_name                                        )                                        node.dependent_internal_classes.append(imp.source) #type: ignore                                        procedural_nodes.add(procedural_dependent_node.node_name) #type: ignore                                                                    except Exception as inner_e:                                logger.error(                                    \"Error processing import usage: {}\\n\"                                    \"Import source: {}\\n\"                                    \"Error: {}\",                                     usage.original_name, imp.source, str(inner_e)                                )                                continue                        logger.debug(                \"Completed processing dependencies for node: {}\\n\"                \"Found {} class dependencies and {} procedural dependencies\",                node.node_name,                 len(node.dependent_internal_classes) - len(procedural_nodes),                len(procedural_nodes)            )                                            return node.dependent_internal_classes                                                        except Exception as e:            logger.opt(exception=True).error(                \"Error processing dependencies for node: {}\\n\"                \"Node type: {}\\n\"                \"Node package: {}\\n\"                \"Internal imports: {}\\n\"                \"Error: {}\\n\"                \"Available qualified names: {}\",                node.node_name,                node.type,                node.package,                internal_imports if 'internal_imports' in locals() else 'Not initialized',                str(e),                list(qualified_dict.keys())            )            return []                                                                "}],"Imports":[{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_chapi_forge_node","UsageName":["UnoplatChapiForgeNode"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_import_type","UsageName":["ImportType"]},{"Source":"unoplat_code_confluence.utility.is_class_name","UsageName":["IsClassName"]},{"Source":"typing","UsageName":["Dict","List","Set"]},{"Source":"loguru","UsageName":["logger"]}],"Position":{"StartLine":13,"StopLine":125,"StopLinePosition":8},"Content":"class PythonNodeDependencyProcessor:    def process_dependencies(self, node: UnoplatChapiForgeNode, qualified_dict: Dict[str, UnoplatChapiForgeNode]) -> List[str]:        \"\"\"Process dependencies for a given node.                We have internal imports with source and usage - original and alias both.        What we have to do is go through each import and identify class dependencies        based on import type. Classes use CamelCase naming convention.                For example:        from myproject.models import UserModel, BaseClass, helper_func        from myproject.utils import helper_function, HelperClass as helper                We identify:        - UserModel, BaseClass as class dependencies -> myproject.models.UserModel, myproject.models.BaseClass        - helper_func as function -> myproject.models        - HelperClass as class dependency -> myproject.utils.HelperClass        - helper_function as function -> myproject.utils                Args:            node: The ChapiUnoplatNode to process dependencies for                    Note:            Modifies node.dependent_internal_classes in place:            - For classes: adds fully qualified name (source.original_name)            - For non-classes: adds only the source module path        \"\"\"        try:            logger.debug(\"Processing dependencies for node: {}\", node.node_name)                        if not node.segregated_imports or ImportType.INTERNAL not in node.segregated_imports:                logger.debug(\"No internal imports found for node: {}\", node.node_name)                return []                        node.dependent_internal_classes = []            procedural_nodes: Set[str] = set()                internal_imports = node.segregated_imports[ImportType.INTERNAL]                        logger.debug(\"Found {} internal imports for node: {}\", len(internal_imports), node.node_name)                        for imp in internal_imports:                if imp.usage_names:                    for usage in imp.usage_names:                        if usage.original_name:                            try:                                if IsClassName.is_python_class_name(usage.original_name):                                    # For classes, add the fully qualified name                                    qualified_name = f\"{imp.source}.{usage.original_name}\"                                    logger.debug(\"Found class dependency: {} -> {}\", node.node_name, qualified_name)                                    node.dependent_internal_classes.append(qualified_name)                                                                else:                                    # For non-classes, add only the source module path                                    try:                                        procedural_dependent_node = qualified_dict[imp.source] #type: ignore                                    except KeyError:                                        logger.error(                                            \"Missing source module in dictionary: {}\\n\"                                            \"Available modules: {}\\n\"                                            \"Current node: {}\",                                             imp.source, list(qualified_dict.keys()), node.node_name                                        )                                        continue                                                                            if procedural_dependent_node.node_name not in procedural_nodes:                                        logger.debug(                                            \"Found procedural dependency: {} -> {}\",                                             node.node_name, procedural_dependent_node.node_name                                        )                                        node.dependent_internal_classes.append(imp.source) #type: ignore                                        procedural_nodes.add(procedural_dependent_node.node_name) #type: ignore                                                                    except Exception as inner_e:                                logger.error(                                    \"Error processing import usage: {}\\n\"                                    \"Import source: {}\\n\"                                    \"Error: {}\",                                     usage.original_name, imp.source, str(inner_e)                                )                                continue                        logger.debug(                \"Completed processing dependencies for node: {}\\n\"                \"Found {} class dependencies and {} procedural dependencies\",                node.node_name,                 len(node.dependent_internal_classes) - len(procedural_nodes),                len(procedural_nodes)            )                                            return node.dependent_internal_classes                                                        except Exception as e:            logger.opt(exception=True).error(                \"Error processing dependencies for node: {}\\n\"                \"Node type: {}\\n\"                \"Node package: {}\\n\"                \"Internal imports: {}\\n\"                \"Error: {}\\n\"                \"Available qualified names: {}\",                node.node_name,                node.type,                node.package,                internal_imports if 'internal_imports' in locals() else 'Not initialized',                str(e),                list(qualified_dict.keys())            )            return []                                                                "},{"NodeName":"UnsupportedLanguageError","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/codebase_parser_factory.py","MultipleExtend":["Exception"],"Imports":[{"Source":"unoplat_code_confluence.configuration.settings","UsageName":["ProgrammingLanguage"]},{"Source":"unoplat_code_confluence.parser.codebase_parser_strategy","UsageName":["CodebaseParserStrategy"]},{"Source":"unoplat_code_confluence.parser.python.python_codebase_parser","UsageName":["PythonCodebaseParser"]}],"Position":{"StartLine":7,"StopLine":10},"Content":"class UnsupportedLanguageError(Exception):    pass"},{"NodeName":"CodebaseParserFactory","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/codebase_parser_factory.py","Functions":[{"Name":"get_parser","Parameters":[{"TypeValue":"programming_language","TypeType":"str"}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":11,"StartLinePosition":4,"StopLine":12,"StopLinePosition":4}}],"Position":{"StartLine":12,"StartLinePosition":4,"StopLine":15,"StopLinePosition":105},"Content":"def get_parser(programming_language: str) -> CodebaseParserStrategy:        if programming_language.lower() == ProgrammingLanguage.PYTHON.value:            return PythonCodebaseParser()        raise UnsupportedLanguageError(f\"No parser implementation for language: {programming_language}\") "}],"Imports":[{"Source":"unoplat_code_confluence.configuration.settings","UsageName":["ProgrammingLanguage"]},{"Source":"unoplat_code_confluence.parser.codebase_parser_strategy","UsageName":["CodebaseParserStrategy"]},{"Source":"unoplat_code_confluence.parser.python.python_codebase_parser","UsageName":["PythonCodebaseParser"]}],"Position":{"StartLine":10,"StopLine":15,"StopLinePosition":105},"Content":"class CodebaseParserFactory:    @staticmethod    def get_parser(programming_language: str) -> CodebaseParserStrategy:        if programming_language.lower() == ProgrammingLanguage.PYTHON.value:            return PythonCodebaseParser()        raise UnsupportedLanguageError(f\"No parser implementation for language: {programming_language}\") "},{"NodeName":"CodebaseParserStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/codebase_parser_strategy.py","MultipleExtend":["ABC"],"Functions":[{"Name":"parse_codebase","Parameters":[{"TypeValue":"codebase_name","TypeType":"str"},{"TypeValue":"json_data","TypeType":"dict"},{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"programming_language_metadata","TypeType":"ProgrammingLanguageMetadata"}],"Annotations":[{"Name":"abstractmethod","Position":{"StartLine":10,"StartLinePosition":4,"StopLine":11,"StopLinePosition":4}}],"Position":{"StartLine":11,"StartLinePosition":4,"StopLine":13,"StopLinePosition":13},"Content":"def parse_codebase(self, codebase_name: str, json_data: dict, local_workspace_path: str, programming_language_metadata: ProgrammingLanguageMetadata) -> UnoplatCodebase:        \"\"\"Parse codebase based on language specific implementation\"\"\"        pass "}],"Imports":[{"Source":"unoplat_code_confluence.configuration.settings","UsageName":["ProgrammingLanguageMetadata"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_codebase","UsageName":["UnoplatCodebase"]},{"Source":"abc","UsageName":["ABC","abstractmethod"]}],"Position":{"StartLine":9,"StopLine":13,"StopLinePosition":13},"Content":"class CodebaseParserStrategy(ABC):    @abstractmethod    def parse_codebase(self, codebase_name: str, json_data: dict, local_workspace_path: str, programming_language_metadata: ProgrammingLanguageMetadata) -> UnoplatCodebase:        \"\"\"Parse codebase based on language specific implementation\"\"\"        pass "},{"NodeName":"CodeConfluenceTreeSitter","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/tree_sitter/code_confluence_tree_sitter.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"language","TypeType":"ProgrammingLanguage"}],"Position":{"StartLine":10,"StartLinePosition":4,"StopLine":23,"StopLinePosition":12},"LocalVariables":[{"TypeValue":"language","TypeType":""},{"TypeValue":"ProgrammingLanguage.PYTHON","TypeType":""},{"TypeValue":"PY_LANGUAGE","TypeType":"Language"},{"TypeValue":"self.parser","TypeType":"Parser"},{"TypeValue":"self.parser.language","TypeType":"PY_LANGUAGE"}],"Content":"def __init__(self, language: ProgrammingLanguage):        \"\"\"Initialize parser based on programming language.                Args:            language: Programming language enum value        \"\"\"                        match language:            case ProgrammingLanguage.PYTHON:                PY_LANGUAGE = Language(tree_sitter_python.language()) #type: ignore                self.parser = Parser()                self.parser.language = PY_LANGUAGE #type: ignore            "}],"Imports":[{"Source":"unoplat_code_confluence.configuration.settings","UsageName":["ProgrammingLanguage"]},{"Source":"tree_sitter","UsageName":["Language","Parser"]},{"Source":"tree_sitter_python"}],"Position":{"StartLine":9,"StopLine":26,"StopLinePosition":4},"Content":"class CodeConfluenceTreeSitter:    def __init__(self, language: ProgrammingLanguage):        \"\"\"Initialize parser based on programming language.                Args:            language: Programming language enum value        \"\"\"                        match language:            case ProgrammingLanguage.PYTHON:                PY_LANGUAGE = Language(tree_sitter_python.language()) #type: ignore                self.parser = Parser()                self.parser.language = PY_LANGUAGE #type: ignore            case _:                raise ValueError(f\"Unsupported programming language: {language}\")                    "},{"NodeName":"CodebaseParser","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/parser/codebase_parser.py","Functions":[{"Name":"parse_codebase","Parameters":[{"TypeValue":"codebase_name","TypeType":"str"},{"TypeValue":"json_data","TypeType":"dict"},{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"programming_language_metadata","TypeType":"ProgrammingLanguageMetadata"}],"FunctionCalls":[{"NodeName":"CodebaseParserFactory","FunctionName":"get_parser","Position":{"StartLine":10,"StartLinePosition":38,"StopLine":10,"StopLinePosition":94}}],"Position":{"StartLine":8,"StartLinePosition":4,"StopLine":11,"StopLinePosition":191},"LocalVariables":[{"TypeValue":"parser","TypeType":"CodebaseParserFactory"}],"Content":"def parse_codebase(self, codebase_name: str, json_data: dict, local_workspace_path: str, programming_language_metadata: ProgrammingLanguageMetadata) -> UnoplatCodebase:        \"\"\"Concrete implementation of the parse_codebase method.\"\"\"        parser = CodebaseParserFactory.get_parser(programming_language_metadata.language.value)        return parser.parse_codebase(codebase_name=codebase_name, json_data=json_data, local_workspace_path=local_workspace_path, programming_language_metadata=programming_language_metadata) "}],"Imports":[{"Source":"unoplat_code_confluence.configuration.settings","UsageName":["ProgrammingLanguageMetadata"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_codebase","UsageName":["UnoplatCodebase"]},{"Source":"unoplat_code_confluence.parser.codebase_parser_factory","UsageName":["CodebaseParserFactory"]}],"Position":{"StartLine":7,"StopLine":11,"StopLinePosition":191},"Content":"class CodebaseParser():    def parse_codebase(self, codebase_name: str, json_data: dict, local_workspace_path: str, programming_language_metadata: ProgrammingLanguageMetadata) -> UnoplatCodebase:        \"\"\"Concrete implementation of the parse_codebase method.\"\"\"        parser = CodebaseParserFactory.get_parser(programming_language_metadata.language.value)        return parser.parse_codebase(codebase_name=codebase_name, json_data=json_data, local_workspace_path=local_workspace_path, programming_language_metadata=programming_language_metadata) "},{"NodeName":"ILoadJson","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/loader/iload_json.py","MultipleExtend":["ABC"],"Functions":[{"Name":"load_json_from_file","Parameters":[{"TypeValue":"file_path","TypeType":"str"}],"Annotations":[{"Name":"abstractmethod","Position":{"StartLine":7,"StartLinePosition":4,"StopLine":8,"StopLinePosition":4}}],"Position":{"StartLine":8,"StartLinePosition":4,"StopLine":10,"StopLinePosition":12},"Content":"def load_json_from_file(self, file_path: str) -> json:        \"\"\"Load JSON data from a file and return a JSON object.\"\"\"        pass"}],"Imports":[{"Source":"abc","UsageName":["ABC","abstractmethod"]},{"Source":"json"}],"Position":{"StartLine":6,"StopLine":10,"StopLinePosition":12},"Content":"class ILoadJson(ABC):    @abstractmethod    def load_json_from_file(self, file_path: str) -> json:        \"\"\"Load JSON data from a file and return a JSON object.\"\"\"        pass"},{"NodeName":"JsonLoader","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/loader/json_loader.py","MultipleExtend":["ILoadJson"],"Functions":[{"Name":"load_json_from_file","Parameters":[{"TypeValue":"file_path","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":38,"StartLinePosition":18,"StopLine":38,"StopLinePosition":74}}],"Annotations":[{"Name":"logger.catch","Position":{"StartLine":14,"StartLinePosition":4,"StopLine":15,"StopLinePosition":4}}],"Position":{"StartLine":15,"StartLinePosition":4,"StopLine":41},"LocalVariables":[{"TypeValue":"data","TypeType":""}],"Content":"def load_json_from_file(self, file_path: str) -> Any:        \"\"\"        Load JSON data from a file and return the JSON object.        This method uses Loguru for enhanced logging and exception handling. It attempts to open and read        a JSON file specified by the file_path argument. If successful, it logs the success and returns the data.        If an exception occurs, it logs the error and re-raises the exception.        Args:            file_path (str): The path to the JSON file to be loaded.        Returns:            json: The JSON object loaded from the file.        Raises:            Exception: Propagates any exceptions that occur during file opening or JSON loading.        \"\"\"        try:            with open(file_path, 'r') as file:                data: Any = json.load(file)            logger.info(f\"JSON data successfully loaded from {file_path}\")            return data        except Exception as e:            logger.error(f\"Failed to load JSON data from {file_path}: {e}\")            raise"}],"Imports":[{"Source":"unoplat_code_confluence.loader.iload_json","UsageName":["ILoadJson"]},{"Source":"json"},{"Source":"typing","UsageName":["Any"]},{"Source":"loguru","UsageName":["logger"]}],"Position":{"StartLine":13,"StopLine":41},"Content":"class JsonLoader(ILoadJson):    @logger.catch    def load_json_from_file(self, file_path: str) -> Any:        \"\"\"        Load JSON data from a file and return the JSON object.        This method uses Loguru for enhanced logging and exception handling. It attempts to open and read        a JSON file specified by the file_path argument. If successful, it logs the success and returns the data.        If an exception occurs, it logs the error and re-raises the exception.        Args:            file_path (str): The path to the JSON file to be loaded.        Returns:            json: The JSON object loaded from the file.        Raises:            Exception: Propagates any exceptions that occur during file opening or JSON loading.        \"\"\"        try:            with open(file_path, 'r') as file:                data: Any = json.load(file)            logger.info(f\"JSON data successfully loaded from {file_path}\")            return data        except Exception as e:            logger.error(f\"Failed to load JSON data from {file_path}: {e}\")            raise"},{"NodeName":"ISummariser","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/markdownparser/isummariser.py","MultipleExtend":["ABC"],"Functions":[{"Name":"summarise_to_markdown","Parameters":[{"TypeValue":"unoplat_codebase_summary","TypeType":"ForgeUnoplatCodebaseSummary"}],"Annotations":[{"Name":"abstractmethod","Position":{"StartLine":10,"StartLinePosition":4,"StopLine":11,"StopLinePosition":4}}],"Position":{"StartLine":11,"StartLinePosition":4,"StopLine":21,"StopLinePosition":12},"Content":"def summarise_to_markdown(self, unoplat_codebase_summary: ForgeUnoplatCodebaseSummary) -> str:        \"\"\"        Summarises the provided unoplat codebase summary to markdown.                Args:            unoplat_codebase_summary (DspyUnoplatCodebaseSummary): The unoplat codebase summary to summarise.                Returns:            str: The markdown string.        \"\"\"        pass"}],"Imports":[{"Source":"unoplat_code_confluence.data_models.forge_summary.forge_unoplat_codebase_summary","UsageName":["ForgeUnoplatCodebaseSummary"]},{"Source":"abc","UsageName":["ABC","abstractmethod"]}],"Position":{"StartLine":8,"StopLine":21,"StopLinePosition":12},"Content":"class ISummariser(ABC):        @abstractmethod    def summarise_to_markdown(self, unoplat_codebase_summary: ForgeUnoplatCodebaseSummary) -> str:        \"\"\"        Summarises the provided unoplat codebase summary to markdown.                Args:            unoplat_codebase_summary (DspyUnoplatCodebaseSummary): The unoplat codebase summary to summarise.                Returns:            str: The markdown string.        \"\"\"        pass"},{"NodeName":"MarkdownSummariser","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/markdownparser/markdownsummariser.py","MultipleExtend":["ISummariser"],"Functions":[{"Name":"summarise_to_markdown","Parameters":[{"TypeValue":"unoplat_codebase_summary","TypeType":"ForgeUnoplatCodebaseSummary"}],"FunctionCalls":[{"NodeName":"markdown_output","FunctionName":"append","Position":{"StartLine":63,"StartLinePosition":27,"StopLine":63,"StopLinePosition":39}}],"Position":{"StartLine":11,"StartLinePosition":4,"StopLine":65,"StopLinePosition":41},"LocalVariables":[{"TypeValue":"markdown_output","TypeType":"[]"}],"Content":"def summarise_to_markdown(self, unoplat_codebase_summary: ForgeUnoplatCodebaseSummary) -> str:        markdown_output = []        # Codebase Summary        markdown_output.append(\"# Codebase Summary\\n\")        markdown_output.append(f\"**Name:** {unoplat_codebase_summary.codebase_name or 'N/A'}\\n\")        markdown_output.append(f\"**Objective:** {unoplat_codebase_summary.codebase_objective}\\n\")        markdown_output.append(f\"**Summary:** {unoplat_codebase_summary.codebase_summary}\\n\\n\")        # Package Summaries        markdown_output.append(\"## Package Summaries\\n\")        for package_name, package_summary in unoplat_codebase_summary.codebase_package.items():            markdown_output.append(f\"### {package_name}\\n\")            markdown_output.append(f\"**Objective:** {package_summary.package_objective}\\n\")            markdown_output.append(f\"**Summary:** {package_summary.package_summary}\\n\")                        if package_summary.class_summary:                markdown_output.append(\"#### Classes\\n\")                for class_detail in package_summary.class_summary:                    markdown_output.append(f\"##### {class_detail.node_name}\\n\")                    markdown_output.append(f\"**Objective:** {class_detail.node_objective}\\n\")                    if class_detail.node_summary:                        markdown_output.append(f\"**Summary:** {class_detail.node_summary}\\n\")                        if class_detail.functions_summary:                        markdown_output.append(\"**Functions:**\\n\")                        for function in class_detail.functions_summary:                            markdown_output.append(f\"- `{function.function_name}`\\n\")                            markdown_output.append(f\"  - Objective: {function.function_summary.objective}\\n\")                            markdown_output.append(f\"  - Implementation: {function.function_summary.implementation_summary.strip().replace('\\n', ' ')}\\n\")                    markdown_output.append(\"\\n\")            if package_summary.sub_package_summaries:                markdown_output.append(\"#### Sub-packages\\n\")                for sub_package_name,sub_package in package_summary.sub_package_summaries.items():                    markdown_output.append(f\"##### {sub_package_name}\\n\")                    markdown_output.append(f\"**Objective:** {sub_package.package_objective}\\n\")                    markdown_output.append(f\"**Summary:** {sub_package.package_summary}\\n\")                    if sub_package.class_summary:                        markdown_output.append(\"**Classes:**\\n\")                        for class_detail in sub_package.class_summary:                            markdown_output.append(f\"- {class_detail.node_name}\\n\")                            markdown_output.append(f\"  - Objective: {class_detail.node_objective}\\n\")                            if class_detail.functions_summary:                                markdown_output.append(\"  - Functions:\\n\")                                for function in class_detail.functions_summary:                                    markdown_output.append(f\"    - `{function.function_name}`\\n\")                                    markdown_output.append(f\"      - Objective: {function.function_summary.objective}\\n\")                                    markdown_output.append(f\"      - Implementation: {function.function_summary.implementation_summary.strip().replace('\\n', ' ')}\\n\")                    markdown_output.append(\"\\n\")            markdown_output.append(\"\\n\")        return \"\\n\".join(markdown_output)"}],"Imports":[{"Source":"unoplat_code_confluence.data_models.forge_summary.forge_unoplat_codebase_summary","UsageName":["ForgeUnoplatCodebaseSummary"]},{"Source":"unoplat_code_confluence.markdownparser.isummariser","UsageName":["ISummariser"]}],"Position":{"StartLine":10,"StopLine":65,"StopLinePosition":41},"Content":"class MarkdownSummariser(ISummariser):        def summarise_to_markdown(self, unoplat_codebase_summary: ForgeUnoplatCodebaseSummary) -> str:        markdown_output = []        # Codebase Summary        markdown_output.append(\"# Codebase Summary\\n\")        markdown_output.append(f\"**Name:** {unoplat_codebase_summary.codebase_name or 'N/A'}\\n\")        markdown_output.append(f\"**Objective:** {unoplat_codebase_summary.codebase_objective}\\n\")        markdown_output.append(f\"**Summary:** {unoplat_codebase_summary.codebase_summary}\\n\\n\")        # Package Summaries        markdown_output.append(\"## Package Summaries\\n\")        for package_name, package_summary in unoplat_codebase_summary.codebase_package.items():            markdown_output.append(f\"### {package_name}\\n\")            markdown_output.append(f\"**Objective:** {package_summary.package_objective}\\n\")            markdown_output.append(f\"**Summary:** {package_summary.package_summary}\\n\")                        if package_summary.class_summary:                markdown_output.append(\"#### Classes\\n\")                for class_detail in package_summary.class_summary:                    markdown_output.append(f\"##### {class_detail.node_name}\\n\")                    markdown_output.append(f\"**Objective:** {class_detail.node_objective}\\n\")                    if class_detail.node_summary:                        markdown_output.append(f\"**Summary:** {class_detail.node_summary}\\n\")                        if class_detail.functions_summary:                        markdown_output.append(\"**Functions:**\\n\")                        for function in class_detail.functions_summary:                            markdown_output.append(f\"- `{function.function_name}`\\n\")                            markdown_output.append(f\"  - Objective: {function.function_summary.objective}\\n\")                            markdown_output.append(f\"  - Implementation: {function.function_summary.implementation_summary.strip().replace('\\n', ' ')}\\n\")                    markdown_output.append(\"\\n\")            if package_summary.sub_package_summaries:                markdown_output.append(\"#### Sub-packages\\n\")                for sub_package_name,sub_package in package_summary.sub_package_summaries.items():                    markdown_output.append(f\"##### {sub_package_name}\\n\")                    markdown_output.append(f\"**Objective:** {sub_package.package_objective}\\n\")                    markdown_output.append(f\"**Summary:** {sub_package.package_summary}\\n\")                    if sub_package.class_summary:                        markdown_output.append(\"**Classes:**\\n\")                        for class_detail in sub_package.class_summary:                            markdown_output.append(f\"- {class_detail.node_name}\\n\")                            markdown_output.append(f\"  - Objective: {class_detail.node_objective}\\n\")                            if class_detail.functions_summary:                                markdown_output.append(\"  - Functions:\\n\")                                for function in class_detail.functions_summary:                                    markdown_output.append(f\"    - `{function.function_name}`\\n\")                                    markdown_output.append(f\"      - Objective: {function.function_summary.objective}\\n\")                                    markdown_output.append(f\"      - Implementation: {function.function_summary.implementation_summary.strip().replace('\\n', ' ')}\\n\")                    markdown_output.append(\"\\n\")            markdown_output.append(\"\\n\")        return \"\\n\".join(markdown_output)"},{"NodeName":"default","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/__main__.py","Functions":[{"Name":"start_pipeline","FunctionCalls":[{"FunctionName":"MarkdownSummariser","Position":{"StartLine":44,"StartLinePosition":36,"StopLine":44,"StopLinePosition":37}}],"Modifiers":["async"],"Position":{"StartLine":23,"StopLine":51},"LocalVariables":[{"TypeValue":"parser","TypeType":"argparse"},{"TypeValue":"args","TypeType":"parser"},{"TypeValue":"settings","TypeType":"AppSettings"},{"TypeValue":"iload_json","TypeType":"JsonLoader"},{"TypeValue":"codebase_parser","TypeType":"CodebaseParser"},{"TypeValue":"isummariser","TypeType":"MarkdownSummariser"}],"Content":"async def start_pipeline():    parser = argparse.ArgumentParser(description=\"Codebase Parser CLI\")    parser.add_argument(        \"--config\",         help=\"Path to configuration file for unoplat utility\",         default=None,        type=str    )    args = parser.parse_args()    # Load settings with optional config override    settings = AppSettings.get_settings(args.config)        # Configure logging from settings    if settings.config.logging_handlers:        logger.configure(handlers=settings.config.logging_handlers)    # Initialize components    iload_json = JsonLoader()    codebase_parser = CodebaseParser()        isummariser = MarkdownSummariser()    # Process repositories        await get_codebase_metadata(settings, iload_json, codebase_parser, isummariser)    "},{"Name":"get_codebase_metadata","Parameters":[{"TypeValue":"settings","TypeType":"AppSettings"},{"TypeValue":"iload_json","TypeType":"JsonLoader"},{"TypeValue":"codebase_parser","TypeType":"CodebaseParser"},{"TypeValue":"isummariser","TypeType":"MarkdownSummariser"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"configure","Position":{"StartLine":55,"StartLinePosition":10,"StopLine":55,"StopLinePosition":62}}],"Modifiers":["async"],"Position":{"StartLine":51,"StopLine":65},"LocalVariables":[{"TypeValue":"parser","TypeType":"argparse"},{"TypeValue":"args","TypeType":"parser"},{"TypeValue":"settings","TypeType":"AppSettings"},{"TypeValue":"iload_json","TypeType":"JsonLoader"},{"TypeValue":"codebase_parser","TypeType":"CodebaseParser"},{"TypeValue":"isummariser","TypeType":"MarkdownSummariser"}],"Content":"async def get_codebase_metadata(settings: AppSettings, iload_json: JsonLoader, codebase_parser: CodebaseParser, isummariser: MarkdownSummariser):    # Collect necessary inputs from the user to set up the codebase indexing        #logger.configure(handlers=app_config.handlers)    logger.configure(handlers=settings.config.logging_handlers)    await start_parsing(        settings,        iload_json,        codebase_parser,        isummariser    )"},{"Name":"ensure_jar_downloaded","Parameters":[{"TypeValue":"github_token","TypeType":"str"},{"TypeValue":"arcguard_cli_repo","TypeType":"str"},{"TypeValue":"local_download_directory","TypeType":"str"}],"FunctionCalls":[{"NodeName":"Downloader","FunctionName":"download_latest_jar","Position":{"StartLine":67,"StartLinePosition":25,"StopLine":67,"StopLinePosition":103}}],"Modifiers":["async"],"Position":{"StartLine":65,"StopLine":71},"LocalVariables":[{"TypeValue":"parser","TypeType":"argparse"},{"TypeValue":"args","TypeType":"parser"},{"TypeValue":"settings","TypeType":"AppSettings"},{"TypeValue":"iload_json","TypeType":"JsonLoader"},{"TypeValue":"codebase_parser","TypeType":"CodebaseParser"},{"TypeValue":"isummariser","TypeType":"MarkdownSummariser"},{"TypeValue":"jar_path","TypeType":"Downloader"}],"Content":"async def ensure_jar_downloaded( github_token: str, arcguard_cli_repo: str, local_download_directory: str):        jar_path = Downloader.download_latest_jar(arcguard_cli_repo, local_download_directory, github_token)        return jar_path"},{"Name":"get_extension","Parameters":[{"TypeValue":"programming_language","TypeType":"str"}],"Modifiers":["async"],"Position":{"StartLine":71,"StopLine":81},"LocalVariables":[{"TypeValue":"parser","TypeType":"argparse"},{"TypeValue":"args","TypeType":"parser"},{"TypeValue":"settings","TypeType":"AppSettings"},{"TypeValue":"iload_json","TypeType":"JsonLoader"},{"TypeValue":"codebase_parser","TypeType":"CodebaseParser"},{"TypeValue":"isummariser","TypeType":"MarkdownSummariser"},{"TypeValue":"jar_path","TypeType":"Downloader"}],"Content":"async def get_extension(programming_language: str):    #TODO: convert this to enum based check    if programming_language == \"java\":        return \"java\"    elif programming_language == \"python\":        return \"py\"    else:        raise ValueError(f\"Unsupported programming language: {programming_language}\")#TODO: do it in parallel for each repository and inside repository if it is a mono repo for each codebase. .Rightnow keep it simple  as we will move to temporal soon."},{"Name":"start_parsing","Parameters":[{"TypeValue":"app_settings","TypeType":"AppSettings"},{"TypeValue":"iload_json","TypeType":"JsonLoader"},{"TypeValue":"codebase_parser","TypeType":"CodebaseParser"},{"TypeValue":"isummariser","TypeType":"MarkdownSummariser"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"info","Position":{"StartLine":161,"StartLinePosition":10,"StopLine":161,"StopLinePosition":79}}],"Modifiers":["async"],"Position":{"StartLine":81,"StopLine":164},"LocalVariables":[{"TypeValue":"parser","TypeType":"argparse"},{"TypeValue":"args","TypeType":"parser"},{"TypeValue":"settings","TypeType":"AppSettings"},{"TypeValue":"iload_json","TypeType":"JsonLoader"},{"TypeValue":"codebase_parser","TypeType":"CodebaseParser"},{"TypeValue":"isummariser","TypeType":"MarkdownSummariser"},{"TypeValue":"jar_path","TypeType":""},{"TypeValue":"github_helper","TypeType":"GithubHelper"},{"TypeValue":"github_repository","TypeType":""},{"TypeValue":"output_path","TypeType":"repository"},{"TypeValue":"programming_language","TypeType":"codebase"},{"TypeValue":"extension","TypeType":""},{"TypeValue":"codebase_path","TypeType":"os"},{"TypeValue":"archguard_handler","TypeType":"ArchGuardHandler"},{"TypeValue":"chapi_metadata_path","TypeType":"archguard_handler"},{"TypeValue":"chapi_metadata","TypeType":"iload_json"},{"TypeValue":"current_timestamp","TypeType":"datetime"},{"TypeValue":"output_filename","TypeType":"f\"{os.path.basename(codebase.name)}_{current_timestamp}.json\""},{"TypeValue":"unoplat_codebase","TypeType":""},{"TypeValue":"github_repository.codebases[codebase_index]","TypeType":"unoplat_codebase"},{"TypeValue":"json_str","TypeType":"github_repository"},{"TypeValue":"json_output_path","TypeType":"os"}],"Content":"async def start_parsing(app_settings: AppSettings, iload_json: JsonLoader, codebase_parser: CodebaseParser, isummariser: MarkdownSummariser):    \"\"\"Start parsing process for repositories and codebases.\"\"\"    logger.info(\"Starting parsing process...\")        jar_path = await ensure_jar_downloaded(        app_settings.secrets.github_token,        app_settings.config.archguard.download_url,         app_settings.config.archguard.download_directory    )        github_helper = GithubHelper(app_settings=app_settings)    # Process each repository    for index, repository in enumerate(app_settings.config.repositories):        logger.info(f\"Processing repository: {repository.git_url}\")        github_repository: UnoplatGitRepository = github_helper.clone_repository(repository)        output_path = repository.output_path                # Process each codebase in the repository        for codebase_index, codebase in enumerate(github_repository.codebases):                       logger.info(f\"Processing codebase at path: {codebase.local_path}\")                        programming_language = codebase.package_manager_metadata.programming_language #type: ignore                        logger.info(f\"Programming Language: {programming_language}\")            logger.info(f\"Output Path: {repository.output_path}\")            logger.info(f\"Codebase Name: {codebase.name}\")                        # Get file extension based on programming language            extension = await get_extension(programming_language)            codebase_path = os.path.join(codebase.local_path, codebase.name)            # Initialize the ArchGuard handler            archguard_handler = ArchGuardHandler(                jar_path=jar_path,                language=programming_language,                codebase_path=codebase_path,                codebase_name=codebase.name,                output_path=output_path,                extension=extension            )                        chapi_metadata_path = archguard_handler.run_scan()            chapi_metadata = iload_json.load_json_from_file(chapi_metadata_path)                           current_timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")            output_filename = f\"{os.path.basename(codebase.name)}_{current_timestamp}.json\"            unoplat_codebase: UnoplatCodebase = codebase_parser.parse_codebase(                json_data = chapi_metadata,                 local_workspace_path = codebase.local_path,                programming_language_metadata = app_settings.repositories[index].codebases[codebase_index].programming_language_metadata,                codebase_name=codebase.name            )            github_repository.codebases[codebase_index] = unoplat_codebase                                # Serialize repository to JSON string with proper formatting        try:            json_str = github_repository.model_dump_json(                indent=2            )                        # Write JSON output to file            json_output_path = os.path.join(repository.output_path, output_filename)            os.makedirs(os.path.dirname(json_output_path), exist_ok=True)                        with open(json_output_path, 'w', encoding='utf-8') as json_file:                json_file.write(json_str)                            logger.info(f\"Successfully wrote repository JSON schema to {json_output_path}\")                        except Exception as e:            logger.error(                f\"Error serializing/writing JSON output for repository {repository.git_url}\\n\"                f\"Codebase: {codebase.name}\\n\"                f\"Error: {str(e)}\"            )                                            logger.info(\"Parsing process completed for all repositories and codebases.\")    "},{"Name":"main","FunctionCalls":[{"NodeName":"asyncio","FunctionName":"run","Position":{"StartLine":166,"StartLinePosition":11,"StopLine":166,"StopLinePosition":32}}],"Position":{"StartLine":164,"StopLine":167},"LocalVariables":[{"TypeValue":"parser","TypeType":"argparse"},{"TypeValue":"args","TypeType":"parser"},{"TypeValue":"settings","TypeType":"AppSettings"},{"TypeValue":"iload_json","TypeType":"JsonLoader"},{"TypeValue":"codebase_parser","TypeType":"CodebaseParser"},{"TypeValue":"isummariser","TypeType":"MarkdownSummariser"},{"TypeValue":"jar_path","TypeType":""},{"TypeValue":"github_helper","TypeType":"GithubHelper"},{"TypeValue":"github_repository","TypeType":""},{"TypeValue":"output_path","TypeType":"repository"},{"TypeValue":"programming_language","TypeType":"codebase"},{"TypeValue":"extension","TypeType":""},{"TypeValue":"codebase_path","TypeType":"os"},{"TypeValue":"archguard_handler","TypeType":"ArchGuardHandler"},{"TypeValue":"chapi_metadata_path","TypeType":"archguard_handler"},{"TypeValue":"chapi_metadata","TypeType":"iload_json"},{"TypeValue":"current_timestamp","TypeType":"datetime"},{"TypeValue":"output_filename","TypeType":"f\"{os.path.basename(codebase.name)}_{current_timestamp}.json\""},{"TypeValue":"unoplat_codebase","TypeType":""},{"TypeValue":"github_repository.codebases[codebase_index]","TypeType":"unoplat_codebase"},{"TypeValue":"json_str","TypeType":"github_repository"},{"TypeValue":"json_output_path","TypeType":"os"}],"Content":"def main():    warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module='pydantic.*')    asyncio.run(start_pipeline())"}],"Imports":[{"Source":"unoplat_code_confluence.codebaseparser.arc_guard_handler","UsageName":["ArchGuardHandler"]},{"Source":"unoplat_code_confluence.configuration.settings","UsageName":["AppSettings"]},{"Source":"unoplat_code_confluence.confluence_git.github_helper","UsageName":["GithubHelper"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_codebase","UsageName":["UnoplatCodebase"]},{"Source":"unoplat_code_confluence.data_models.chapi_forge.unoplat_git_repository","UsageName":["UnoplatGitRepository"]},{"Source":"unoplat_code_confluence.downloader.downloader","UsageName":["Downloader"]},{"Source":"unoplat_code_confluence.loader.json_loader","UsageName":["JsonLoader"]},{"Source":"unoplat_code_confluence.markdownparser.markdownsummariser","UsageName":["MarkdownSummariser"]},{"Source":"unoplat_code_confluence.parser.codebase_parser","UsageName":["CodebaseParser"]},{"Source":"os"},{"Source":"argparse"},{"Source":"asyncio"},{"Source":"datetime"},{"Source":"warnings"},{"Source":"loguru","UsageName":["logger"]}],"Content":"# Standard Library"},{"NodeName":"TotalFileCount","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/utility/total_file_count.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"directory","TypeType":""},{"TypeValue":"extension","TypeType":""}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"info","Position":{"StartLine":13,"StartLinePosition":14,"StopLine":13,"StopLinePosition":101}}],"Position":{"StartLine":10,"StartLinePosition":4,"StopLine":15,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.directory","TypeType":"directory"},{"TypeValue":"self.extension","TypeType":"extension"}],"Content":"def __init__(self, directory, extension):        self.directory = directory        self.extension = extension        logger.info(f\"FileCounter initialized with directory: {directory} and extension: {extension}\")    "},{"Name":"count_files","FunctionCalls":[{"NodeName":"logger","FunctionName":"info","Position":{"StartLine":27,"StartLinePosition":14,"StopLine":27,"StopLinePosition":54}}],"Position":{"StartLine":15,"StartLinePosition":4,"StopLine":31},"LocalVariables":[{"TypeValue":"self.directory","TypeType":"directory"},{"TypeValue":"self.extension","TypeType":""},{"TypeValue":"pattern","TypeType":"os"},{"TypeValue":"files","TypeType":"glob"}],"Content":"def count_files(self):        logger.info(\"Counting files...\")        # Ensure the extension starts with a dot        if not self.extension.startswith('.'):            self.extension = '.' + self.extension                # Create a pattern for glob to match all files with the extension        pattern = os.path.join(self.directory, '**', '*' + self.extension)                # Use glob.glob with recursive=True to find all files matching the pattern        files = glob.glob(pattern, recursive=True)                logger.info(f\"Total files found: {len(files)}\")                # Return the count of files        return len(files)"}],"Imports":[{"Source":"os"},{"Source":"glob"},{"Source":"loguru","UsageName":["logger"]}],"Position":{"StartLine":9,"StopLine":31},"Content":"class TotalFileCount:    def __init__(self, directory, extension):        self.directory = directory        self.extension = extension        logger.info(f\"FileCounter initialized with directory: {directory} and extension: {extension}\")    def count_files(self):        logger.info(\"Counting files...\")        # Ensure the extension starts with a dot        if not self.extension.startswith('.'):            self.extension = '.' + self.extension                # Create a pattern for glob to match all files with the extension        pattern = os.path.join(self.directory, '**', '*' + self.extension)                # Use glob.glob with recursive=True to find all files matching the pattern        files = glob.glob(pattern, recursive=True)                logger.info(f\"Total files found: {len(files)}\")                # Return the count of files        return len(files)"},{"NodeName":"IsClassName","Type":"CLASS","FilePath":"/Users/jayghiya/.unoplat/repositories/unoplat-code-confluence/unoplat-code-confluence/unoplat_code_confluence/utility/is_class_name.py","Functions":[{"Name":"is_python_class_name","Parameters":[{"TypeValue":"name","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":56,"StartLinePosition":18,"StopLine":61,"StopLinePosition":12}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":12,"StartLinePosition":4,"StopLine":13,"StopLinePosition":4}}],"Position":{"StartLine":13,"StartLinePosition":4,"StopLine":62,"StopLinePosition":25},"LocalVariables":[{"TypeValue":"_valid_class_pattern","TypeType":"re"}],"Content":"def is_python_class_name(name: str) -> bool:        \"\"\"Check if a name follows Python class naming convention (CamelCase).                Args:            name: Name to check                    Returns:            bool: True if name follows Python class naming convention.                  Returns False for invalid inputs after logging error.        \"\"\"        try:            # Handle None or non-string types            if not isinstance(name, str):                logger.error(                    \"Invalid input type for class name check: {}\\n\"                    \"Expected str, got {}\",                     name, type(name).__name__                )                return False                            # Handle empty string            if not name:                logger.debug(\"Empty string provided for class name check\")                return False                            # Class names should:            # 1. Start with uppercase letter            # 2. Not contain underscores (CamelCase not snake_case)            # 3. Not be all uppercase (to exclude constants)            # 4. Allow single uppercase letter (e.g., A, B, T for generics)            # 5. Not contain special characters                        # Handle single character case            if len(name) == 1:                return name.isupper()                            # Check if name matches valid pattern and is not all uppercase            return bool(                IsClassName._valid_class_pattern.match(name) and                not name.isupper()            )                    except Exception as e:            logger.error(                \"Unexpected error checking class name: {}\\n\"                \"Input: {}\\n\"                \"Error: {}\",                 name, type(name).__name__, str(e)            )            return False "}],"Imports":[{"Source":"re"},{"Source":"loguru","UsageName":["logger"]}],"Position":{"StartLine":8,"StopLine":62,"StopLinePosition":25},"Content":"class IsClassName:    # Class-level regex pattern for valid Python class names    _valid_class_pattern = re.compile(r'^[A-Z][a-zA-Z0-9]*$')    @staticmethod    def is_python_class_name(name: str) -> bool:        \"\"\"Check if a name follows Python class naming convention (CamelCase).                Args:            name: Name to check                    Returns:            bool: True if name follows Python class naming convention.                  Returns False for invalid inputs after logging error.        \"\"\"        try:            # Handle None or non-string types            if not isinstance(name, str):                logger.error(                    \"Invalid input type for class name check: {}\\n\"                    \"Expected str, got {}\",                     name, type(name).__name__                )                return False                            # Handle empty string            if not name:                logger.debug(\"Empty string provided for class name check\")                return False                            # Class names should:            # 1. Start with uppercase letter            # 2. Not contain underscores (CamelCase not snake_case)            # 3. Not be all uppercase (to exclude constants)            # 4. Allow single uppercase letter (e.g., A, B, T for generics)            # 5. Not contain special characters                        # Handle single character case            if len(name) == 1:                return name.isupper()                            # Check if name matches valid pattern and is not all uppercase            return bool(                IsClassName._valid_class_pattern.match(name) and                not name.isupper()            )                    except Exception as e:            logger.error(                \"Unexpected error checking class name: {}\\n\"                \"Input: {}\\n\"                \"Error: {}\",                 name, type(name).__name__, str(e)            )            return False "}]