[{"NodeName":"GithubHelper","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/confluence_git/github_helper.py","Functions":[{"Name":"clone_repository","Parameters":[{"TypeValue":"repository_settings","TypeType":"RepositorySettings"},{"TypeValue":"github_token","TypeType":"str"}],"FunctionCalls":[{"NodeName":"codebases","FunctionName":"append","Position":{"StartLine":118,"StartLinePosition":25,"StopLine":118,"StopLinePosition":41}}],"Position":{"StartLine":28,"StartLinePosition":4,"StopLine":133,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"auth","TypeType":"Auth"},{"TypeValue":"github_client","TypeType":"Github"},{"TypeValue":"repo_url","TypeType":"repository_settings"},{"TypeValue":"repo_path","TypeType":"os"},{"TypeValue":"repo_name","TypeType":"repo_path"},{"TypeValue":"github_repo","TypeType":"github_client"},{"TypeValue":"local_path","TypeType":"os"},{"TypeValue":"repo_metadata","TypeType":""},{"TypeValue":"readme_content","TypeType":"None"},{"TypeValue":"codebases","TypeType":""},{"TypeValue":"path_components","TypeType":"codebase_config"},{"TypeValue":"source_directory","TypeType":"local_path"},{"TypeValue":"root_package_components","TypeType":"codebase_config"},{"TypeValue":"programming_language_metadata","TypeType":""},{"TypeValue":"codebase","TypeType":"UnoplatCodebase"}],"Content":"def clone_repository(self, repository_settings: RepositorySettings, github_token: str) -> UnoplatGitRepository:        \"\"\"        Clone the repository and return repository details        Works with URL formats:        - https://github.com/organization/repository        - https://github.com/organization/repository.git        - git@github.com:organization/repository.git        \"\"\"        # Initialize Github client with personal access token        auth = Auth.Token(github_token)        github_client = Github(auth=auth)        # Get repository from URL        repo_url = repository_settings.git_url        # Extract owner and repo name from different URL formats        if repo_url.startswith(\"git@\"):            # Handle SSH format: git@github.com:org/repo.git            repo_path = repo_url.split(\"github.com:\")[-1]        else:            # Handle HTTPS format: https://github.com/org/repo[.git]            repo_path = repo_url.split(\"github.com/\")[-1]        # Remove .git suffix if present        repo_path = repo_path.replace(\".git\", \"\")        repo_name = repo_path.split(\"/\")[-1]        try:            # Get repository object from Github using owner/repo format            github_repo = github_client.get_repo(repo_path)            # Create local directory if it doesn't exist            local_path = os.path.join(os.path.expanduser(\"~\"), \".unoplat\", \"repositories\")            os.makedirs(local_path, exist_ok=True)            repo_path = os.path.join(local_path, repo_name)            # Clone repository if not already cloned            if not os.path.exists(repo_path):                Repo.clone_from(repo_url, repo_path)            # Get repository metadata            repo_metadata: Dict[str, Any] = {                \"stars\": github_repo.stargazers_count,                \"forks\": github_repo.forks_count,                \"default_branch\": github_repo.default_branch,                \"created_at\": str(github_repo.created_at),                \"updated_at\": str(github_repo.updated_at),                \"language\": github_repo.language,            }            # Get README content            try:                readme_content = github_repo.get_readme().decoded_content.decode(\"utf-8\")            except:                readme_content = None            # Create UnoplatCodebase objects for each codebase config            codebases: List[UnoplatCodebase] = []            for codebase_config in repository_settings.codebases:  # type: CodebaseConfig                # First build path with codebase_folder                local_path = repo_path                if codebase_config.codebase_folder and codebase_config.codebase_folder != \".\":                    path_components = codebase_config.codebase_folder.split(\"/\")                    for component in path_components:                        local_path = os.path.join(local_path, component)                source_directory = local_path                # Then append root_package components if present                if codebase_config.root_package and codebase_config.root_package != \".\":                    root_package_components = codebase_config.root_package.split(\"/\")                    for component in root_package_components:                        local_path = os.path.join(local_path, component)                else:                    if codebase_config.programming_language_metadata.language.value == \"python\":                        raise Exception(\"Root package should be specified for python codebases\")                programming_language_metadata: ProgrammingLanguageMetadata = codebase_config.programming_language_metadata                # Verify the path exists                if not os.path.exists(local_path):                    raise Exception(f\"Codebase path not found: {local_path}\")                codebase = UnoplatCodebase(                    name=codebase_config.root_package,  # type: ignore                    local_path=local_path,                    source_directory=source_directory,                    package_manager_metadata=UnoplatPackageManagerMetadata(programming_language=programming_language_metadata.language.value, package_manager=programming_language_metadata.package_manager, programming_language_version=programming_language_metadata.language_version),                )                codebases.append(codebase)            # Create and return UnoplatGitRepository            return UnoplatGitRepository(                repository_url=repo_url,                repository_name=repo_name,                repository_metadata=repo_metadata,  # type: ignore                codebases=codebases,                readme=readme_content,                github_organization=github_repo.organization.login if github_repo.organization else None,            )        except Exception as e:            raise Exception(f\"Failed to clone repository: {str(e)}\")    "},{"Name":"close","FunctionCalls":[{"NodeName":"self","FunctionName":"github_client","Position":{"StartLine":137,"StartLinePosition":12,"StopLine":137,"StopLinePosition":13}},{"NodeName":"self","FunctionName":"close","Position":{"StartLine":137,"StartLinePosition":26,"StopLine":137,"StopLinePosition":33}}],"Position":{"StartLine":133,"StartLinePosition":4,"StopLine":138},"LocalVariables":[{"TypeValue":"auth","TypeType":"Auth"},{"TypeValue":"github_client","TypeType":"Github"},{"TypeValue":"repo_url","TypeType":"repository_settings"},{"TypeValue":"repo_path","TypeType":"os"},{"TypeValue":"repo_name","TypeType":"repo_path"},{"TypeValue":"github_repo","TypeType":"github_client"},{"TypeValue":"local_path","TypeType":"os"},{"TypeValue":"repo_metadata","TypeType":""},{"TypeValue":"readme_content","TypeType":"None"},{"TypeValue":"codebases","TypeType":""},{"TypeValue":"path_components","TypeType":"codebase_config"},{"TypeValue":"source_directory","TypeType":"local_path"},{"TypeValue":"root_package_components","TypeType":"codebase_config"},{"TypeValue":"programming_language_metadata","TypeType":""},{"TypeValue":"codebase","TypeType":"UnoplatCodebase"}],"Content":"def close(self):        \"\"\"        Close the Github client connection        \"\"\"        self.github_client.close()"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_codebase","UsageName":["UnoplatCodebase"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_git_repository","UsageName":["UnoplatGitRepository"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["ProgrammingLanguageMetadata","RepositorySettings"]},{"Source":"os"},{"Source":"typing","UsageName":["Any","Dict","List"]},{"Source":"git","UsageName":["Repo"]},{"Source":"github","UsageName":["Auth","Github"]}],"Position":{"StartLine":26,"StopLine":138},"Content":"class GithubHelper:    # works with - vhttps://github.com/organization/repository,https://github.com/organization/repository.git,git@github.com:organization/repository.git    def clone_repository(self, repository_settings: RepositorySettings, github_token: str) -> UnoplatGitRepository:        \"\"\"        Clone the repository and return repository details        Works with URL formats:        - https://github.com/organization/repository        - https://github.com/organization/repository.git        - git@github.com:organization/repository.git        \"\"\"        # Initialize Github client with personal access token        auth = Auth.Token(github_token)        github_client = Github(auth=auth)        # Get repository from URL        repo_url = repository_settings.git_url        # Extract owner and repo name from different URL formats        if repo_url.startswith(\"git@\"):            # Handle SSH format: git@github.com:org/repo.git            repo_path = repo_url.split(\"github.com:\")[-1]        else:            # Handle HTTPS format: https://github.com/org/repo[.git]            repo_path = repo_url.split(\"github.com/\")[-1]        # Remove .git suffix if present        repo_path = repo_path.replace(\".git\", \"\")        repo_name = repo_path.split(\"/\")[-1]        try:            # Get repository object from Github using owner/repo format            github_repo = github_client.get_repo(repo_path)            # Create local directory if it doesn't exist            local_path = os.path.join(os.path.expanduser(\"~\"), \".unoplat\", \"repositories\")            os.makedirs(local_path, exist_ok=True)            repo_path = os.path.join(local_path, repo_name)            # Clone repository if not already cloned            if not os.path.exists(repo_path):                Repo.clone_from(repo_url, repo_path)            # Get repository metadata            repo_metadata: Dict[str, Any] = {                \"stars\": github_repo.stargazers_count,                \"forks\": github_repo.forks_count,                \"default_branch\": github_repo.default_branch,                \"created_at\": str(github_repo.created_at),                \"updated_at\": str(github_repo.updated_at),                \"language\": github_repo.language,            }            # Get README content            try:                readme_content = github_repo.get_readme().decoded_content.decode(\"utf-8\")            except:                readme_content = None            # Create UnoplatCodebase objects for each codebase config            codebases: List[UnoplatCodebase] = []            for codebase_config in repository_settings.codebases:  # type: CodebaseConfig                # First build path with codebase_folder                local_path = repo_path                if codebase_config.codebase_folder and codebase_config.codebase_folder != \".\":                    path_components = codebase_config.codebase_folder.split(\"/\")                    for component in path_components:                        local_path = os.path.join(local_path, component)                source_directory = local_path                # Then append root_package components if present                if codebase_config.root_package and codebase_config.root_package != \".\":                    root_package_components = codebase_config.root_package.split(\"/\")                    for component in root_package_components:                        local_path = os.path.join(local_path, component)                else:                    if codebase_config.programming_language_metadata.language.value == \"python\":                        raise Exception(\"Root package should be specified for python codebases\")                programming_language_metadata: ProgrammingLanguageMetadata = codebase_config.programming_language_metadata                # Verify the path exists                if not os.path.exists(local_path):                    raise Exception(f\"Codebase path not found: {local_path}\")                codebase = UnoplatCodebase(                    name=codebase_config.root_package,  # type: ignore                    local_path=local_path,                    source_directory=source_directory,                    package_manager_metadata=UnoplatPackageManagerMetadata(programming_language=programming_language_metadata.language.value, package_manager=programming_language_metadata.package_manager, programming_language_version=programming_language_metadata.language_version),                )                codebases.append(codebase)            # Create and return UnoplatGitRepository            return UnoplatGitRepository(                repository_url=repo_url,                repository_name=repo_name,                repository_metadata=repo_metadata,  # type: ignore                codebases=codebases,                readme=readme_content,                github_organization=github_repo.organization.login if github_repo.organization else None,            )        except Exception as e:            raise Exception(f\"Failed to clone repository: {str(e)}\")    def close(self):        \"\"\"        Close the Github client connection        \"\"\"        self.github_client.close()"},{"NodeName":"CodebaseChildWorkflow","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/processor/codebase_child_workflow.py","Functions":[{"Name":"__init__","FunctionCalls":[{"FunctionName":"PackageMetadataActivity","Position":{"StartLine":28,"StartLinePosition":64,"StopLine":28,"StopLinePosition":65}}],"Position":{"StartLine":27,"StartLinePosition":4,"StopLine":30,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.package_metadata_activity","TypeType":"PackageMetadataActivity"}],"Content":"def __init__(self):        self.package_metadata_activity = PackageMetadataActivity()    "},{"Name":"run","Parameters":[{"TypeValue":"repository_qualified_name","TypeType":"str"},{"TypeValue":"codebase_qualified_name","TypeType":"str"},{"TypeValue":"local_path","TypeType":"str"},{"TypeValue":"source_directory","TypeType":"str"},{"TypeValue":"package_manager_metadata","TypeType":"UnoplatPackageManagerMetadata"}],"FunctionCalls":[{"NodeName":"workflow","FunctionName":"logger","Position":{"StartLine":61,"StartLinePosition":16,"StopLine":61,"StopLinePosition":17}},{"NodeName":"workflow","FunctionName":"info","Position":{"StartLine":61,"StartLinePosition":23,"StopLine":61,"StopLinePosition":102}}],"Annotations":[{"Name":"workflow.run","Position":{"StartLine":30,"StartLinePosition":4,"StopLine":31,"StopLinePosition":4}}],"Modifiers":["async"],"Position":{"StartLine":31,"StartLinePosition":4,"StopLine":64},"LocalVariables":[{"TypeValue":"self.package_metadata_activity","TypeType":"PackageMetadataActivity"},{"TypeValue":"programming_language_metadata","TypeType":"ProgrammingLanguageMetadata"},{"TypeValue":"parsed_metadata","TypeType":""}],"Content":"async def run(self, repository_qualified_name: str, codebase_qualified_name: str, local_path: str, source_directory: str, package_manager_metadata: UnoplatPackageManagerMetadata) -> None:        \"\"\"Execute the codebase workflow\"\"\"        workflow.logger.info(f\"Starting codebase workflow for {codebase_qualified_name}\")        # 1. Parse package metadata        workflow.logger.info(f\"Creating programming language metadata for {package_manager_metadata.programming_language}\")        programming_language_metadata = ProgrammingLanguageMetadata(language=ProgrammingLanguage(package_manager_metadata.programming_language.lower()), package_manager=PackageManagerType(package_manager_metadata.package_manager.lower()), language_version=package_manager_metadata.programming_language_version)        workflow.logger.info(f\"Parsing package metadata for {codebase_qualified_name}\")        parsed_metadata: UnoplatPackageManagerMetadata = await workflow.execute_activity(activity=PackageMetadataActivity.get_package_metadata, args=[local_path, programming_language_metadata], start_to_close_timeout=timedelta(minutes=10))        # 2. Ingest package metadata into graph        workflow.logger.info(f\"Ingesting package metadata for {codebase_qualified_name} into graph\")        await workflow.execute_activity(activity=PackageManagerMetadataIngestion.insert_package_manager_metadata, args=[codebase_qualified_name, parsed_metadata], start_to_close_timeout=timedelta(minutes=10))        # 3. Process codebase (linting, AST generation, parsing)        workflow.logger.info(f\"Processing codebase for {codebase_qualified_name}\")        await workflow.execute_activity(            activity=CodebaseProcessingActivity.process_codebase,            args=[                local_path,                source_directory,                repository_qualified_name,                codebase_qualified_name,                parsed_metadata.dependencies,                programming_language_metadata            ],            start_to_close_timeout=timedelta(minutes=30)        )        workflow.logger.info(f\"Codebase workflow completed successfully for {codebase_qualified_name}\")                "}],"Annotations":[{"Name":"workflow.defn","KeyValues":[{"Key":"name","Value":"\"child-codebase-workflow\""}],"Position":{"StartLine":25,"StopLine":26}}],"Imports":[{"Source":"datetime","UsageName":["timedelta"]},{"Source":"temporalio","UsageName":["workflow"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["PackageManagerType","ProgrammingLanguage","ProgrammingLanguageMetadata"]},{"Source":"src.code_confluence_flow_bridge.processor.codebase_processing.codebase_processing_activity","UsageName":["CodebaseProcessingActivity"]},{"Source":"src.code_confluence_flow_bridge.processor.package_metadata_activity.package_manager_metadata_activity","UsageName":["PackageMetadataActivity"]},{"Source":"src.code_confluence_flow_bridge.processor.package_metadata_activity.package_manager_metadata_ingestion","UsageName":["PackageManagerMetadataIngestion"]}],"Position":{"StartLine":26,"StopLine":64},"Content":"class  CodebaseChildWorkflow:    def __init__(self):        self.package_metadata_activity = PackageMetadataActivity()    @workflow.run    async def run(self, repository_qualified_name: str, codebase_qualified_name: str, local_path: str, source_directory: str, package_manager_metadata: UnoplatPackageManagerMetadata) -> None:        \"\"\"Execute the codebase workflow\"\"\"        workflow.logger.info(f\"Starting codebase workflow for {codebase_qualified_name}\")        # 1. Parse package metadata        workflow.logger.info(f\"Creating programming language metadata for {package_manager_metadata.programming_language}\")        programming_language_metadata = ProgrammingLanguageMetadata(language=ProgrammingLanguage(package_manager_metadata.programming_language.lower()), package_manager=PackageManagerType(package_manager_metadata.package_manager.lower()), language_version=package_manager_metadata.programming_language_version)        workflow.logger.info(f\"Parsing package metadata for {codebase_qualified_name}\")        parsed_metadata: UnoplatPackageManagerMetadata = await workflow.execute_activity(activity=PackageMetadataActivity.get_package_metadata, args=[local_path, programming_language_metadata], start_to_close_timeout=timedelta(minutes=10))        # 2. Ingest package metadata into graph        workflow.logger.info(f\"Ingesting package metadata for {codebase_qualified_name} into graph\")        await workflow.execute_activity(activity=PackageManagerMetadataIngestion.insert_package_manager_metadata, args=[codebase_qualified_name, parsed_metadata], start_to_close_timeout=timedelta(minutes=10))        # 3. Process codebase (linting, AST generation, parsing)        workflow.logger.info(f\"Processing codebase for {codebase_qualified_name}\")        await workflow.execute_activity(            activity=CodebaseProcessingActivity.process_codebase,            args=[                local_path,                source_directory,                repository_qualified_name,                codebase_qualified_name,                parsed_metadata.dependencies,                programming_language_metadata            ],            start_to_close_timeout=timedelta(minutes=30)        )        workflow.logger.info(f\"Codebase workflow completed successfully for {codebase_qualified_name}\")                "},{"NodeName":"PackageMetadataActivity","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/processor/package_metadata_activity/package_manager_metadata_activity.py","Functions":[{"Name":"__init__","FunctionCalls":[{"FunctionName":"PackageManagerParser","Position":{"StartLine":21,"StartLinePosition":58,"StopLine":21,"StopLinePosition":59}}],"Position":{"StartLine":20,"StartLinePosition":4,"StopLine":23,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.package_manager_parser","TypeType":"PackageManagerParser"}],"Content":"def __init__(self):        self.package_manager_parser = PackageManagerParser()    "},{"Name":"get_package_metadata","Parameters":[{"TypeValue":"local_path","TypeType":"str"},{"TypeValue":"programming_language_metadata","TypeType":"ProgrammingLanguageMetadata"}],"FunctionCalls":[{"NodeName":"activity","FunctionName":"logger","Position":{"StartLine":55,"StartLinePosition":20,"StopLine":55,"StopLinePosition":21}},{"NodeName":"activity","FunctionName":"error","Position":{"StartLine":55,"StartLinePosition":27,"StopLine":55,"StopLinePosition":289}}],"Annotations":[{"Name":"activity.defn","Position":{"StartLine":23,"StartLinePosition":4,"StopLine":24,"StopLinePosition":4}}],"Position":{"StartLine":24,"StartLinePosition":4,"StopLine":57},"LocalVariables":[{"TypeValue":"self.package_manager_parser","TypeType":"PackageManagerParser"},{"TypeValue":"info","TypeType":"activity"},{"TypeValue":"package_metadata","TypeType":"self"}],"Content":"def get_package_metadata(self, local_path: str, programming_language_metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"        Process package manager specific metadata        Args:            local_path: Local path to the codebase            programming_language_metadata: Programming language metadata        Returns:            UnoplatPackageManagerMetadata: Processed package manager metadata        \"\"\"        try:            info = activity.info()            activity.logger.info(                \"Processing package metadata\",                extra={                    \"temporal_workflow_id\": info.workflow_id,                    \"temporal_activity_id\": info.activity_id,                    \"codebase_path\": local_path,                    \"programming_language\": programming_language_metadata.language.value,                    \"language_version\": programming_language_metadata.language_version,                    \"package_manager\": programming_language_metadata.package_manager.value,                },            )            package_metadata = self.package_manager_parser.parse_package_metadata(local_workspace_path=local_path, programming_language_metadata=programming_language_metadata)            activity.logger.info(\"Successfully processed package metadata\", extra={\"temporal_workflow_id\": info.workflow_id, \"temporal_activity_id\": info.activity_id, \"codebase_path\": local_path, \"status\": \"success\"})            return package_metadata        except Exception as e:            activity.logger.error(\"Failed to process package metadata\", extra={\"temporal_workflow_id\": activity.info().workflow_id, \"temporal_activity_id\": activity.info().activity_id, \"error_type\": type(e).__name__, \"error_details\": str(e), \"codebase_path\": local_path, \"status\": \"error\"})            raise ApplicationError(message=f\"Failed to process package metadata for {local_path}\", type=\"PACKAGE_METADATA_ERROR\")"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["ProgrammingLanguageMetadata"]},{"Source":"src.code_confluence_flow_bridge.parser.package_manager.package_manager_parser","UsageName":["PackageManagerParser"]},{"Source":"temporalio","UsageName":["activity"]},{"Source":"temporalio.exceptions","UsageName":["ApplicationError"]}],"Position":{"StartLine":19,"StopLine":57},"Content":"class PackageMetadataActivity:    def __init__(self):        self.package_manager_parser = PackageManagerParser()    @activity.defn    def get_package_metadata(self, local_path: str, programming_language_metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"        Process package manager specific metadata        Args:            local_path: Local path to the codebase            programming_language_metadata: Programming language metadata        Returns:            UnoplatPackageManagerMetadata: Processed package manager metadata        \"\"\"        try:            info = activity.info()            activity.logger.info(                \"Processing package metadata\",                extra={                    \"temporal_workflow_id\": info.workflow_id,                    \"temporal_activity_id\": info.activity_id,                    \"codebase_path\": local_path,                    \"programming_language\": programming_language_metadata.language.value,                    \"language_version\": programming_language_metadata.language_version,                    \"package_manager\": programming_language_metadata.package_manager.value,                },            )            package_metadata = self.package_manager_parser.parse_package_metadata(local_workspace_path=local_path, programming_language_metadata=programming_language_metadata)            activity.logger.info(\"Successfully processed package metadata\", extra={\"temporal_workflow_id\": info.workflow_id, \"temporal_activity_id\": info.activity_id, \"codebase_path\": local_path, \"status\": \"success\"})            return package_metadata        except Exception as e:            activity.logger.error(\"Failed to process package metadata\", extra={\"temporal_workflow_id\": activity.info().workflow_id, \"temporal_activity_id\": activity.info().activity_id, \"error_type\": type(e).__name__, \"error_details\": str(e), \"codebase_path\": local_path, \"status\": \"error\"})            raise ApplicationError(message=f\"Failed to process package metadata for {local_path}\", type=\"PACKAGE_METADATA_ERROR\")"},{"NodeName":"PackageManagerMetadataIngestion","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/processor/package_metadata_activity/package_manager_metadata_ingestion.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"code_confluence_graph_ingestion","TypeType":"CodeConfluenceGraphIngestion"}],"FunctionCalls":[{"NodeName":"activity","FunctionName":"logger","Position":{"StartLine":19,"StartLinePosition":16,"StopLine":19,"StopLinePosition":17}},{"NodeName":"activity","FunctionName":"info","Position":{"StartLine":19,"StartLinePosition":23,"StopLine":19,"StopLinePosition":74}}],"Position":{"StartLine":17,"StartLinePosition":4,"StopLine":21,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.code_confluence_graph_ingestion","TypeType":"code_confluence_graph_ingestion"}],"Content":"def __init__(self, code_confluence_graph_ingestion: CodeConfluenceGraphIngestion):        self.code_confluence_graph_ingestion = code_confluence_graph_ingestion        activity.logger.info(\"Initialized PackageManagerMetadataIngestion\")    "},{"Name":"insert_package_manager_metadata","Parameters":[{"TypeValue":"codebase_qualified_name","TypeType":"str"},{"TypeValue":"package_manager_metadata","TypeType":"UnoplatPackageManagerMetadata"}],"FunctionCalls":[{"NodeName":"activity","FunctionName":"logger","Position":{"StartLine":42,"StartLinePosition":20,"StopLine":42,"StopLinePosition":21}},{"NodeName":"activity","FunctionName":"error","Position":{"StartLine":42,"StartLinePosition":27,"StopLine":42,"StopLinePosition":309}}],"Annotations":[{"Name":"activity.defn","Position":{"StartLine":21,"StartLinePosition":4,"StopLine":22,"StopLinePosition":4}}],"Modifiers":["async"],"Position":{"StartLine":22,"StartLinePosition":4,"StopLine":44},"LocalVariables":[{"TypeValue":"self.code_confluence_graph_ingestion","TypeType":"code_confluence_graph_ingestion"},{"TypeValue":"info","TypeType":"activity"}],"Content":"async def insert_package_manager_metadata(self, codebase_qualified_name: str, package_manager_metadata: UnoplatPackageManagerMetadata) -> None:        \"\"\"        Insert package manager metadata into graph database        Args:            codebase_qualified_name: Qualified name of the codebase            package_manager_metadata: Package manager metadata to insert        \"\"\"        try:            info = activity.info()            activity.logger.info(                \"Starting package manager metadata ingestion\",                extra={\"temporal_workflow_id\": info.workflow_id, \"temporal_activity_id\": info.activity_id, \"codebase_name\": codebase_qualified_name, \"programming_language\": package_manager_metadata.programming_language, \"package_manager\": package_manager_metadata.package_manager},            )            await self.code_confluence_graph_ingestion.insert_code_confluence_codebase_package_manager_metadata(codebase_qualified_name=codebase_qualified_name, package_manager_metadata=package_manager_metadata)            activity.logger.info(\"Successfully ingested package manager metadata\", extra={\"temporal_workflow_id\": info.workflow_id, \"temporal_activity_id\": info.activity_id, \"codebase_name\": codebase_qualified_name, \"status\": \"success\"})        except Exception as e:            activity.logger.error(\"Failed to ingest package manager metadata\", extra={\"temporal_workflow_id\": activity.info().workflow_id, \"temporal_activity_id\": activity.info().activity_id, \"error_type\": type(e).__name__, \"error_details\": str(e), \"codebase_name\": codebase_qualified_name, \"status\": \"error\"})            raise ApplicationError(message=f\"Failed to ingest package manager metadata for {codebase_qualified_name}\", type=\"PACKAGE_METADATA_INGESTION_ERROR\")"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"src.code_confluence_flow_bridge.processor.db.graph_db.code_confluence_graph_ingestion","UsageName":["CodeConfluenceGraphIngestion"]},{"Source":"temporalio","UsageName":["activity"]},{"Source":"temporalio.exceptions","UsageName":["ApplicationError"]}],"Position":{"StartLine":12,"StopLine":44},"Content":"class PackageManagerMetadataIngestion:    \"\"\"    Temporal activity class for package manager metadata ingestion    \"\"\"    def __init__(self, code_confluence_graph_ingestion: CodeConfluenceGraphIngestion):        self.code_confluence_graph_ingestion = code_confluence_graph_ingestion        activity.logger.info(\"Initialized PackageManagerMetadataIngestion\")    @activity.defn    async def insert_package_manager_metadata(self, codebase_qualified_name: str, package_manager_metadata: UnoplatPackageManagerMetadata) -> None:        \"\"\"        Insert package manager metadata into graph database        Args:            codebase_qualified_name: Qualified name of the codebase            package_manager_metadata: Package manager metadata to insert        \"\"\"        try:            info = activity.info()            activity.logger.info(                \"Starting package manager metadata ingestion\",                extra={\"temporal_workflow_id\": info.workflow_id, \"temporal_activity_id\": info.activity_id, \"codebase_name\": codebase_qualified_name, \"programming_language\": package_manager_metadata.programming_language, \"package_manager\": package_manager_metadata.package_manager},            )            await self.code_confluence_graph_ingestion.insert_code_confluence_codebase_package_manager_metadata(codebase_qualified_name=codebase_qualified_name, package_manager_metadata=package_manager_metadata)            activity.logger.info(\"Successfully ingested package manager metadata\", extra={\"temporal_workflow_id\": info.workflow_id, \"temporal_activity_id\": info.activity_id, \"codebase_name\": codebase_qualified_name, \"status\": \"success\"})        except Exception as e:            activity.logger.error(\"Failed to ingest package manager metadata\", extra={\"temporal_workflow_id\": activity.info().workflow_id, \"temporal_activity_id\": activity.info().activity_id, \"error_type\": type(e).__name__, \"error_details\": str(e), \"codebase_name\": codebase_qualified_name, \"status\": \"error\"})            raise ApplicationError(message=f\"Failed to ingest package manager metadata for {codebase_qualified_name}\", type=\"PACKAGE_METADATA_INGESTION_ERROR\")"},{"NodeName":"GitActivity","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/processor/git_activity/confluence_git_activity.py","Functions":[{"Name":"__init__","FunctionCalls":[{"NodeName":"activity","FunctionName":"logger","Position":{"StartLine":20,"StartLinePosition":16,"StopLine":20,"StopLinePosition":17}},{"NodeName":"activity","FunctionName":"info","Position":{"StartLine":20,"StartLinePosition":23,"StopLine":20,"StopLinePosition":72}}],"Position":{"StartLine":18,"StartLinePosition":4,"StopLine":22,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.github_helper","TypeType":"GithubHelper"}],"Content":"def __init__(self):        self.github_helper = GithubHelper()        activity.logger.info(\"Initialized GitActivity with GithubHelper\")    "},{"Name":"process_git_activity","Parameters":[{"TypeValue":"repository_settings","TypeType":"RepositorySettings"},{"TypeValue":"github_token","TypeType":"str"}],"FunctionCalls":[{"NodeName":"activity","FunctionName":"logger","Position":{"StartLine":41,"StartLinePosition":20,"StopLine":41,"StopLinePosition":21}},{"NodeName":"activity","FunctionName":"error","Position":{"StartLine":41,"StartLinePosition":27,"StopLine":41,"StopLinePosition":296}}],"Annotations":[{"Name":"activity.defn","Position":{"StartLine":22,"StartLinePosition":4,"StopLine":23,"StopLinePosition":4}}],"Position":{"StartLine":23,"StartLinePosition":4,"StopLine":43},"LocalVariables":[{"TypeValue":"self.github_helper","TypeType":"GithubHelper"},{"TypeValue":"info","TypeType":"activity"},{"TypeValue":"activity_data","TypeType":"self"}],"Content":"def process_git_activity(self, repository_settings: RepositorySettings, github_token: str) -> UnoplatGitRepository:        \"\"\"        Process Git activity using GithubHelper        Returns:            UnoplatGitRepository containing processed git activity data        \"\"\"        try:            # Get activity info for context            info = activity.info()            activity.logger.info(\"Starting git activity processing\", extra={\"temporal_workflow_id\": info.workflow_id, \"temporal_activity_id\": info.activity_id, \"temporal_attempt\": info.attempt, \"git_url\": repository_settings.git_url})            activity_data = self.github_helper.clone_repository(repository_settings, github_token)            activity.logger.info(\"Successfully processed git activity\", extra={\"temporal_workflow_id\": info.workflow_id, \"temporal_activity_id\": info.activity_id, \"git_url\": repository_settings.git_url, \"status\": \"success\"})            return activity_data        except Exception as e:            activity.logger.error(\"Failed to process git activity\", extra={\"temporal_workflow_id\": activity.info().workflow_id, \"temporal_activity_id\": activity.info().activity_id, \"error_type\": type(e).__name__, \"error_details\": str(e), \"git_url\": repository_settings.git_url, \"status\": \"error\"})            raise ApplicationError(message=\"Failed to process git activity\", type=\"GIT_ACTIVITY_ERROR\", details=[{\"repository\": repository_settings.git_url, \"error\": str(e)}])"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.confluence_git.github_helper","UsageName":["GithubHelper"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_git_repository","UsageName":["UnoplatGitRepository"]},{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["RepositorySettings"]},{"Source":"temporalio","UsageName":["activity"]},{"Source":"temporalio.exceptions","UsageName":["ApplicationError"]}],"Position":{"StartLine":13,"StopLine":43},"Content":"class GitActivity:    \"\"\"    Temporal activity class for GitHub operations using GithubHelper    \"\"\"    def __init__(self):        self.github_helper = GithubHelper()        activity.logger.info(\"Initialized GitActivity with GithubHelper\")    @activity.defn    def process_git_activity(self, repository_settings: RepositorySettings, github_token: str) -> UnoplatGitRepository:        \"\"\"        Process Git activity using GithubHelper        Returns:            UnoplatGitRepository containing processed git activity data        \"\"\"        try:            # Get activity info for context            info = activity.info()            activity.logger.info(\"Starting git activity processing\", extra={\"temporal_workflow_id\": info.workflow_id, \"temporal_activity_id\": info.activity_id, \"temporal_attempt\": info.attempt, \"git_url\": repository_settings.git_url})            activity_data = self.github_helper.clone_repository(repository_settings, github_token)            activity.logger.info(\"Successfully processed git activity\", extra={\"temporal_workflow_id\": info.workflow_id, \"temporal_activity_id\": info.activity_id, \"git_url\": repository_settings.git_url, \"status\": \"success\"})            return activity_data        except Exception as e:            activity.logger.error(\"Failed to process git activity\", extra={\"temporal_workflow_id\": activity.info().workflow_id, \"temporal_activity_id\": activity.info().activity_id, \"error_type\": type(e).__name__, \"error_details\": str(e), \"git_url\": repository_settings.git_url, \"status\": \"error\"})            raise ApplicationError(message=\"Failed to process git activity\", type=\"GIT_ACTIVITY_ERROR\", details=[{\"repository\": repository_settings.git_url, \"error\": str(e)}])"},{"NodeName":"ConfluenceGitGraph","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/processor/git_activity/confluence_git_graph.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"code_confluence_graph_ingestion","TypeType":"CodeConfluenceGraphIngestion"}],"FunctionCalls":[{"NodeName":"activity","FunctionName":"logger","Position":{"StartLine":22,"StartLinePosition":16,"StopLine":22,"StopLinePosition":17}},{"NodeName":"activity","FunctionName":"info","Position":{"StartLine":22,"StartLinePosition":23,"StopLine":22,"StopLinePosition":95}}],"Position":{"StartLine":20,"StartLinePosition":4,"StopLine":24,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.code_confluence_graph_ingestion","TypeType":"code_confluence_graph_ingestion"}],"Content":"def __init__(self, code_confluence_graph_ingestion: CodeConfluenceGraphIngestion):        self.code_confluence_graph_ingestion = code_confluence_graph_ingestion        activity.logger.info(\"Initialized ConfluenceGitGraph with CodeConfluenceGraphIngestion\")    "},{"Name":"insert_git_repo_into_graph_db","Parameters":[{"TypeValue":"git_repo","TypeType":"UnoplatGitRepository"}],"FunctionCalls":[{"NodeName":"activity","FunctionName":"logger","Position":{"StartLine":46,"StartLinePosition":20,"StopLine":46,"StopLinePosition":21}},{"NodeName":"activity","FunctionName":"error","Position":{"StartLine":46,"StartLinePosition":27,"StopLine":46,"StopLinePosition":195}}],"Annotations":[{"Name":"activity.defn","Position":{"StartLine":24,"StartLinePosition":4,"StopLine":25,"StopLinePosition":4}}],"Modifiers":["async"],"Position":{"StartLine":25,"StartLinePosition":4,"StopLine":48},"LocalVariables":[{"TypeValue":"self.code_confluence_graph_ingestion","TypeType":"code_confluence_graph_ingestion"},{"TypeValue":"info","TypeType":"activity"},{"TypeValue":"parent_child_clone_metadata","TypeType":""},{"TypeValue":"error_msg","TypeType":"f\"Failed to insert git repo into graph db: {git_repo.repository_url}\""}],"Content":"async def insert_git_repo_into_graph_db(self, git_repo: UnoplatGitRepository) -> ParentChildCloneMetadata:        \"\"\"        Insert a git repository into the graph database        Args:            git_repo: UnoplatGitRepository containing git repository data        Returns:            ParentChildCloneMetadata containing the qualified name of the git repository and the codebase qualified names        \"\"\"        try:            info = activity.info()            activity.logger.info(\"Starting graph db insertion\", extra={\"workflow_id\": info.workflow_id, \"activity_id\": info.activity_id, \"repository\": git_repo.repository_url})            parent_child_clone_metadata = await self.code_confluence_graph_ingestion.insert_code_confluence_git_repo(git_repo=git_repo)            activity.logger.info(\"Successfully inserted git repo into graph db\", extra={\"workflow_id\": info.workflow_id, \"activity_id\": info.activity_id, \"repository\": git_repo.repository_url})            return parent_child_clone_metadata        except Exception as e:            error_msg = f\"Failed to insert git repo into graph db: {git_repo.repository_url}\"            activity.logger.error(error_msg, extra={\"workflow_id\": activity.info().workflow_id, \"activity_id\": activity.info().activity_id, \"error\": str(e), \"repository\": git_repo.repository_url})            raise ApplicationError(message=error_msg, type=\"GRAPH_INGESTION_ERROR\")"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_git_repository","UsageName":["UnoplatGitRepository"]},{"Source":"src.code_confluence_flow_bridge.models.workflow.parent_child_clone_metadata","UsageName":["ParentChildCloneMetadata"]},{"Source":"src.code_confluence_flow_bridge.processor.db.graph_db.code_confluence_graph_ingestion","UsageName":["CodeConfluenceGraphIngestion"]},{"Source":"temporalio","UsageName":["activity"]},{"Source":"temporalio.exceptions","UsageName":["ApplicationError"]}],"Position":{"StartLine":15,"StopLine":48},"Content":"class ConfluenceGitGraph:    \"\"\"    Temporal activity class for GitHub operations using GithubHelper    \"\"\"    def __init__(self, code_confluence_graph_ingestion: CodeConfluenceGraphIngestion):        self.code_confluence_graph_ingestion = code_confluence_graph_ingestion        activity.logger.info(\"Initialized ConfluenceGitGraph with CodeConfluenceGraphIngestion\")    @activity.defn    async def insert_git_repo_into_graph_db(self, git_repo: UnoplatGitRepository) -> ParentChildCloneMetadata:        \"\"\"        Insert a git repository into the graph database        Args:            git_repo: UnoplatGitRepository containing git repository data        Returns:            ParentChildCloneMetadata containing the qualified name of the git repository and the codebase qualified names        \"\"\"        try:            info = activity.info()            activity.logger.info(\"Starting graph db insertion\", extra={\"workflow_id\": info.workflow_id, \"activity_id\": info.activity_id, \"repository\": git_repo.repository_url})            parent_child_clone_metadata = await self.code_confluence_graph_ingestion.insert_code_confluence_git_repo(git_repo=git_repo)            activity.logger.info(\"Successfully inserted git repo into graph db\", extra={\"workflow_id\": info.workflow_id, \"activity_id\": info.activity_id, \"repository\": git_repo.repository_url})            return parent_child_clone_metadata        except Exception as e:            error_msg = f\"Failed to insert git repo into graph db: {git_repo.repository_url}\"            activity.logger.error(error_msg, extra={\"workflow_id\": activity.info().workflow_id, \"activity_id\": activity.info().activity_id, \"error\": str(e), \"repository\": git_repo.repository_url})            raise ApplicationError(message=error_msg, type=\"GRAPH_INGESTION_ERROR\")"},{"NodeName":"CodebaseProcessingActivity","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/processor/codebase_processing/codebase_processing_activity.py","Functions":[{"Name":"process_codebase","Parameters":[{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"source_directory","TypeType":"str"},{"TypeValue":"repository_qualified_name","TypeType":"str"},{"TypeValue":"codebase_qualified_name","TypeType":"str"},{"TypeValue":"dependencies","TypeType":"Optional[List[str]]"},{"TypeValue":"programming_language_metadata","TypeType":"ProgrammingLanguageMetadata"}],"FunctionCalls":[{"NodeName":"activity","FunctionName":"logger","Position":{"StartLine":85,"StartLinePosition":16,"StopLine":85,"StopLinePosition":17}},{"NodeName":"activity","FunctionName":"info","Position":{"StartLine":85,"StartLinePosition":23,"StopLine":85,"StopLinePosition":91}}],"Annotations":[{"Name":"activity.defn","Position":{"StartLine":23,"StartLinePosition":4,"StopLine":24,"StopLinePosition":4}}],"Modifiers":["async"],"Position":{"StartLine":24,"StartLinePosition":4,"StopLine":87},"LocalVariables":[{"TypeValue":"linter_parser","TypeType":"LinterParser"},{"TypeValue":"lint_result","TypeType":"linter_parser"},{"TypeValue":"scanner_jar_path","TypeType":"str"},{"TypeValue":"arch_guard","TypeType":"ArchGuardHandler"},{"TypeValue":"chapi_json_path","TypeType":"arch_guard"},{"TypeValue":"json_data","TypeType":"json"},{"TypeValue":"parser","TypeType":"CodebaseParser"},{"TypeValue":"list_packages","TypeType":""}],"Content":"async def process_codebase(        self,        local_workspace_path: str,        source_directory: str,        repository_qualified_name: str,        codebase_qualified_name: str,        dependencies: Optional[List[str]],        programming_language_metadata: ProgrammingLanguageMetadata,    ) -> List[UnoplatPackage]:        \"\"\"        Process codebase through linting, AST generation, and parsing.        Args:            local_workspace_path: Path to the local workspace            codebase_name: Name of the codebase            dependencies: List of project dependencies            programming_language_metadata: Metadata about the programming language        Returns:            UnoplatCodebase: Parsed codebase data        \"\"\"        activity.logger.info(f\"Starting codebase processing for {codebase_qualified_name}\")                linter_parser = LinterParser()        lint_result = linter_parser.lint_codebase(            local_workspace_path=local_workspace_path,            dependencies=[],            programming_language_metadata=programming_language_metadata        )                if not lint_result:            activity.logger.exception(\"Linting process completed with warnings\")        # 2. Generate AST using ArchGuard        scanner_jar_path = str(Path(__file__).parent.parent.parent.parent.parent / \"assets\" / \"scanner_cli-2.2.8-all.jar\")                arch_guard = ArchGuardHandler(            jar_path=scanner_jar_path,            language=programming_language_metadata.language.value,            codebase_path=local_workspace_path,            codebase_name=codebase_qualified_name,            output_path=local_workspace_path,            extension=\".py\"  # For Python files        )                chapi_json_path = arch_guard.run_scan()        # 3. Parse the codebase using CodebaseParser        with open(chapi_json_path, 'r') as f:            json_data = json.load(f)        parser = CodebaseParser()        list_packages: List[UnoplatPackage] = parser.parse_codebase(            codebase_name=codebase_qualified_name,            json_data=json_data,            local_workspace_path=local_workspace_path,            source_directory=source_directory,            programming_language_metadata=programming_language_metadata        )        activity.logger.info(f\"Completed codebase processing for {codebase_qualified_name}\")        return list_packages "}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package","UsageName":["UnoplatPackage"]},{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["ProgrammingLanguageMetadata"]},{"Source":"src.code_confluence_flow_bridge.parser.codebase_parser","UsageName":["CodebaseParser"]},{"Source":"src.code_confluence_flow_bridge.parser.linters.linter_parser","UsageName":["LinterParser"]},{"Source":"src.code_confluence_flow_bridge.processor.archguard.arc_guard_handler","UsageName":["ArchGuardHandler"]},{"Source":"json"},{"Source":"pathlib","UsageName":["Path"]},{"Source":"typing","UsageName":["List","Optional"]},{"Source":"temporalio","UsageName":["activity"]}],"Position":{"StartLine":20,"StopLine":87},"Content":"class CodebaseProcessingActivity:    \"\"\"Activity for processing codebase through linting, AST generation and parsing.\"\"\"    @activity.defn      async def process_codebase(        self,        local_workspace_path: str,        source_directory: str,        repository_qualified_name: str,        codebase_qualified_name: str,        dependencies: Optional[List[str]],        programming_language_metadata: ProgrammingLanguageMetadata,    ) -> List[UnoplatPackage]:        \"\"\"        Process codebase through linting, AST generation, and parsing.        Args:            local_workspace_path: Path to the local workspace            codebase_name: Name of the codebase            dependencies: List of project dependencies            programming_language_metadata: Metadata about the programming language        Returns:            UnoplatCodebase: Parsed codebase data        \"\"\"        activity.logger.info(f\"Starting codebase processing for {codebase_qualified_name}\")                linter_parser = LinterParser()        lint_result = linter_parser.lint_codebase(            local_workspace_path=local_workspace_path,            dependencies=[],            programming_language_metadata=programming_language_metadata        )                if not lint_result:            activity.logger.exception(\"Linting process completed with warnings\")        # 2. Generate AST using ArchGuard        scanner_jar_path = str(Path(__file__).parent.parent.parent.parent.parent / \"assets\" / \"scanner_cli-2.2.8-all.jar\")                arch_guard = ArchGuardHandler(            jar_path=scanner_jar_path,            language=programming_language_metadata.language.value,            codebase_path=local_workspace_path,            codebase_name=codebase_qualified_name,            output_path=local_workspace_path,            extension=\".py\"  # For Python files        )                chapi_json_path = arch_guard.run_scan()        # 3. Parse the codebase using CodebaseParser        with open(chapi_json_path, 'r') as f:            json_data = json.load(f)        parser = CodebaseParser()        list_packages: List[UnoplatPackage] = parser.parse_codebase(            codebase_name=codebase_qualified_name,            json_data=json_data,            local_workspace_path=local_workspace_path,            source_directory=source_directory,            programming_language_metadata=programming_language_metadata        )        activity.logger.info(f\"Completed codebase processing for {codebase_qualified_name}\")        return list_packages "},{"NodeName":"CodeConfluenceGraphIngestion","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/processor/db/graph_db/code_confluence_graph_ingestion.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"code_confluence_env","TypeType":"EnvironmentSettings"}],"FunctionCalls":[{"FunctionName":"CodeConfluenceGraph","Position":{"StartLine":33,"StartLinePosition":56,"StopLine":33,"StopLinePosition":96}}],"Position":{"StartLine":32,"StartLinePosition":4,"StopLine":35,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.code_confluence_graph","TypeType":"CodeConfluenceGraph"}],"Content":"def __init__(self, code_confluence_env: EnvironmentSettings):        self.code_confluence_graph = CodeConfluenceGraph(code_confluence_env=code_confluence_env)    "},{"Name":"initialize","FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":42,"StartLinePosition":18,"StopLine":42,"StopLinePosition":64}}],"Modifiers":["async"],"Position":{"StartLine":35,"StartLinePosition":4,"StopLine":45,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.code_confluence_graph","TypeType":"CodeConfluenceGraph"}],"Content":"async def initialize(self) -> None:        \"\"\"Initialize graph connection and schema\"\"\"        try:            await self.code_confluence_graph.connect()            await self.code_confluence_graph.create_schema()            logger.info(\"Graph initialization complete\")        except Exception as e:            logger.error(f\"Failed to initialize graph: {str(e)}\")            raise    "},{"Name":"close","Modifiers":["async"],"Position":{"StartLine":45,"StartLinePosition":4,"StopLine":49,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.code_confluence_graph","TypeType":"CodeConfluenceGraph"}],"Content":"async def close(self) -> None:        \"\"\"Close graph connection\"\"\"        await self.code_confluence_graph.close()    "},{"Name":"insert_code_confluence_git_repo","Parameters":[{"TypeValue":"git_repo","TypeType":"UnoplatGitRepository"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":99,"StartLinePosition":18,"StopLine":99,"StopLinePosition":49}}],"Modifiers":["async"],"Position":{"StartLine":49,"StartLinePosition":4,"StopLine":102,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.code_confluence_graph","TypeType":"CodeConfluenceGraph"},{"TypeValue":"qualified_name","TypeType":"f\"{git_repo.github_organization}_{git_repo.repository_name}\""},{"TypeValue":"parent_child_clone_metadata","TypeType":"ParentChildCloneMetadata"},{"TypeValue":"repo_dict","TypeType":"{\"qualified_name\":qualified_name,\"repository_url\":git_repo.repository_url,\"repository_name\":git_repo.repository_name,\"repository_metadata\":git_repo.repository_metadata,\"readme\":git_repo.readme,\"github_organization\":git_repo.github_organization}"},{"TypeValue":"repo_results","TypeType":""},{"TypeValue":"repo_node","TypeType":"repo_results"},{"TypeValue":"codebase_qualified_name","TypeType":"f\"{qualified_name}_{codebase.name}\""},{"TypeValue":"codebase_dict","TypeType":"{\"qualified_name\":codebase_qualified_name,\"name\":codebase.name,\"readme\":codebase.readme,\"local_path\":codebase.local_path}"},{"TypeValue":"codebase_results","TypeType":""},{"TypeValue":"codebase_node","TypeType":"codebase_results"},{"TypeValue":"error_msg","TypeType":"f\"Failed to insert repository {qualified_name}\""}],"Content":"async def insert_code_confluence_git_repo(self, git_repo: UnoplatGitRepository) -> ParentChildCloneMetadata:        \"\"\"        Insert a git repository into the graph database        Args:            git_repo: UnoplatGitRepository containing git repository data        Returns:            ParentChildCloneMetadata: Metadata about created nodes        Raises:            ApplicationError: If repository insertion fails        \"\"\"        qualified_name = f\"{git_repo.github_organization}_{git_repo.repository_name}\"        parent_child_clone_metadata = ParentChildCloneMetadata(repository_qualified_name=qualified_name, codebase_qualified_names=[])        try:            async with self.code_confluence_graph.transaction:                # Create repository node                repo_dict = {\"qualified_name\": qualified_name, \"repository_url\": git_repo.repository_url, \"repository_name\": git_repo.repository_name, \"repository_metadata\": git_repo.repository_metadata, \"readme\": git_repo.readme, \"github_organization\": git_repo.github_organization}                repo_results = await CodeConfluenceGitRepository.create_or_update(repo_dict)                if not repo_results:                    raise ApplicationError(message=f\"Failed to create repository node: {qualified_name}\", type=\"REPOSITORY_CREATION_ERROR\", details=[{\"repository\": qualified_name}])                repo_node = repo_results[0]                logger.success(f\"Created repository node: {qualified_name}\")                # Create codebase nodes and establish relationships                for codebase in git_repo.codebases:                    codebase_qualified_name = f\"{qualified_name}_{codebase.name}\"                    codebase_dict = {\"qualified_name\": codebase_qualified_name, \"name\": codebase.name, \"readme\": codebase.readme, \"local_path\": codebase.local_path}                    parent_child_clone_metadata.codebase_qualified_names.append(codebase_qualified_name)                    codebase_results = await CodeConfluenceCodebase.create_or_update(codebase_dict)                    if not codebase_results:                        raise ApplicationError(message=f\"Failed to create codebase node: {codebase.name}\", type=\"CODEBASE_CREATION_ERROR\", details=[{\"repository\": qualified_name, \"codebase\": codebase.name}])                    codebase_node = codebase_results[0]                    # Establish relationships                    await repo_node.codebases.connect(codebase_node)                    await codebase_node.git_repository.connect(repo_node)                logger.success(f\"Successfully ingested repository {qualified_name}\")                return parent_child_clone_metadata        except Exception as e:            error_msg = f\"Failed to insert repository {qualified_name}\"            logger.error(f\"{error_msg}: {str(e)}\")            raise ApplicationError(message=error_msg, type=\"GRAPH_INGESTION_ERROR\") from e    "},{"Name":"insert_code_confluence_codebase_package_manager_metadata","Parameters":[{"TypeValue":"codebase_qualified_name","TypeType":"str"},{"TypeValue":"package_manager_metadata","TypeType":"UnoplatPackageManagerMetadata"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":146,"StartLinePosition":18,"StopLine":146,"StopLinePosition":49}}],"Modifiers":["async"],"Position":{"StartLine":102,"StartLinePosition":4,"StopLine":148},"LocalVariables":[{"TypeValue":"self.code_confluence_graph","TypeType":"CodeConfluenceGraph"},{"TypeValue":"qualified_name","TypeType":"f\"{git_repo.github_organization}_{git_repo.repository_name}\""},{"TypeValue":"parent_child_clone_metadata","TypeType":"ParentChildCloneMetadata"},{"TypeValue":"repo_dict","TypeType":"{\"qualified_name\":qualified_name,\"repository_url\":git_repo.repository_url,\"repository_name\":git_repo.repository_name,\"repository_metadata\":git_repo.repository_metadata,\"readme\":git_repo.readme,\"github_organization\":git_repo.github_organization}"},{"TypeValue":"repo_results","TypeType":""},{"TypeValue":"repo_node","TypeType":"repo_results"},{"TypeValue":"codebase_qualified_name","TypeType":"f\"{qualified_name}_{codebase.name}\""},{"TypeValue":"codebase_dict","TypeType":"{\"qualified_name\":codebase_qualified_name,\"name\":codebase.name,\"readme\":codebase.readme,\"local_path\":codebase.local_path}"},{"TypeValue":"codebase_results","TypeType":""},{"TypeValue":"codebase_node","TypeType":""},{"TypeValue":"error_msg","TypeType":"f\"Failed to insert package manager metadata for {codebase_qualified_name}\""},{"TypeValue":"metadata_dict","TypeType":"{\"qualified_name\":f\"{codebase_qualified_name}_package_manager_metadata\",\"dependencies\":{k:v.model_dump()fork,vinpackage_manager_metadata.dependencies.items()},\"package_manager\":package_manager_metadata.package_manager,\"programming_language\":package_manager_metadata.programming_language,\"programming_language_version\":package_manager_metadata.programming_language_version,\"project_version\":package_manager_metadata.project_version,\"description\":package_manager_metadata.description,\"license\":package_manager_metadata.license,\"package_name\":package_manager_metadata.package_name,\"entry_points\":package_manager_metadata.entry_points,\"authors\":package_manager_metadata.authorsor[],}"},{"TypeValue":"metadata_results","TypeType":""},{"TypeValue":"metadata_node","TypeType":""}],"Content":"async def insert_code_confluence_codebase_package_manager_metadata(self, codebase_qualified_name: str, package_manager_metadata: UnoplatPackageManagerMetadata) -> None:        \"\"\"        Insert codebase package manager metadata into the graph database        Args:            codebase_qualified_name: Qualified name of the codebase            package_manager_metadata: UnoplatPackageManagerMetadata containing package manager metadata        \"\"\"        try:            async with self.code_confluence_graph.transaction:                # Use get() instead of filter() for unique index                try:                    codebase_node = await CodeConfluenceCodebase.nodes.get(qualified_name=codebase_qualified_name)                except CodeConfluenceCodebase.DoesNotExist:                    raise ApplicationError(message=f\"Codebase not found: {codebase_qualified_name}\", type=\"CODEBASE_NOT_FOUND\")                # Create package manager metadata node                metadata_dict = {                    \"qualified_name\": f\"{codebase_qualified_name}_package_manager_metadata\",                    \"dependencies\": {k: v.model_dump() for k, v in package_manager_metadata.dependencies.items()},                    \"package_manager\": package_manager_metadata.package_manager,                    \"programming_language\": package_manager_metadata.programming_language,                    \"programming_language_version\": package_manager_metadata.programming_language_version,                    \"project_version\": package_manager_metadata.project_version,                    \"description\": package_manager_metadata.description,                    \"license\": package_manager_metadata.license,                    \"package_name\": package_manager_metadata.package_name,                    \"entry_points\": package_manager_metadata.entry_points,                    \"authors\": package_manager_metadata.authors or [],                }                metadata_results = await CodeConfluencePackageManagerMetadata.create_or_update(metadata_dict)                if not metadata_results:                    raise ApplicationError(message=f\"Failed to create package manager metadata for {codebase_qualified_name}\", type=\"METADATA_CREATION_ERROR\")                metadata_node: CodeConfluencePackageManagerMetadata = metadata_results[0]                # Connect metadata to codebase                await codebase_node.package_manager_metadata.connect(metadata_node)                logger.success(f\"Successfully inserted package manager metadata for {codebase_qualified_name}\")        except Exception as e:            error_msg = f\"Failed to insert package manager metadata for {codebase_qualified_name}\"            logger.error(f\"{error_msg}: {str(e)}\")            raise ApplicationError(message=error_msg, type=\"PACKAGE_METADATA_ERROR\")"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_git_repository","UsageName":["UnoplatGitRepository"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["EnvironmentSettings"]},{"Source":"src.code_confluence_flow_bridge.models.workflow.parent_child_clone_metadata","UsageName":["ParentChildCloneMetadata"]},{"Source":"src.code_confluence_flow_bridge.processor.db.graph_db.code_confluence_graph","UsageName":["CodeConfluenceGraph"]},{"Source":"loguru","UsageName":["logger"]},{"Source":"temporalio.exceptions","UsageName":["ApplicationError"]},{"Source":"unoplat_code_confluence_commons","UsageName":["CodeConfluencePackageManagerMetadata"]},{"Source":"unoplat_code_confluence_commons.graph_models.code_confluence_codebase","UsageName":["CodeConfluenceCodebase"]},{"Source":"unoplat_code_confluence_commons.graph_models.code_confluence_git_repository","UsageName":["CodeConfluenceGitRepository"]}],"Position":{"StartLine":31,"StopLine":148},"Content":"class CodeConfluenceGraphIngestion:    def __init__(self, code_confluence_env: EnvironmentSettings):        self.code_confluence_graph = CodeConfluenceGraph(code_confluence_env=code_confluence_env)    async def initialize(self) -> None:        \"\"\"Initialize graph connection and schema\"\"\"        try:            await self.code_confluence_graph.connect()            await self.code_confluence_graph.create_schema()            logger.info(\"Graph initialization complete\")        except Exception as e:            logger.error(f\"Failed to initialize graph: {str(e)}\")            raise    async def close(self) -> None:        \"\"\"Close graph connection\"\"\"        await self.code_confluence_graph.close()    async def insert_code_confluence_git_repo(self, git_repo: UnoplatGitRepository) -> ParentChildCloneMetadata:        \"\"\"        Insert a git repository into the graph database        Args:            git_repo: UnoplatGitRepository containing git repository data        Returns:            ParentChildCloneMetadata: Metadata about created nodes        Raises:            ApplicationError: If repository insertion fails        \"\"\"        qualified_name = f\"{git_repo.github_organization}_{git_repo.repository_name}\"        parent_child_clone_metadata = ParentChildCloneMetadata(repository_qualified_name=qualified_name, codebase_qualified_names=[])        try:            async with self.code_confluence_graph.transaction:                # Create repository node                repo_dict = {\"qualified_name\": qualified_name, \"repository_url\": git_repo.repository_url, \"repository_name\": git_repo.repository_name, \"repository_metadata\": git_repo.repository_metadata, \"readme\": git_repo.readme, \"github_organization\": git_repo.github_organization}                repo_results = await CodeConfluenceGitRepository.create_or_update(repo_dict)                if not repo_results:                    raise ApplicationError(message=f\"Failed to create repository node: {qualified_name}\", type=\"REPOSITORY_CREATION_ERROR\", details=[{\"repository\": qualified_name}])                repo_node = repo_results[0]                logger.success(f\"Created repository node: {qualified_name}\")                # Create codebase nodes and establish relationships                for codebase in git_repo.codebases:                    codebase_qualified_name = f\"{qualified_name}_{codebase.name}\"                    codebase_dict = {\"qualified_name\": codebase_qualified_name, \"name\": codebase.name, \"readme\": codebase.readme, \"local_path\": codebase.local_path}                    parent_child_clone_metadata.codebase_qualified_names.append(codebase_qualified_name)                    codebase_results = await CodeConfluenceCodebase.create_or_update(codebase_dict)                    if not codebase_results:                        raise ApplicationError(message=f\"Failed to create codebase node: {codebase.name}\", type=\"CODEBASE_CREATION_ERROR\", details=[{\"repository\": qualified_name, \"codebase\": codebase.name}])                    codebase_node = codebase_results[0]                    # Establish relationships                    await repo_node.codebases.connect(codebase_node)                    await codebase_node.git_repository.connect(repo_node)                logger.success(f\"Successfully ingested repository {qualified_name}\")                return parent_child_clone_metadata        except Exception as e:            error_msg = f\"Failed to insert repository {qualified_name}\"            logger.error(f\"{error_msg}: {str(e)}\")            raise ApplicationError(message=error_msg, type=\"GRAPH_INGESTION_ERROR\") from e    async def insert_code_confluence_codebase_package_manager_metadata(self, codebase_qualified_name: str, package_manager_metadata: UnoplatPackageManagerMetadata) -> None:        \"\"\"        Insert codebase package manager metadata into the graph database        Args:            codebase_qualified_name: Qualified name of the codebase            package_manager_metadata: UnoplatPackageManagerMetadata containing package manager metadata        \"\"\"        try:            async with self.code_confluence_graph.transaction:                # Use get() instead of filter() for unique index                try:                    codebase_node = await CodeConfluenceCodebase.nodes.get(qualified_name=codebase_qualified_name)                except CodeConfluenceCodebase.DoesNotExist:                    raise ApplicationError(message=f\"Codebase not found: {codebase_qualified_name}\", type=\"CODEBASE_NOT_FOUND\")                # Create package manager metadata node                metadata_dict = {                    \"qualified_name\": f\"{codebase_qualified_name}_package_manager_metadata\",                    \"dependencies\": {k: v.model_dump() for k, v in package_manager_metadata.dependencies.items()},                    \"package_manager\": package_manager_metadata.package_manager,                    \"programming_language\": package_manager_metadata.programming_language,                    \"programming_language_version\": package_manager_metadata.programming_language_version,                    \"project_version\": package_manager_metadata.project_version,                    \"description\": package_manager_metadata.description,                    \"license\": package_manager_metadata.license,                    \"package_name\": package_manager_metadata.package_name,                    \"entry_points\": package_manager_metadata.entry_points,                    \"authors\": package_manager_metadata.authors or [],                }                metadata_results = await CodeConfluencePackageManagerMetadata.create_or_update(metadata_dict)                if not metadata_results:                    raise ApplicationError(message=f\"Failed to create package manager metadata for {codebase_qualified_name}\", type=\"METADATA_CREATION_ERROR\")                metadata_node: CodeConfluencePackageManagerMetadata = metadata_results[0]                # Connect metadata to codebase                await codebase_node.package_manager_metadata.connect(metadata_node)                logger.success(f\"Successfully inserted package manager metadata for {codebase_qualified_name}\")        except Exception as e:            error_msg = f\"Failed to insert package manager metadata for {codebase_qualified_name}\"            logger.error(f\"{error_msg}: {str(e)}\")            raise ApplicationError(message=error_msg, type=\"PACKAGE_METADATA_ERROR\")"},{"NodeName":"CodeConfluenceGraph","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/processor/db/graph_db/code_confluence_graph.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"code_confluence_env","TypeType":"EnvironmentSettings"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"info","Position":{"StartLine":32,"StartLinePosition":14,"StopLine":32,"StopLinePosition":59}}],"Position":{"StartLine":17,"StartLinePosition":4,"StopLine":34,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"host","TypeType":"code_confluence_env"},{"TypeValue":"port","TypeType":"code_confluence_env"},{"TypeValue":"username","TypeType":"code_confluence_env"},{"TypeValue":"password","TypeType":"code_confluence_env"},{"TypeValue":"max_connection_lifetime","TypeType":"code_confluence_env"},{"TypeValue":"max_connection_pool_size","TypeType":"code_confluence_env"},{"TypeValue":"connection_acquisition_timeout","TypeType":"code_confluence_env"},{"TypeValue":"self.connection_url","TypeType":"f\"bolt://{username}:{password}@{host}:{port}\"f\"?max_connection_lifetime={max_connection_lifetime}\"f\"&max_connection_pool_size={max_connection_pool_size}\"f\"&connection_acquisition_timeout={connection_acquisition_timeout}\""},{"TypeValue":"config.DATABASE_URL","TypeType":"self"}],"Content":"def __init__(self, code_confluence_env: EnvironmentSettings):        \"\"\"Initialize connection settings\"\"\"        host = code_confluence_env.neo4j_host        port = code_confluence_env.neo4j_port        username = code_confluence_env.neo4j_username        password = code_confluence_env.neo4j_password.get_secret_value()        max_connection_lifetime = code_confluence_env.neo4j_max_connection_lifetime        max_connection_pool_size = code_confluence_env.neo4j_max_connection_pool_size        connection_acquisition_timeout = code_confluence_env.neo4j_connection_acquisition_timeout        if not all([host, port, username, password]):            raise ValueError(\"Required environment variables NEO4J_HOST, NEO4J_PORT, NEO4J_USERNAME, NEO4J_PASSWORD must be set\")        self.connection_url = f\"bolt://{username}:{password}@{host}:{port}\" f\"?max_connection_lifetime={max_connection_lifetime}\" f\"&max_connection_pool_size={max_connection_pool_size}\" f\"&connection_acquisition_timeout={connection_acquisition_timeout}\"        config.DATABASE_URL = self.connection_url        logger.info(\"Neo4j connection settings initialized\")    "},{"Name":"connect","FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":40,"StartLinePosition":18,"StopLine":40,"StopLinePosition":64}}],"Modifiers":["async"],"Position":{"StartLine":34,"StartLinePosition":4,"StopLine":43,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"host","TypeType":"code_confluence_env"},{"TypeValue":"port","TypeType":"code_confluence_env"},{"TypeValue":"username","TypeType":"code_confluence_env"},{"TypeValue":"password","TypeType":"code_confluence_env"},{"TypeValue":"max_connection_lifetime","TypeType":"code_confluence_env"},{"TypeValue":"max_connection_pool_size","TypeType":"code_confluence_env"},{"TypeValue":"connection_acquisition_timeout","TypeType":"code_confluence_env"},{"TypeValue":"self.connection_url","TypeType":"f\"bolt://{username}:{password}@{host}:{port}\"f\"?max_connection_lifetime={max_connection_lifetime}\"f\"&max_connection_pool_size={max_connection_pool_size}\"f\"&connection_acquisition_timeout={connection_acquisition_timeout}\""},{"TypeValue":"config.DATABASE_URL","TypeType":"self"}],"Content":"async def connect(self) -> None:        \"\"\"Establish async connection to Neo4j\"\"\"        try:            await adb.set_connection(self.connection_url)            logger.info(\"Successfully connected to Neo4j database\")        except Exception as e:            logger.error(f\"Failed to connect to Neo4j: {str(e)}\")            raise    "},{"Name":"close","FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":49,"StartLinePosition":18,"StopLine":49,"StopLinePosition":68}}],"Modifiers":["async"],"Position":{"StartLine":43,"StartLinePosition":4,"StopLine":52,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"host","TypeType":"code_confluence_env"},{"TypeValue":"port","TypeType":"code_confluence_env"},{"TypeValue":"username","TypeType":"code_confluence_env"},{"TypeValue":"password","TypeType":"code_confluence_env"},{"TypeValue":"max_connection_lifetime","TypeType":"code_confluence_env"},{"TypeValue":"max_connection_pool_size","TypeType":"code_confluence_env"},{"TypeValue":"connection_acquisition_timeout","TypeType":"code_confluence_env"},{"TypeValue":"self.connection_url","TypeType":"f\"bolt://{username}:{password}@{host}:{port}\"f\"?max_connection_lifetime={max_connection_lifetime}\"f\"&max_connection_pool_size={max_connection_pool_size}\"f\"&connection_acquisition_timeout={connection_acquisition_timeout}\""},{"TypeValue":"config.DATABASE_URL","TypeType":"self"}],"Content":"async def close(self) -> None:        \"\"\"Close the async connection\"\"\"        try:            await adb.close_connection()            logger.info(\"Successfully closed Neo4j connection\")        except Exception as e:            logger.error(f\"Error closing Neo4j connection: {str(e)}\")            raise    "},{"Name":"create_schema","FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":58,"StartLinePosition":18,"StopLine":58,"StopLinePosition":61}}],"Modifiers":["async"],"Position":{"StartLine":52,"StartLinePosition":4,"StopLine":61,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"host","TypeType":"code_confluence_env"},{"TypeValue":"port","TypeType":"code_confluence_env"},{"TypeValue":"username","TypeType":"code_confluence_env"},{"TypeValue":"password","TypeType":"code_confluence_env"},{"TypeValue":"max_connection_lifetime","TypeType":"code_confluence_env"},{"TypeValue":"max_connection_pool_size","TypeType":"code_confluence_env"},{"TypeValue":"connection_acquisition_timeout","TypeType":"code_confluence_env"},{"TypeValue":"self.connection_url","TypeType":"f\"bolt://{username}:{password}@{host}:{port}\"f\"?max_connection_lifetime={max_connection_lifetime}\"f\"&max_connection_pool_size={max_connection_pool_size}\"f\"&connection_acquisition_timeout={connection_acquisition_timeout}\""},{"TypeValue":"config.DATABASE_URL","TypeType":"self"}],"Content":"async def create_schema(self) -> None:        \"\"\"Create schema asynchronously\"\"\"        try:            await adb.install_all_labels()            logger.info(\"Successfully installed all labels\")        except Exception as e:            logger.error(f\"Failed to create schema: {str(e)}\")            raise    "},{"Name":"transaction","Annotations":[{"Name":"property","Position":{"StartLine":61,"StartLinePosition":4,"StopLine":62,"StopLinePosition":4}}],"Position":{"StartLine":62,"StartLinePosition":4,"StopLine":65},"LocalVariables":[{"TypeValue":"host","TypeType":"code_confluence_env"},{"TypeValue":"port","TypeType":"code_confluence_env"},{"TypeValue":"username","TypeType":"code_confluence_env"},{"TypeValue":"password","TypeType":"code_confluence_env"},{"TypeValue":"max_connection_lifetime","TypeType":"code_confluence_env"},{"TypeValue":"max_connection_pool_size","TypeType":"code_confluence_env"},{"TypeValue":"connection_acquisition_timeout","TypeType":"code_confluence_env"},{"TypeValue":"self.connection_url","TypeType":"f\"bolt://{username}:{password}@{host}:{port}\"f\"?max_connection_lifetime={max_connection_lifetime}\"f\"&max_connection_pool_size={max_connection_pool_size}\"f\"&connection_acquisition_timeout={connection_acquisition_timeout}\""},{"TypeValue":"config.DATABASE_URL","TypeType":"self"}],"Content":"def transaction(self):        \"\"\"Get transaction context manager\"\"\"        return adb.transaction"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["EnvironmentSettings"]},{"Source":"loguru","UsageName":["logger"]},{"Source":"neomodel","UsageName":["adb","config"]}],"Position":{"StartLine":12,"StopLine":65},"Content":"class CodeConfluenceGraph:    \"\"\"    Async Neo4j graph database connection manager using neomodel    \"\"\"    def __init__(self, code_confluence_env: EnvironmentSettings):        \"\"\"Initialize connection settings\"\"\"        host = code_confluence_env.neo4j_host        port = code_confluence_env.neo4j_port        username = code_confluence_env.neo4j_username        password = code_confluence_env.neo4j_password.get_secret_value()        max_connection_lifetime = code_confluence_env.neo4j_max_connection_lifetime        max_connection_pool_size = code_confluence_env.neo4j_max_connection_pool_size        connection_acquisition_timeout = code_confluence_env.neo4j_connection_acquisition_timeout        if not all([host, port, username, password]):            raise ValueError(\"Required environment variables NEO4J_HOST, NEO4J_PORT, NEO4J_USERNAME, NEO4J_PASSWORD must be set\")        self.connection_url = f\"bolt://{username}:{password}@{host}:{port}\" f\"?max_connection_lifetime={max_connection_lifetime}\" f\"&max_connection_pool_size={max_connection_pool_size}\" f\"&connection_acquisition_timeout={connection_acquisition_timeout}\"        config.DATABASE_URL = self.connection_url        logger.info(\"Neo4j connection settings initialized\")    async def connect(self) -> None:        \"\"\"Establish async connection to Neo4j\"\"\"        try:            await adb.set_connection(self.connection_url)            logger.info(\"Successfully connected to Neo4j database\")        except Exception as e:            logger.error(f\"Failed to connect to Neo4j: {str(e)}\")            raise    async def close(self) -> None:        \"\"\"Close the async connection\"\"\"        try:            await adb.close_connection()            logger.info(\"Successfully closed Neo4j connection\")        except Exception as e:            logger.error(f\"Error closing Neo4j connection: {str(e)}\")            raise    async def create_schema(self) -> None:        \"\"\"Create schema asynchronously\"\"\"        try:            await adb.install_all_labels()            logger.info(\"Successfully installed all labels\")        except Exception as e:            logger.error(f\"Failed to create schema: {str(e)}\")            raise    @property    def transaction(self):        \"\"\"Get transaction context manager\"\"\"        return adb.transaction"},{"NodeName":"ArchGuardHandler","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/processor/archguard/arc_guard_handler.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"jar_path","TypeType":"str"},{"TypeValue":"language","TypeType":"str"},{"TypeValue":"codebase_path","TypeType":"str"},{"TypeValue":"codebase_name","TypeType":"str"},{"TypeValue":"output_path","TypeType":"str"},{"TypeValue":"extension","TypeType":"str"}],"Position":{"StartLine":10,"StartLinePosition":4,"StopLine":27,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.jar_path","TypeType":"jar_path"},{"TypeValue":"self.language","TypeType":"language"},{"TypeValue":"self.codebase_path","TypeType":"codebase_path"},{"TypeValue":"self.output_path","TypeType":"output_path"},{"TypeValue":"self.codebase_name","TypeType":"codebase_name"},{"TypeValue":"self.extension","TypeType":"extension"}],"Content":"def __init__(        self,         jar_path: str,         language: str,         codebase_path: str,        codebase_name: str,         output_path: str,        extension: str    ) -> None:        self.jar_path = jar_path        self.language = language        self.codebase_path = codebase_path        self.output_path = output_path        self.codebase_name = codebase_name        self.extension = extension            "},{"Name":"run_scan","FunctionCalls":[{"NodeName":"activity","FunctionName":"logger","Position":{"StartLine":91,"StartLinePosition":16,"StopLine":91,"StopLinePosition":17}},{"NodeName":"activity","FunctionName":"info","Position":{"StartLine":91,"StartLinePosition":23,"StopLine":94,"StopLinePosition":8}}],"Position":{"StartLine":27,"StartLinePosition":4,"StopLine":97,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.jar_path","TypeType":"jar_path"},{"TypeValue":"self.language","TypeType":"language"},{"TypeValue":"self.codebase_path","TypeType":"codebase_path"},{"TypeValue":"self.output_path","TypeType":"output_path"},{"TypeValue":"self.codebase_name","TypeType":"codebase_name"},{"TypeValue":"self.extension","TypeType":"extension"},{"TypeValue":"command","TypeType":"[\"java\",\"-jar\",self.jar_path,\"--with-function-code\",f\"--language={self.language}\",\"--output=arrow\",\"--output=json\",f\"--path={self.codebase_path}\",f\"--output-dir={self.output_path}\"]"},{"TypeValue":"process","TypeType":"subprocess"},{"TypeValue":"output","TypeType":"process"},{"TypeValue":"","TypeType":"process"},{"TypeValue":"chapi_metadata_path","TypeType":"self"}],"Content":"def run_scan(self) -> str:        \"\"\"        Run ArchGuard scan on the codebase.                Returns:            str: Path to the generated JSON file        \"\"\"        activity.logger.info(            \"Starting ArchGuard scan\",            extra={                \"language\": self.language,                \"codebase\": self.codebase_name            }        )        command = [            \"java\", \"-jar\", self.jar_path,            \"--with-function-code\",            f\"--language={self.language}\",            \"--output=arrow\", \"--output=json\",             f\"--path={self.codebase_path}\",            f\"--output-dir={self.output_path}\"        ]        activity.logger.debug(            \"Executing command\",            extra={\"command\": ' '.join(command)}        )                process = subprocess.Popen(            command,             stdout=subprocess.PIPE,             stderr=subprocess.PIPE,             text=True,             bufsize=1        )                if process.stdout is None:            activity.logger.error(\"Failed to open subprocess stdout\")            raise RuntimeError(\"Failed to open subprocess stdout\")                    while True:            output = process.stdout.readline()            if output == '' and process.poll() is not None:                break            if output:                activity.logger.debug(output.strip())                        _stdout, stderr = process.communicate()        if process.returncode == 0:            activity.logger.info(\"ArchGuard scan completed successfully\")            chapi_metadata_path = self.modify_output_filename(                \"0_codes.json\",                 f\"{self.codebase_name}_codes.json\"            )        else:            activity.logger.error(                \"Error in ArchGuard scanning\",                extra={                    \"error\": stderr,                    \"return_code\": process.returncode                }            )            raise RuntimeError(f\"ArchGuard scan failed: {stderr}\")        activity.logger.info(            \"Scan statistics\",            extra={\"output_path\": chapi_metadata_path}        )        return chapi_metadata_path            "},{"Name":"modify_output_filename","Parameters":[{"TypeValue":"old_filename","TypeType":"str"},{"TypeValue":"new_filename","TypeType":"str"}],"FunctionCalls":[{"NodeName":"activity","FunctionName":"logger","Position":{"StartLine":126,"StartLinePosition":20,"StopLine":126,"StopLinePosition":21}},{"NodeName":"activity","FunctionName":"error","Position":{"StartLine":126,"StartLinePosition":27,"StopLine":133,"StopLinePosition":12}}],"Position":{"StartLine":97,"StartLinePosition":4,"StopLine":136},"LocalVariables":[{"TypeValue":"self.jar_path","TypeType":"jar_path"},{"TypeValue":"self.language","TypeType":"language"},{"TypeValue":"self.codebase_path","TypeType":"codebase_path"},{"TypeValue":"self.output_path","TypeType":"output_path"},{"TypeValue":"self.codebase_name","TypeType":"codebase_name"},{"TypeValue":"self.extension","TypeType":"extension"},{"TypeValue":"command","TypeType":"[\"java\",\"-jar\",self.jar_path,\"--with-function-code\",f\"--language={self.language}\",\"--output=arrow\",\"--output=json\",f\"--path={self.codebase_path}\",f\"--output-dir={self.output_path}\"]"},{"TypeValue":"process","TypeType":"subprocess"},{"TypeValue":"output","TypeType":"process"},{"TypeValue":"","TypeType":"process"},{"TypeValue":"chapi_metadata_path","TypeType":"self"},{"TypeValue":"current_directory","TypeType":"os"},{"TypeValue":"old_file_path","TypeType":"os"},{"TypeValue":"new_file_path","TypeType":"os"}],"Content":"def modify_output_filename(self, old_filename: str, new_filename: str) -> str:        \"\"\"        Rename the output file from default name to codebase-specific name.                Args:            old_filename: Original filename            new_filename: New filename to use                    Returns:            str: Path to renamed file                    Raises:            OSError: If file rename fails        \"\"\"        current_directory = os.getcwd()        old_file_path = os.path.join(current_directory, old_filename)        new_file_path = os.path.join(current_directory, new_filename)                try:            os.rename(old_file_path, new_file_path)            activity.logger.debug(                \"Renamed output file\",                extra={                    \"old_path\": old_file_path,                    \"new_path\": new_file_path                }            )            return new_file_path        except OSError as e:            activity.logger.error(                \"Failed to rename output file\",                extra={                    \"error\": str(e),                    \"old_path\": old_file_path,                    \"new_path\": new_file_path                }            )            raise"}],"Imports":[{"Source":"os"},{"Source":"subprocess"},{"Source":"temporalio","UsageName":["activity"]}],"Position":{"StartLine":9,"StopLine":136},"Content":"class ArchGuardHandler:    def __init__(        self,         jar_path: str,         language: str,         codebase_path: str,        codebase_name: str,         output_path: str,        extension: str    ) -> None:        self.jar_path = jar_path        self.language = language        self.codebase_path = codebase_path        self.output_path = output_path        self.codebase_name = codebase_name        self.extension = extension            def run_scan(self) -> str:        \"\"\"        Run ArchGuard scan on the codebase.                Returns:            str: Path to the generated JSON file        \"\"\"        activity.logger.info(            \"Starting ArchGuard scan\",            extra={                \"language\": self.language,                \"codebase\": self.codebase_name            }        )        command = [            \"java\", \"-jar\", self.jar_path,            \"--with-function-code\",            f\"--language={self.language}\",            \"--output=arrow\", \"--output=json\",             f\"--path={self.codebase_path}\",            f\"--output-dir={self.output_path}\"        ]        activity.logger.debug(            \"Executing command\",            extra={\"command\": ' '.join(command)}        )                process = subprocess.Popen(            command,             stdout=subprocess.PIPE,             stderr=subprocess.PIPE,             text=True,             bufsize=1        )                if process.stdout is None:            activity.logger.error(\"Failed to open subprocess stdout\")            raise RuntimeError(\"Failed to open subprocess stdout\")                    while True:            output = process.stdout.readline()            if output == '' and process.poll() is not None:                break            if output:                activity.logger.debug(output.strip())                        _stdout, stderr = process.communicate()        if process.returncode == 0:            activity.logger.info(\"ArchGuard scan completed successfully\")            chapi_metadata_path = self.modify_output_filename(                \"0_codes.json\",                 f\"{self.codebase_name}_codes.json\"            )        else:            activity.logger.error(                \"Error in ArchGuard scanning\",                extra={                    \"error\": stderr,                    \"return_code\": process.returncode                }            )            raise RuntimeError(f\"ArchGuard scan failed: {stderr}\")        activity.logger.info(            \"Scan statistics\",            extra={\"output_path\": chapi_metadata_path}        )        return chapi_metadata_path            def modify_output_filename(self, old_filename: str, new_filename: str) -> str:        \"\"\"        Rename the output file from default name to codebase-specific name.                Args:            old_filename: Original filename            new_filename: New filename to use                    Returns:            str: Path to renamed file                    Raises:            OSError: If file rename fails        \"\"\"        current_directory = os.getcwd()        old_file_path = os.path.join(current_directory, old_filename)        new_file_path = os.path.join(current_directory, new_filename)                try:            os.rename(old_file_path, new_file_path)            activity.logger.debug(                \"Renamed output file\",                extra={                    \"old_path\": old_file_path,                    \"new_path\": new_file_path                }            )            return new_file_path        except OSError as e:            activity.logger.error(                \"Failed to rename output file\",                extra={                    \"error\": str(e),                    \"old_path\": old_file_path,                    \"new_path\": new_file_path                }            )            raise"},{"NodeName":"RepoWorkflow","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/processor/repo_workflow.py","Functions":[{"Name":"run","Parameters":[{"TypeValue":"repository_settings","TypeType":"RepositorySettings"},{"TypeValue":"github_token","TypeType":"str"}],"FunctionCalls":[{"NodeName":"workflow","FunctionName":"logger","Position":{"StartLine":66,"StartLinePosition":16,"StopLine":66,"StopLinePosition":17}},{"NodeName":"workflow","FunctionName":"info","Position":{"StartLine":66,"StartLinePosition":23,"StopLine":66,"StopLinePosition":108}}],"Annotations":[{"Name":"workflow.run","Position":{"StartLine":36,"StartLinePosition":4,"StopLine":37,"StopLinePosition":4}}],"Modifiers":["async"],"Position":{"StartLine":37,"StartLinePosition":4,"StopLine":68},"LocalVariables":[{"TypeValue":"git_repo_metadata","TypeType":""},{"TypeValue":"parent_child_clone_metadata","TypeType":""}],"Content":"async def run(self, repository_settings: RepositorySettings, github_token: str) -> UnoplatGitRepository:        \"\"\"        Execute the repository activity workflow        Args:            repository_settings: Repository configuration            app_settings: Application settings including GitHub token        Returns:            RepoActivityResult containing the processing outcome        \"\"\"        workflow.logger.info(f\"Starting repository workflow for {repository_settings.git_url}\")        # 1. First executes a git activity        workflow.logger.info(\"Executing git activity to process repository\")        git_repo_metadata: UnoplatGitRepository = await workflow.execute_activity(activity=GitActivity.process_git_activity, args=(repository_settings, github_token), start_to_close_timeout=timedelta(minutes=10))        # 2. Then insert the git repo into the graph db        workflow.logger.info(\"Inserting git repository metadata into graph database\")        parent_child_clone_metadata: ParentChildCloneMetadata = await workflow.execute_activity(activity=ConfluenceGitGraph.insert_git_repo_into_graph_db, args=(git_repo_metadata,), start_to_close_timeout=timedelta(minutes=10))        # 3. Then spawns child workflows for each codebase        workflow.logger.info(f\"Spawning {len(git_repo_metadata.codebases)} child workflows for codebases\")        for codebase_qualified_name, unoplat_codebase in zip(parent_child_clone_metadata.codebase_qualified_names, git_repo_metadata.codebases):            workflow.logger.info(f\"Starting child workflow for codebase: {codebase_qualified_name}\")            await workflow.start_child_workflow(                CodebaseChildWorkflow.run, args=[parent_child_clone_metadata.repository_qualified_name, codebase_qualified_name, unoplat_codebase.local_path,unoplat_codebase.source_directory, unoplat_codebase.package_manager_metadata], id=f\"codebase-child-workflow-{codebase_qualified_name}\", parent_close_policy=ParentClosePolicy.ABANDON            )        workflow.logger.info(f\"Repository workflow completed successfully for {repository_settings.git_url}\")        return git_repo_metadata"}],"Annotations":[{"Name":"workflow.defn","KeyValues":[{"Key":"name","Value":"\"repo-activity-workflow\""}],"Position":{"StartLine":30,"StopLine":31}}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.workflow.parent_child_clone_metadata","UsageName":["ParentChildCloneMetadata"]},{"Source":"datetime","UsageName":["timedelta"]},{"Source":"temporalio","UsageName":["workflow"]},{"Source":"temporalio.workflow","UsageName":["ParentClosePolicy"]},{"Source":"src.code_confluence_flow_bridge.processor.codebase_child_workflow","UsageName":["CodebaseChildWorkflow"]},{"Source":"src.code_confluence_flow_bridge.processor.git_activity.confluence_git_activity","UsageName":["GitActivity"]},{"Source":"src.code_confluence_flow_bridge.processor.git_activity.confluence_git_graph","UsageName":["ConfluenceGitGraph"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_git_repository","UsageName":["UnoplatGitRepository"]},{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["RepositorySettings"]}],"Position":{"StartLine":31,"StopLine":68},"Content":"class RepoWorkflow:    \"\"\"    Workflow that orchestrates repository processing activities    \"\"\"    @workflow.run    async def run(self, repository_settings: RepositorySettings, github_token: str) -> UnoplatGitRepository:        \"\"\"        Execute the repository activity workflow        Args:            repository_settings: Repository configuration            app_settings: Application settings including GitHub token        Returns:            RepoActivityResult containing the processing outcome        \"\"\"        workflow.logger.info(f\"Starting repository workflow for {repository_settings.git_url}\")        # 1. First executes a git activity        workflow.logger.info(\"Executing git activity to process repository\")        git_repo_metadata: UnoplatGitRepository = await workflow.execute_activity(activity=GitActivity.process_git_activity, args=(repository_settings, github_token), start_to_close_timeout=timedelta(minutes=10))        # 2. Then insert the git repo into the graph db        workflow.logger.info(\"Inserting git repository metadata into graph database\")        parent_child_clone_metadata: ParentChildCloneMetadata = await workflow.execute_activity(activity=ConfluenceGitGraph.insert_git_repo_into_graph_db, args=(git_repo_metadata,), start_to_close_timeout=timedelta(minutes=10))        # 3. Then spawns child workflows for each codebase        workflow.logger.info(f\"Spawning {len(git_repo_metadata.codebases)} child workflows for codebases\")        for codebase_qualified_name, unoplat_codebase in zip(parent_child_clone_metadata.codebase_qualified_names, git_repo_metadata.codebases):            workflow.logger.info(f\"Starting child workflow for codebase: {codebase_qualified_name}\")            await workflow.start_child_workflow(                CodebaseChildWorkflow.run, args=[parent_child_clone_metadata.repository_qualified_name, codebase_qualified_name, unoplat_codebase.local_path,unoplat_codebase.source_directory, unoplat_codebase.package_manager_metadata], id=f\"codebase-child-workflow-{codebase_qualified_name}\", parent_close_policy=ParentClosePolicy.ABANDON            )        workflow.logger.info(f\"Repository workflow completed successfully for {repository_settings.git_url}\")        return git_repo_metadata"},{"NodeName":"ProgrammingLanguage","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/configuration/settings.py","MultipleExtend":["str","Enum"],"Imports":[{"Source":"enum","UsageName":["Enum"]},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","SecretStr"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":10,"StopLine":14},"Content":"class ProgrammingLanguage(str, Enum):    PYTHON = \"python\""},{"NodeName":"PackageManagerType","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/configuration/settings.py","MultipleExtend":["str","Enum"],"Imports":[{"Source":"enum","UsageName":["Enum"]},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","SecretStr"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":14,"StopLine":20},"Content":"class PackageManagerType(str, Enum):    POETRY = \"poetry\"    PIP = \"pip\"    UV = \"uv\""},{"NodeName":"DatabaseType","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/configuration/settings.py","MultipleExtend":["str","Enum"],"Imports":[{"Source":"enum","UsageName":["Enum"]},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","SecretStr"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":20,"StopLine":25},"Content":"class DatabaseType(str, Enum):    NEO4J = \"neo4j\"# Configuration Models (BaseModel for JSON config)"},{"NodeName":"ProgrammingLanguageMetadata","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/configuration/settings.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"enum","UsageName":["Enum"]},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","SecretStr"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":25,"StopLine":31},"Content":"class ProgrammingLanguageMetadata(BaseModel):    language: ProgrammingLanguage    package_manager: PackageManagerType    language_version: Optional[str] = None"},{"NodeName":"CodebaseConfig","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/configuration/settings.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"enum","UsageName":["Enum"]},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","SecretStr"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":31,"StopLine":37},"Content":"class CodebaseConfig(BaseModel):    codebase_folder: str    root_package: Optional[str] = None    programming_language_metadata: ProgrammingLanguageMetadata"},{"NodeName":"RepositorySettings","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/configuration/settings.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"enum","UsageName":["Enum"]},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","SecretStr"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":37,"StopLine":43},"Content":"class RepositorySettings(BaseModel):    git_url: str    output_path: str    codebases: List[CodebaseConfig]"},{"NodeName":"LLMProviderConfig","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/configuration/settings.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"enum","UsageName":["Enum"]},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","SecretStr"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":43,"StopLine":49},"Content":"class LLMProviderConfig(BaseModel):    llm_model_provider: str    llm_model_provider_args: Dict[str, Any]# Environment Settings (BaseSettings for environment variables)"},{"NodeName":"EnvironmentSettings","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/configuration/settings.py","MultipleExtend":["BaseSettings"],"Imports":[{"Source":"enum","UsageName":["Enum"]},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field","SecretStr"]},{"Source":"pydantic_settings","UsageName":["BaseSettings","SettingsConfigDict"]}],"Position":{"StartLine":49,"StopLine":74},"Content":"class EnvironmentSettings(BaseSettings):    \"\"\"Environment variables and secrets\"\"\"    model_config = SettingsConfigDict(        env_prefix=\"UNOPLAT_\",        env_file=(\".env.dev\", \".env.test\", \".env.prod\"),        env_file_encoding=\"utf-8\",        case_sensitive=True,        extra=\"ignore\",  # Ignore extra fields        populate_by_name=True,        env_parse_none_str=None,  # Changed from list to None to fix type error    )    neo4j_host: str = Field(default=..., alias=\"NEO4J_HOST\")    neo4j_port: int = Field(default=..., alias=\"NEO4J_PORT\")    neo4j_username: str = Field(default=..., alias=\"NEO4J_USERNAME\")    neo4j_password: SecretStr = Field(default=..., alias=\"NEO4J_PASSWORD\")    neo4j_max_connection_lifetime: int = Field(default=3600, alias=\"NEO4J_MAX_CONNECTION_LIFETIME\", description=\"The maximum lifetime of a connection to the Neo4j database in seconds\")    neo4j_max_connection_pool_size: int = Field(default=50, alias=\"NEO4J_MAX_CONNECTION_POOL_SIZE\", description=\"The maximum number of connections in the Neo4j connection pool\")    neo4j_connection_acquisition_timeout: int = Field(default=60, alias=\"NEO4J_CONNECTION_ACQUISITION_TIMEOUT\", description=\"The maximum time to wait for a connection to be acquired from the Neo4j connection pool in seconds\")"},{"NodeName":"UnoplatPackageManagerMetadata","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi_forge/unoplat_package_manager_metadata.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_project_dependency","UsageName":["UnoplatProjectDependency"]},{"Source":"typing","UsageName":["Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":13,"StopLine":31},"Content":"class UnoplatPackageManagerMetadata(BaseModel):    dependencies: Dict[str, UnoplatProjectDependency] = Field(default_factory=dict, description=\"The dependencies of the project\")    package_name: Optional[str] = Field(default=None, description=\"The name of the package\")    programming_language: str = Field(description=\"The programming language of the project\")    package_manager: str = Field(description=\"The package manager of the project\")    programming_language_version: Optional[str] = Field(default=None, description=\"The version of the programming language\")    project_version: Optional[str] = Field(default=None, description=\"The version of the project\")    description: Optional[str] = Field(default=None, description=\"The description of the project\")    authors: Optional[List[str]] = Field(default=None, description=\"The authors of the project\")    license: Optional[str] = Field(default=None, description=\"The license of the project\")    entry_points: Dict[str, str] = Field(default_factory=dict, description=\"Dictionary of script names to their entry points. Example: {'cli': 'package.module:main', 'serve': 'uvicorn app:main'}\")    # New fields for additional metadata    homepage: Optional[str] = Field(default=None, description=\"The homepage URL of the project\")    repository: Optional[str] = Field(default=None, description=\"The repository URL of the project\")    documentation: Optional[str] = Field(default=None, description=\"The documentation URL of the project\")    keywords: List[str] = Field(default_factory=list, description=\"List of keywords/tags for the project\")    maintainers: List[str] = Field(default_factory=list, description=\"List of project maintainers\")    readme: Optional[str] = Field(default=None, description=\"Path to or content of the project's README file\")"},{"NodeName":"UnoplatChapiForgeNode","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi_forge/unoplat_chapi_forge_node.py","MultipleExtend":["ChapiNode"],"Functions":[{"Name":"from_chapi_node","Parameters":[{"TypeValue":"cls","TypeType":""},{"TypeValue":"chapi_node","TypeType":"ChapiNode"}],"Annotations":[{"Name":"classmethod","Position":{"StartLine":31,"StartLinePosition":4,"StopLine":32,"StopLinePosition":4}}],"Position":{"StartLine":32,"StartLinePosition":4,"StopLine":51},"LocalVariables":[{"TypeValue":"qualified_name","TypeType":""},{"TypeValue":"comments_description","TypeType":""},{"TypeValue":"segregated_imports","TypeType":""},{"TypeValue":"dependent_internal_classes","TypeType":""},{"TypeValue":"functions","TypeType":""},{"TypeValue":"global_variables","TypeType":""},{"TypeValue":"chapi_node_data","TypeType":""},{"TypeValue":"func_qualified_name","TypeType":"f\"{additional_fields['qualified_name']}.{function['Name']}\""},{"TypeValue":"function[\"QualifiedName\"]","TypeType":"func_qualified_name"}],"Content":"def from_chapi_node(cls, chapi_node: ChapiNode, **additional_fields):        chapi_node_data: dict[str, Any] = chapi_node.model_dump(by_alias=True)        # Check for qualified_name (assuming this is required)        if \"qualified_name\" in additional_fields and additional_fields[\"qualified_name\"]:            chapi_node_data.update({\"QualifiedName\": additional_fields[\"qualified_name\"]})        # More thorough checking for segregated_imports        if \"segregated_imports\" in additional_fields and additional_fields[\"segregated_imports\"] is not None:            chapi_node_data.update({\"SegregatedImports\": additional_fields[\"segregated_imports\"]})        # Check functions if they exist        if chapi_node_data.get(\"Functions\"):  # Using .get() is safer than direct access            for function in chapi_node_data[\"Functions\"]:                func_qualified_name = f\"{additional_fields['qualified_name']}.{function['Name']}\"                function[\"QualifiedName\"] = func_qualified_name        # Create new child instance with combined data        return cls.model_validate(chapi_node_data)"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_class_global_fieldmodel","UsageName":["ClassGlobalFieldModel"]},{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_node","UsageName":["ChapiNode"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_chapi_forge_function","UsageName":["UnoplatChapiForgeFunction"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_import","UsageName":["UnoplatImport"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_import_type","UsageName":["ImportType"]},{"Source":"typing","UsageName":["Any","Dict","List","Optional"]},{"Source":"pydantic","UsageName":["Field"]}],"Position":{"StartLine":23,"StopLine":51},"Content":"class UnoplatChapiForgeNode(ChapiNode):    qualified_name: str = Field(default=None, alias=\"QualifiedName\", exclude=True, description=\"name of class with absolute path\")    comments_description: Optional[str] = Field(default=None, alias=\"CommentsDescription\", description=\"description of the node from comments\")    segregated_imports: Optional[Dict[ImportType, List[UnoplatImport]]] = Field(default=None, alias=\"SegregatedImports\", description=\"SegregatedImports in terms of internal ,external ,standard and local libraries\")    dependent_internal_classes: Optional[List[str]] = Field(default_factory=list, alias=\"DependentInternalClasses\", description=\"list of unique internalclasses that this node is dependent on\")    functions: Optional[List[UnoplatChapiForgeFunction]] = Field(default_factory=list, alias=\"Functions\", description=\"functions of the node\")  # type: ignore    global_variables: Optional[List[ClassGlobalFieldModel]] = Field(default_factory=list, alias=\"GlobalVariables\", description=\"global variables of the node\")  # type: ignore    @classmethod    def from_chapi_node(cls, chapi_node: ChapiNode, **additional_fields):        chapi_node_data: dict[str, Any] = chapi_node.model_dump(by_alias=True)        # Check for qualified_name (assuming this is required)        if \"qualified_name\" in additional_fields and additional_fields[\"qualified_name\"]:            chapi_node_data.update({\"QualifiedName\": additional_fields[\"qualified_name\"]})        # More thorough checking for segregated_imports        if \"segregated_imports\" in additional_fields and additional_fields[\"segregated_imports\"] is not None:            chapi_node_data.update({\"SegregatedImports\": additional_fields[\"segregated_imports\"]})        # Check functions if they exist        if chapi_node_data.get(\"Functions\"):  # Using .get() is safer than direct access            for function in chapi_node_data[\"Functions\"]:                func_qualified_name = f\"{additional_fields['qualified_name']}.{function['Name']}\"                function[\"QualifiedName\"] = func_qualified_name        # Create new child instance with combined data        return cls.model_validate(chapi_node_data)"},{"NodeName":"UnoplatGitRepository","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi_forge/unoplat_git_repository.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_codebase","UsageName":["UnoplatCodebase"]},{"Source":"typing","UsageName":["Any","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":14,"StopLine":22},"Content":"class UnoplatGitRepository(BaseModel):    repository_url: str = Field(description=\"The URL of the repository\")    repository_name: str = Field(description=\"The name of the repository\")    repository_metadata: dict[str, Any] = Field(description=\"The metadata of the repository\")    codebases: List[UnoplatCodebase] = Field(default_factory=list, description=\"The codebases of the repository\")    readme: Optional[str] = Field(default=None, description=\"The readme of the repository\")    domain: Optional[str] = Field(default=None, description=\"The domain of the repository\")    github_organization: Optional[str] = Field(default=None, description=\"The github organization of the repository\")"},{"NodeName":"UnoplatChapiForgeFunction","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi_forge/unoplat_chapi_forge_function.py","MultipleExtend":["ChapiFunction"],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_function","UsageName":["ChapiFunction"]},{"Source":"typing","UsageName":["Optional"]},{"Source":"pydantic","UsageName":["Field"]}],"Position":{"StartLine":11,"StopLine":14},"Content":"class UnoplatChapiForgeFunction(ChapiFunction):    qualified_name: str = Field(alias=\"QualifiedName\", description=\"The qualified name of the function\")    comments_description: Optional[str] = Field(default=None, alias=\"CommentsDescription\", description=\"description of the function from comments\")"},{"NodeName":"ImportType","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi_forge/unoplat_import_type.py","MultipleExtend":["Enum"],"Imports":[{"Source":"enum","UsageName":["Enum"]}],"Position":{"StartLine":6,"StopLine":11},"Content":"class ImportType(Enum):    INTERNAL = \"INTERNAL\"  # First party imports    EXTERNAL = \"EXTERNAL\"  # Third party imports    STANDARD = \"STANDARD\"  # Standard library imports    LOCAL = \"LOCAL\""},{"NodeName":"UnoplatProjectDependency","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi_forge/unoplat_project_dependency.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_version","UsageName":["UnoplatVersion"]},{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":13,"StopLine":22},"Content":"class UnoplatProjectDependency(BaseModel):    version: UnoplatVersion = Field(default_factory=UnoplatVersion, description=\"Version constraint\")    group: Optional[str] = Field(default=None, description=\"Group of the dependency (e.g. dev, test)\")    extras: Optional[List[str]] = Field(default=None, description=\"List of extras for this dependency\")    source: Optional[str] = Field(default=None, description=\"Source of the dependency (e.g. 'git', 'url', 'path')\")    source_url: Optional[str] = Field(default=None, description=\"URL or path to the dependency source\")    source_reference: Optional[str] = Field(default=None, description=\"Branch, tag, or commit reference\")    subdirectory: Optional[str] = Field(default=None, description=\"Subdirectory within source\")    environment_marker: Optional[str] = Field(default=None, description=\"PEP 508 environment marker string (e.g., python_version < '3.7')\")"},{"NodeName":"UnoplatCodebase","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi_forge/unoplat_codebase.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package","UsageName":["UnoplatPackage"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"typing","UsageName":["Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":16,"StopLine":23},"Content":"class UnoplatCodebase(BaseModel):    name: str = Field(description=\"Name of the codebase usually the root package name\")    readme: Optional[str] = Field(default=None)    packages: Optional[UnoplatPackage] = Field(default=None)    package_manager_metadata: UnoplatPackageManagerMetadata = Field(description=\"The package manager metadata of the codebase\")    local_path: str = Field(description=\"Codebase source code path\")    source_directory: str = Field(description=\"Codebase root path\")"},{"NodeName":"UnoplatVersion","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi_forge/unoplat_version.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":8,"StopLine":13},"Content":"class UnoplatVersion(BaseModel):    specifier: Optional[str] = Field(default=None, description=\"The specifier of the version\")    minimum_version: Optional[str] = Field(default=None, description=\"The minimum version of the project\")    maximum_version: Optional[str] = Field(default=None, description=\"The maximum version of the project\")    current_version: Optional[str] = Field(default=None, description=\"The current version of the project\")"},{"NodeName":"ImportedName","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi_forge/unoplat_import.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_import_type","UsageName":["ImportType"]},{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":13,"StopLine":18},"Content":"class ImportedName(BaseModel):    original_name: Optional[str] = None    alias: Optional[str] = None"},{"NodeName":"UnoplatImport","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi_forge/unoplat_import.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_import_type","UsageName":["ImportType"]},{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":18,"StopLine":22},"Content":"class UnoplatImport(BaseModel):    source: Optional[str] = Field(default=None, alias=\"Source\")    usage_names: Optional[List[ImportedName]] = Field(default_factory=lambda: [], alias=\"UsageName\")    import_type: Optional[ImportType] = Field(default=None, alias=\"ImportType\")"},{"NodeName":"UnoplatPackage","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi_forge/unoplat_package.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_chapi_forge_node","UsageName":["UnoplatChapiForgeNode"]},{"Source":"typing","UsageName":["Dict","List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":14,"StopLine":20},"Content":"class UnoplatPackage(BaseModel):    name: Optional[str] = Field(default=None, description=\"Name of the package\")    nodes: Dict[str, List[UnoplatChapiForgeNode]] = Field(default_factory=dict, description=\"Dict of file paths to their nodes (classes/procedural code)\")    sub_packages: Optional[Dict[str, \"UnoplatPackage\"]] = Field(default_factory=dict, description=\"Dict of the sub-packages for the package\")"},{"NodeName":"ParentChildCloneMetadata","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/workflow/parent_child_clone_metadata.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":4,"StopLine":7},"Content":"class ParentChildCloneMetadata(BaseModel):    repository_qualified_name: str    codebase_qualified_names: list[str] = Field(default_factory=list)"},{"NodeName":"LinterType","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/workflow/code_confluence_linter.py","MultipleExtend":["str","Enum"],"Imports":[{"Source":"enum","UsageName":["Enum"]}],"Position":{"StartLine":4,"StopLine":6},"Content":"class LinterType(str, Enum):    RUFF = \"ruff\""},{"NodeName":"ChapiFunctionFieldModel","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi/chapi_function_field_model.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":8,"StopLine":12},"Content":"class ChapiFunctionFieldModel(BaseModel):    function_variable_name: str = Field(default=None, alias=\"TypeValue\", description=\"function field name\")    function_variable_type: Optional[str] = Field(default=None, alias=\"TypeType\", description=\"function field type hint\")    function_variable_value: Optional[str] = Field(default=None, alias=\"DefaultValue\", description=\"function field value\")"},{"NodeName":"ChapiNode","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi/chapi_node.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_annotation","UsageName":["ChapiAnnotation"]},{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_class_global_fieldmodel","UsageName":["ClassGlobalFieldModel"]},{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_function","UsageName":["ChapiFunction"]},{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_import","UsageName":["ChapiImport"]},{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_position","UsageName":["Position"]},{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":19,"StopLine":54},"Content":"class ChapiNode(BaseModel):    \"\"\"Represents a node in the code structure (class, function, etc.).\"\"\"        node_name: Optional[str] = Field(        default=None,         alias=\"NodeName\",         description=\"name of the class, method, function, etc.\"    )    type: Optional[str] = Field(default=None, alias=\"Type\")    file_path: Optional[str] = Field(default=None, alias=\"FilePath\")    module: Optional[str] = Field(default=None, alias=\"Module\")    package: Optional[str] = Field(default=None, alias=\"Package\")    multiple_extend: Optional[List[str]] = Field(        default_factory=lambda: [],         alias=\"MultipleExtend\"    )    fields: Optional[List[ClassGlobalFieldModel]] = Field(        default_factory=lambda: [],         alias=\"Fields\"    )    extend: Optional[str] = Field(default=None, alias=\"Extend\")    imports: Optional[List[ChapiImport]] = Field(        default_factory=lambda: [],         alias=\"Imports\"    )    functions: Optional[List[ChapiFunction]] = Field(        default_factory=lambda: [],         alias=\"Functions\"    )    position: Optional[Position] = Field(default=None, alias=\"Position\")    content: Optional[str] = Field(default=None, alias=\"Content\")    annotations: Optional[List[ChapiAnnotation]] = Field(        default_factory=lambda: [],         alias=\"Annotations\"    )"},{"NodeName":"ChapiAnnotationKeyVal","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi/chapi_annotation_key_val.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":8,"StopLine":11},"Content":"class ChapiAnnotationKeyVal(BaseModel):    key: Optional[str] = Field(default=None, alias=\"Key\", description=\"Key of the annotation\")    value: Optional[str] = Field(default=None, alias=\"Value\", description=\"Value of the annotation\")"},{"NodeName":"ChapiFunctionCall","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi/chapi_functioncall.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_parameter","UsageName":["ChapiParameter"]},{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_position","UsageName":["Position"]},{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":13,"StopLine":24},"Content":"class ChapiFunctionCall(BaseModel):    \"\"\"    FunctionCall is a data model for a function call being made in a function.    \"\"\"    package: Optional[str] = Field(default=None, alias=\"Package\", description=\"package name of the function call\")    type: Optional[str] = Field(default=None, alias=\"Type\", description=\"type of the function call\")    node_name: Optional[str] = Field(default=None, alias=\"NodeName\", description=\"name of the class being called\")    function_name: Optional[str] = Field(default=None, alias=\"FunctionName\", description=\"name of the function being called\")    parameters: List[ChapiParameter] = Field(default_factory=list, alias=\"Parameters\", description=\"parameters of the function call\")    position: Optional[Position] = Field(default=None, alias=\"Position\", exclude=True, description=\"position of the function call in the code\")"},{"NodeName":"ChapiImport","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi/chapi_import.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":8,"StopLine":11},"Content":"class ChapiImport(BaseModel):    source: Optional[str] = Field(default=None, alias=\"Source\")    usage_name: Optional[List[str]] = Field(default_factory=list, alias=\"UsageName\")"},{"NodeName":"ChapiParameter","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi/chapi_parameter.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":8,"StopLine":12},"Content":"class ChapiParameter(BaseModel):    type_value: Optional[str] = Field(default=None, description=\"parameter name\", alias=\"TypeValue\")    type_type: Optional[str] = Field(default=None, description=\"parameter type\", alias=\"TypeType\")    default_value: Optional[str] = Field(default=None, description=\"parameter default value\", alias=\"DefaultValue\")"},{"NodeName":"ChapiFunction","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi/chapi_function.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_annotation","UsageName":["ChapiAnnotation"]},{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_function_field_model","UsageName":["ChapiFunctionFieldModel"]},{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_functioncall","UsageName":["ChapiFunctionCall"]},{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_parameter","UsageName":["ChapiParameter"]},{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_position","UsageName":["Position"]},{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":21,"StopLine":31},"Content":"class ChapiFunction(BaseModel):    name: Optional[str] = Field(default=None, alias=\"Name\")    return_type: Optional[str] = Field(default=None, alias=\"ReturnType\")    function_calls: List[ChapiFunctionCall] = Field(default_factory=list, alias=\"FunctionCalls\")    parameters: List[ChapiParameter] = Field(default_factory=list, alias=\"Parameters\", description=\"parameters of the function\")    annotations: List[ChapiAnnotation] = Field(default_factory=list, alias=\"Annotations\")    position: Optional[Position] = Field(default=None, alias=\"Position\")    local_variables: List[ChapiFunctionFieldModel] = Field(default_factory=list, alias=\"LocalVariables\")    body_hash: Optional[int] = Field(default=None, alias=\"BodyHash\")    content: Optional[str] = Field(default=None, alias=\"Content\")"},{"NodeName":"ChapiAnnotation","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi/chapi_annotation.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_annotation_key_val","UsageName":["ChapiAnnotationKeyVal"]},{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_position","UsageName":["Position"]},{"Source":"typing","UsageName":["Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":14,"StopLine":18},"Content":"class ChapiAnnotation(BaseModel):    name: Optional[str] = Field(default=None, alias=\"Name\")    key_values: Optional[list[ChapiAnnotationKeyVal]] = Field(default_factory=list, alias=\"KeyValues\")    position: Optional[Position] = Field(default=None, alias=\"Position\")"},{"NodeName":"Position","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi/chapi_position.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"typing","UsageName":["Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":8,"StopLine":13},"Content":"class Position(BaseModel):    start_line: Optional[int] = Field(default=None, alias=\"StartLine\")    start_line_position: Optional[int] = Field(default=None, alias=\"StartLinePosition\")    stop_line: Optional[int] = Field(default=None, alias=\"StopLine\")    stop_line_position: Optional[int] = Field(default=None, alias=\"StopLinePosition\")"},{"NodeName":"ClassGlobalFieldModel","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/models/chapi/chapi_class_global_fieldmodel.py","MultipleExtend":["BaseModel"],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_annotation","UsageName":["ChapiAnnotation"]},{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_position","UsageName":["Position"]},{"Source":"typing","UsageName":["List","Optional"]},{"Source":"pydantic","UsageName":["BaseModel","Field"]}],"Position":{"StartLine":14,"StopLine":20},"Content":"class ClassGlobalFieldModel(BaseModel):    class_field_name: str = Field(..., alias=\"TypeValue\", description=\"Class Field Name\")    class_field_type: Optional[str] = Field(default=None, alias=\"TypeType\", description=\"Class Field Type\")    class_field_value: Optional[str] = Field(default=None, alias=\"DefaultValue\", description=\"Class Variable Value\")    annotations: Optional[List[ChapiAnnotation]] = Field(default=None, alias=\"Annotations\", description=\"Class Field Annotation\")    position: Optional[Position] = Field(default=None, alias=\"Position\", description=\"Class Field Position\")"},{"NodeName":"PythonPackageNamingStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/python/python_package_naming_strategy.py","Functions":[{"Name":"get_package_name","Parameters":[{"TypeValue":"file_path","TypeType":"str"},{"TypeValue":"workspace_path","TypeType":"str"}],"FunctionCalls":[{"NodeName":"os","FunctionName":"path","Position":{"StartLine":10,"StartLinePosition":25,"StopLine":10,"StopLinePosition":26}},{"NodeName":"os","FunctionName":"dirname","Position":{"StartLine":10,"StartLinePosition":30,"StopLine":10,"StopLinePosition":52}},{"NodeName":"os","FunctionName":"replace","Position":{"StartLine":10,"StartLinePosition":53,"StopLine":10,"StopLinePosition":78}}],"Position":{"StartLine":8,"StartLinePosition":4,"StopLine":13},"LocalVariables":[{"TypeValue":"relative_path","TypeType":"os"},{"TypeValue":"package_name","TypeType":"os"}],"Content":"def get_package_name(self, file_path: str, workspace_path: str) -> str:        relative_path = os.path.relpath(file_path, workspace_path)        package_name = os.path.dirname(relative_path).replace(os.path.sep, \".\")        return package_name"}],"Imports":[{"Source":"os"}],"Position":{"StartLine":5,"StopLine":13},"Content":"class PythonPackageNamingStrategy:    \"\"\"Python-specific package naming strategy.\"\"\"    def get_package_name(self, file_path: str, workspace_path: str) -> str:        relative_path = os.path.relpath(file_path, workspace_path)        package_name = os.path.dirname(relative_path).replace(os.path.sep, \".\")        return package_name"},{"NodeName":"PythonExtractInheritance","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/python/python_extract_inheritance.py","Functions":[{"Name":"extract_inheritance","Parameters":[{"TypeValue":"node","TypeType":"UnoplatChapiForgeNode"},{"TypeValue":"internal_imports","TypeType":"List[UnoplatImport]"}],"FunctionCalls":[{"NodeName":"internal_imports","FunctionName":"remove","Position":{"StartLine":55,"StartLinePosition":28,"StopLine":55,"StopLinePosition":39}}],"Position":{"StartLine":14,"StartLinePosition":4,"StopLine":58},"LocalVariables":[{"TypeValue":"class_map","TypeType":"{}"},{"TypeValue":"source","TypeType":"imp"},{"TypeValue":"class_name","TypeType":"usage"},{"TypeValue":"class_map[class_name]","TypeType":"(source,usage.original_name,imp)"},{"TypeValue":"imports_to_remove","TypeType":"[]"},{"TypeValue":"","TypeType":"class_map"},{"TypeValue":"node.multiple_extend[i]","TypeType":"f\"{source}.{original_name}\""}],"Content":"def extract_inheritance(self, node: UnoplatChapiForgeNode, internal_imports: List[UnoplatImport]) -> List[UnoplatImport]:        \"\"\"Extract inheritance information from a Python node.           we have list of mulitple extends but they have just class names. We need full qualified names to resolve the actual class.           We have internal imports which consists of always absolute paths as source and then usage names are in form of orignal name and alias.           Now what we have to do is match usage name (be it original or alias if alias exists) with name in multiple extend and replace the name with full qualified name with original name(original name always)           Then in house remove the internal import from list of internal import. Do all modifications in place.        Args:            node: The ChapiUnoplatNode to extract inheritance from            internal_imports: The list of unoplat imports        Returns:            List[UnoplatImport]: Modified list of internal imports with matched imports removed        \"\"\"        if not node.multiple_extend or not internal_imports:            return internal_imports        # Create a map of class names to their full paths        class_map = {}        for imp in internal_imports:            source = imp.source            if imp.usage_names:                for usage in imp.usage_names:                    # If there's an alias, use that, otherwise use original name                    class_name = usage.alias if usage.alias else usage.original_name                    class_map[class_name] = (source, usage.original_name, imp)        # Track imports to remove using list        imports_to_remove = []        # Process each inherited class        for i, class_name in enumerate(node.multiple_extend):            if class_name in class_map:                # Replace the class name with its fully qualified name                source, original_name, imp = class_map[class_name]                node.multiple_extend[i] = f\"{source}.{original_name}\"                if imp not in imports_to_remove:  # Avoid duplicates                    imports_to_remove.append(imp)        # Remove matched imports in-place        for imp in imports_to_remove:            internal_imports.remove(imp)        return internal_imports"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_chapi_forge_node","UsageName":["UnoplatChapiForgeNode"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_import","UsageName":["UnoplatImport"]},{"Source":"typing","UsageName":["List"]}],"Position":{"StartLine":13,"StopLine":58},"Content":"class PythonExtractInheritance:    def extract_inheritance(self, node: UnoplatChapiForgeNode, internal_imports: List[UnoplatImport]) -> List[UnoplatImport]:        \"\"\"Extract inheritance information from a Python node.           we have list of mulitple extends but they have just class names. We need full qualified names to resolve the actual class.           We have internal imports which consists of always absolute paths as source and then usage names are in form of orignal name and alias.           Now what we have to do is match usage name (be it original or alias if alias exists) with name in multiple extend and replace the name with full qualified name with original name(original name always)           Then in house remove the internal import from list of internal import. Do all modifications in place.        Args:            node: The ChapiUnoplatNode to extract inheritance from            internal_imports: The list of unoplat imports        Returns:            List[UnoplatImport]: Modified list of internal imports with matched imports removed        \"\"\"        if not node.multiple_extend or not internal_imports:            return internal_imports        # Create a map of class names to their full paths        class_map = {}        for imp in internal_imports:            source = imp.source            if imp.usage_names:                for usage in imp.usage_names:                    # If there's an alias, use that, otherwise use original name                    class_name = usage.alias if usage.alias else usage.original_name                    class_map[class_name] = (source, usage.original_name, imp)        # Track imports to remove using list        imports_to_remove = []        # Process each inherited class        for i, class_name in enumerate(node.multiple_extend):            if class_name in class_map:                # Replace the class name with its fully qualified name                source, original_name, imp = class_map[class_name]                node.multiple_extend[i] = f\"{source}.{original_name}\"                if imp not in imports_to_remove:  # Avoid duplicates                    imports_to_remove.append(imp)        # Remove matched imports in-place        for imp in imports_to_remove:            internal_imports.remove(imp)        return internal_imports"},{"NodeName":"PythonImportSegregationStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/python/python_import_segregation_strategy.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"code_confluence_tree_sitter","TypeType":"CodeConfluenceTreeSitter"}],"FunctionCalls":[{"NodeName":"code_confluence_tree_sitter","FunctionName":"get_parser","Position":{"StartLine":38,"StartLinePosition":49,"StopLine":38,"StopLinePosition":61}}],"Position":{"StartLine":31,"StartLinePosition":4,"StopLine":43,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.file_reader","TypeType":"ProgrammingFileReader"},{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"}],"Content":"def __init__(self, code_confluence_tree_sitter: CodeConfluenceTreeSitter):        \"\"\"Initialize the import segregation strategy with required components.                Args:            code_confluence_tree_sitter: Initialized Tree-sitter instance for Python        \"\"\"        self.file_reader = ProgrammingFileReader()        self.parser = code_confluence_tree_sitter.get_parser()                            "},{"Name":"_is_internal_import","Parameters":[{"TypeValue":"module_path","TypeType":"str"},{"TypeValue":"source_directory","TypeType":"str"}],"Position":{"StartLine":43,"StartLinePosition":4,"StopLine":50,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.file_reader","TypeType":"ProgrammingFileReader"},{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"}],"Content":"def _is_internal_import(self, module_path: str, source_directory: str) -> bool:        \"\"\"Check if an import is internal based on source directory.\"\"\"        return (            module_path.startswith(f\"{source_directory}.\") or             module_path == source_directory        )    "},{"Name":"_extract_module_path","Parameters":[{"TypeValue":"node","TypeType":"Node"}],"Position":{"StartLine":50,"StartLinePosition":4,"StopLine":54,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.file_reader","TypeType":"ProgrammingFileReader"},{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"}],"Content":"def _extract_module_path(self, node: Node) -> str:        \"\"\"Extract the module path from a Tree-sitter node.\"\"\"        return node.text.decode('utf-8').strip()    "},{"Name":"_process_from_import_statement","Parameters":[{"TypeValue":"node","TypeType":"Node"},{"TypeValue":"source_directory","TypeType":"str"},{"TypeValue":"code_bytes","TypeType":"bytes"}],"Position":{"StartLine":54,"StartLinePosition":4,"StopLine":120,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.file_reader","TypeType":"ProgrammingFileReader"},{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"source","TypeType":""},{"TypeValue":"usage_names","TypeType":""},{"TypeValue":"current","TypeType":"current"},{"TypeValue":"name_node","TypeType":"current"},{"TypeValue":"alias_node","TypeType":"current"},{"TypeValue":"name_text","TypeType":""},{"TypeValue":"alias_text","TypeType":"alias_node"},{"TypeValue":"import_type","TypeType":"(ImportType.INTERNALifsource.startswith(f\"{source_directory}.\")orsource==source_directoryelseImportType.EXTERNAL)"}],"Content":"def _process_from_import_statement(        self, node: Node, source_directory: str, code_bytes: bytes    ) -> Optional[UnoplatImport]:        \"\"\"        Process a from-import statement AST node using next_named_sibling to walk through imports.                Args:            node: The from-import statement node (dotted_name node captured as @module).            source_directory: The source directory prefix for internal imports.            code_bytes: The original source code bytes.                Returns:            A UnoplatImport object or None if parsing fails.        \"\"\"        # Get the source from the dotted_name node (which is our @module capture)        source: str = self._extract_module_path(node)                usage_names: List[ImportedName] = []                # Start with the first aliased_import after the 'import' keyword        current = node.next_named_sibling                # Walk through all aliased_imports using next_named_sibling        while current:            if current.type == \"aliased_import\":                name_node = current.child(0)                alias_node = current.child(2)                                if name_node:                    name_text: str = name_node.text.decode(\"utf8\").strip() #type: ignore                    alias_text: Optional[str] = None                                        if alias_node:                        alias_text = alias_node.text.decode(\"utf8\").strip() #type: ignore                                        usage_names.append(                        ImportedName(                            original_name=name_text,                            alias=alias_text                        )                    )            elif current.type == \"dotted_name\":                name_node = current.child(0)                if name_node:                    name_text: str = name_node.text.decode(\"utf8\").strip() #type: ignore                    usage_names.append(ImportedName(original_name=name_text))                        # Move to next sibling            current = current.next_named_sibling                if not usage_names:            return None        # Determine import type based on source prefix        import_type = (            ImportType.INTERNAL             if source.startswith(f\"{source_directory}.\") or source == source_directory            else ImportType.EXTERNAL        )                    return UnoplatImport(            Source=source,            UsageName=usage_names,            ImportType=import_type        )    "},{"Name":"process_imports","Parameters":[{"TypeValue":"source_directory","TypeType":"str"},{"TypeValue":"class_metadata","TypeType":"ChapiNode"}],"FunctionCalls":[{"FunctionName":"sections","Position":{"StartLine":168,"StartLinePosition":28,"StopLine":168,"StopLinePosition":47}},{"NodeName":"sections","FunctionName":"append","Position":{"StartLine":168,"StartLinePosition":48,"StopLine":168,"StopLinePosition":62}}],"Position":{"StartLine":120,"StartLinePosition":4,"StopLine":170,"StopLinePosition":8},"LocalVariables":[{"TypeValue":"self.file_reader","TypeType":"ProgrammingFileReader"},{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"source","TypeType":""},{"TypeValue":"usage_names","TypeType":""},{"TypeValue":"current","TypeType":"current"},{"TypeValue":"name_node","TypeType":"current"},{"TypeValue":"alias_node","TypeType":"current"},{"TypeValue":"name_text","TypeType":""},{"TypeValue":"alias_text","TypeType":"alias_node"},{"TypeValue":"import_type","TypeType":"(ImportType.INTERNALifsource.startswith(f\"{source_directory}.\")orsource==source_directoryelseImportType.EXTERNAL)"},{"TypeValue":"sections","TypeType":""},{"TypeValue":"file_content","TypeType":""},{"TypeValue":"code_bytes","TypeType":""},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"escaped_prefix","TypeType":"source_directory"},{"TypeValue":"query","TypeType":"self"},{"TypeValue":"matches","TypeType":"query"},{"TypeValue":"module_nodes","TypeType":"captures"},{"TypeValue":"self._process_from_import_statement(module_node,source_directory,code_bytes)","TypeType":""}],"Content":"def process_imports(        self, source_directory: str, class_metadata: ChapiNode    ) -> Dict[ImportType, List[UnoplatImport]]:        \"\"\"Process and categorize imports from a Python source file.        Args:            source_directory: The source directory prefix for internal imports            class_metadata: Metadata about the class/file being processed        Returns:            Dictionary mapping ImportType to list of UnoplatImport objects.        \"\"\"        # Initialize sections dictionary        sections: Dict[ImportType, List[UnoplatImport]] = {            ImportType.INTERNAL: [],            ImportType.STANDARD: [],            ImportType.EXTERNAL: [],            ImportType.LOCAL: []        }        # Read and parse the file        file_content: str = self.file_reader.read_file(class_metadata.file_path)         code_bytes: bytes = file_content.encode(\"utf8\")        tree = self.parser.parse(code_bytes)        # Define query to capture from-imports with the given source_directory        # Escape dots in source_directory for the regex pattern        escaped_prefix = source_directory.replace(\".\", \"\\\\.\")        query = self.parser.language.query(            f\"\"\"            (              import_from_statement                 module_name: (dotted_name) @module            )            (#match? @module \"^{escaped_prefix}\")            \"\"\"        )        # Get matches        matches = query.matches(tree.root_node)        # Process each match using next_named_sibling traversal        for _, captures in matches:            module_nodes = captures.get(\"module\", [])            for module_node in module_nodes:                if result := self._process_from_import_statement(                    module_node, source_directory, code_bytes                ):                    sections[result.import_type].append(result) #type:ignore        "}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_node","UsageName":["ChapiNode"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_import","UsageName":["ImportedName","UnoplatImport"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_import_type","UsageName":["ImportType"]},{"Source":"src.code_confluence_flow_bridge.parser.python.utils.read_programming_file","UsageName":["ProgrammingFileReader"]},{"Source":"src.code_confluence_flow_bridge.parser.tree_sitter.code_confluence_tree_sitter","UsageName":["CodeConfluenceTreeSitter"]},{"Source":"typing","UsageName":["Dict","List","Optional"]},{"Source":"tree_sitter","UsageName":["Node"]}],"Position":{"StartLine":23,"StopLine":171},"Content":"class PythonImportSegregationStrategy:    \"\"\"Strategy for segregating Python imports into different categories based on their types.    This class processes Python source files to extract and categorize imports based on their    types using Tree-sitter parsing. It specifically identifies internal imports based on    the source directory prefix.    \"\"\"    def __init__(self, code_confluence_tree_sitter: CodeConfluenceTreeSitter):        \"\"\"Initialize the import segregation strategy with required components.                Args:            code_confluence_tree_sitter: Initialized Tree-sitter instance for Python        \"\"\"        self.file_reader = ProgrammingFileReader()        self.parser = code_confluence_tree_sitter.get_parser()                            def _is_internal_import(self, module_path: str, source_directory: str) -> bool:        \"\"\"Check if an import is internal based on source directory.\"\"\"        return (            module_path.startswith(f\"{source_directory}.\") or             module_path == source_directory        )    def _extract_module_path(self, node: Node) -> str:        \"\"\"Extract the module path from a Tree-sitter node.\"\"\"        return node.text.decode('utf-8').strip()    def _process_from_import_statement(        self, node: Node, source_directory: str, code_bytes: bytes    ) -> Optional[UnoplatImport]:        \"\"\"        Process a from-import statement AST node using next_named_sibling to walk through imports.                Args:            node: The from-import statement node (dotted_name node captured as @module).            source_directory: The source directory prefix for internal imports.            code_bytes: The original source code bytes.                Returns:            A UnoplatImport object or None if parsing fails.        \"\"\"        # Get the source from the dotted_name node (which is our @module capture)        source: str = self._extract_module_path(node)                usage_names: List[ImportedName] = []                # Start with the first aliased_import after the 'import' keyword        current = node.next_named_sibling                # Walk through all aliased_imports using next_named_sibling        while current:            if current.type == \"aliased_import\":                name_node = current.child(0)                alias_node = current.child(2)                                if name_node:                    name_text: str = name_node.text.decode(\"utf8\").strip() #type: ignore                    alias_text: Optional[str] = None                                        if alias_node:                        alias_text = alias_node.text.decode(\"utf8\").strip() #type: ignore                                        usage_names.append(                        ImportedName(                            original_name=name_text,                            alias=alias_text                        )                    )            elif current.type == \"dotted_name\":                name_node = current.child(0)                if name_node:                    name_text: str = name_node.text.decode(\"utf8\").strip() #type: ignore                    usage_names.append(ImportedName(original_name=name_text))                        # Move to next sibling            current = current.next_named_sibling                if not usage_names:            return None        # Determine import type based on source prefix        import_type = (            ImportType.INTERNAL             if source.startswith(f\"{source_directory}.\") or source == source_directory            else ImportType.EXTERNAL        )                    return UnoplatImport(            Source=source,            UsageName=usage_names,            ImportType=import_type        )    def process_imports(        self, source_directory: str, class_metadata: ChapiNode    ) -> Dict[ImportType, List[UnoplatImport]]:        \"\"\"Process and categorize imports from a Python source file.        Args:            source_directory: The source directory prefix for internal imports            class_metadata: Metadata about the class/file being processed        Returns:            Dictionary mapping ImportType to list of UnoplatImport objects.        \"\"\"        # Initialize sections dictionary        sections: Dict[ImportType, List[UnoplatImport]] = {            ImportType.INTERNAL: [],            ImportType.STANDARD: [],            ImportType.EXTERNAL: [],            ImportType.LOCAL: []        }        # Read and parse the file        file_content: str = self.file_reader.read_file(class_metadata.file_path)         code_bytes: bytes = file_content.encode(\"utf8\")        tree = self.parser.parse(code_bytes)        # Define query to capture from-imports with the given source_directory        # Escape dots in source_directory for the regex pattern        escaped_prefix = source_directory.replace(\".\", \"\\\\.\")        query = self.parser.language.query(            f\"\"\"            (              import_from_statement                 module_name: (dotted_name) @module            )            (#match? @module \"^{escaped_prefix}\")            \"\"\"        )        # Get matches        matches = query.matches(tree.root_node)        # Process each match using next_named_sibling traversal        for _, captures in matches:            module_nodes = captures.get(\"module\", [])            for module_node in module_nodes:                if result := self._process_from_import_statement(                    module_node, source_directory, code_bytes                ):                    sections[result.import_type].append(result) #type:ignore        return sections"},{"NodeName":"ProgrammingFileReader","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/python/utils/read_programming_file.py","Functions":[{"Name":"read_file","Parameters":[{"TypeValue":"file_path","TypeType":"str|Path"}],"FunctionCalls":[{"FunctionName":"Path","Position":{"StartLine":21,"StartLinePosition":24,"StopLine":21,"StopLinePosition":34}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":6,"StartLinePosition":4,"StopLine":7,"StopLinePosition":4}}],"Position":{"StartLine":7,"StartLinePosition":4,"StopLine":30},"LocalVariables":[{"TypeValue":"file_path","TypeType":"Path"}],"Content":"def read_file(file_path: str | Path) -> str:        \"\"\"        Reads the content of a file and returns it as a string.        Args:            file_path (str | Path): Path to the file to be read        Returns:            str: Content of the file as a string        Raises:            FileNotFoundError: If the specified file does not exist            IOError: If there are issues reading the file        \"\"\"        file_path = Path(file_path)        if not file_path.exists():            raise FileNotFoundError(f\"File not found: {file_path}\")        try:            with open(file_path, \"r\", encoding=\"utf-8\") as file:                return file.read()        except IOError as e:            raise IOError(f\"Error reading file {file_path}: {str(e)}\")"}],"Imports":[{"Source":"pathlib","UsageName":["Path"]}],"Position":{"StartLine":5,"StopLine":30},"Content":"class ProgrammingFileReader:    @staticmethod    def read_file(file_path: str | Path) -> str:        \"\"\"        Reads the content of a file and returns it as a string.        Args:            file_path (str | Path): Path to the file to be read        Returns:            str: Content of the file as a string        Raises:            FileNotFoundError: If the specified file does not exist            IOError: If there are issues reading the file        \"\"\"        file_path = Path(file_path)        if not file_path.exists():            raise FileNotFoundError(f\"File not found: {file_path}\")        try:            with open(file_path, \"r\", encoding=\"utf-8\") as file:                return file.read()        except IOError as e:            raise IOError(f\"Error reading file {file_path}: {str(e)}\")"},{"NodeName":"PythonCodebaseParser","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/python/python_codebase_parser.py","MultipleExtend":["CodebaseParserStrategy"],"Functions":[{"Name":"__init__","FunctionCalls":[{"FunctionName":"NodeVariablesParser","Position":{"StartLine":74,"StartLinePosition":56,"StopLine":74,"StopLinePosition":117}}],"Position":{"StartLine":66,"StartLinePosition":4,"StopLine":77,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.python_extract_inheritance","TypeType":"PythonExtractInheritance"},{"TypeValue":"self.package_naming_strategy","TypeType":"PythonPackageNamingStrategy"},{"TypeValue":"self.qualified_name_strategy","TypeType":"PythonQualifiedNameStrategy"},{"TypeValue":"self.python_import_segregation_strategy","TypeType":"PythonImportSegregationStrategy"},{"TypeValue":"self.code_confluence_tree_sitter","TypeType":"CodeConfluenceTreeSitter"},{"TypeValue":"self.sort_function_dependencies","TypeType":"SortFunctionDependencies"},{"TypeValue":"self.python_node_dependency_processor","TypeType":"PythonNodeDependencyProcessor"},{"TypeValue":"self.node_variables_parser","TypeType":"NodeVariablesParser"}],"Content":"def __init__(self):        self.python_extract_inheritance = PythonExtractInheritance()        self.package_naming_strategy = PythonPackageNamingStrategy()        self.qualified_name_strategy = PythonQualifiedNameStrategy()        self.python_import_segregation_strategy = PythonImportSegregationStrategy()        self.code_confluence_tree_sitter = CodeConfluenceTreeSitter(language=ProgrammingLanguage.PYTHON)        self.sort_function_dependencies = SortFunctionDependencies()        self.python_node_dependency_processor = PythonNodeDependencyProcessor()        self.node_variables_parser = NodeVariablesParser(code_confluence_tree_sitter=self.code_confluence_tree_sitter)    # we handle procedural , class and mix of procedural and class nodes.    "},{"Name":"__preprocess_nodes","Parameters":[{"TypeValue":"json_data","TypeType":"dict"},{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"source_directory","TypeType":"str"},{"TypeValue":"codebase_name","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"debug","Position":{"StartLine":104,"StartLinePosition":18,"StopLine":104,"StopLinePosition":37}}],"Position":{"StartLine":77,"StartLinePosition":4,"StopLine":108,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.python_extract_inheritance","TypeType":"PythonExtractInheritance"},{"TypeValue":"self.package_naming_strategy","TypeType":"PythonPackageNamingStrategy"},{"TypeValue":"self.qualified_name_strategy","TypeType":"PythonQualifiedNameStrategy"},{"TypeValue":"self.python_import_segregation_strategy","TypeType":"PythonImportSegregationStrategy"},{"TypeValue":"self.code_confluence_tree_sitter","TypeType":"CodeConfluenceTreeSitter"},{"TypeValue":"self.sort_function_dependencies","TypeType":"SortFunctionDependencies"},{"TypeValue":"self.python_node_dependency_processor","TypeType":"PythonNodeDependencyProcessor"},{"TypeValue":"self.node_variables_parser","TypeType":"NodeVariablesParser"},{"TypeValue":"file_path_nodes","TypeType":""},{"TypeValue":"qualified_names_dict","TypeType":""},{"TypeValue":"node","TypeType":""},{"TypeValue":"unoplat_node","TypeType":""},{"TypeValue":"file_path_nodes[unoplat_node.file_path]","TypeType":"[unoplat_node]"},{"TypeValue":"qualified_names_dict[unoplat_node.qualified_name]","TypeType":"unoplat_node"}],"Content":"def __preprocess_nodes(self, json_data: dict, local_workspace_path: str, source_directory: str, codebase_name: str) -> Tuple[Dict[str, List[UnoplatChapiForgeNode]], Dict[str, UnoplatChapiForgeNode]]:        \"\"\"Preprocess nodes to extract qualified names and segregate imports.\"\"\"        file_path_nodes: Dict[str, List[UnoplatChapiForgeNode]] = {}        qualified_names_dict: Dict[str, UnoplatChapiForgeNode] = {}        for item in json_data:            try:                node: ChapiNode = ChapiNode.model_validate(item)                unoplat_node: UnoplatChapiForgeNode = self.__common_node_processing(node, local_workspace_path, source_directory)                # Add debug logging                logger.debug(f\"Processing node: {node.node_name}\")                logger.debug(f\"Qualified name: {unoplat_node.qualified_name}\")                if unoplat_node.file_path not in file_path_nodes:                    file_path_nodes[unoplat_node.file_path] = [unoplat_node]  # type: ignore                else:                    file_path_nodes[unoplat_node.file_path].append(unoplat_node)                qualified_names_dict[unoplat_node.qualified_name] = unoplat_node            except Exception as e:                logger.error(f\"Error building qualified name map: {e}\")        # Add debug logging for final map        logger.debug(\"Qualified names in dict:\")        for qname in qualified_names_dict.keys():            logger.debug(f\"  {qname}\")        return file_path_nodes, qualified_names_dict    "},{"Name":"__common_node_processing","Parameters":[{"TypeValue":"node","TypeType":"ChapiNode"},{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"source_directory","TypeType":"str"}],"FunctionCalls":[{"NodeName":"self","FunctionName":"package_naming_strategy","Position":{"StartLine":152,"StartLinePosition":31,"StopLine":152,"StopLinePosition":32}},{"NodeName":"self","FunctionName":"get_package_name","Position":{"StartLine":152,"StartLinePosition":55,"StopLine":152,"StopLinePosition":109}}],"Position":{"StartLine":108,"StartLinePosition":4,"StopLine":165,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.python_extract_inheritance","TypeType":"PythonExtractInheritance"},{"TypeValue":"self.package_naming_strategy","TypeType":"PythonPackageNamingStrategy"},{"TypeValue":"self.qualified_name_strategy","TypeType":"PythonQualifiedNameStrategy"},{"TypeValue":"self.python_import_segregation_strategy","TypeType":"PythonImportSegregationStrategy"},{"TypeValue":"self.code_confluence_tree_sitter","TypeType":"CodeConfluenceTreeSitter"},{"TypeValue":"self.sort_function_dependencies","TypeType":"SortFunctionDependencies"},{"TypeValue":"self.python_node_dependency_processor","TypeType":"PythonNodeDependencyProcessor"},{"TypeValue":"self.node_variables_parser","TypeType":"NodeVariablesParser"},{"TypeValue":"file_path_nodes","TypeType":""},{"TypeValue":"qualified_names_dict","TypeType":""},{"TypeValue":"node","TypeType":""},{"TypeValue":"unoplat_node","TypeType":""},{"TypeValue":"file_path_nodes[unoplat_node.file_path]","TypeType":"[unoplat_node]"},{"TypeValue":"qualified_names_dict[unoplat_node.qualified_name]","TypeType":"unoplat_node"},{"TypeValue":"node.node_name","TypeType":"os"},{"TypeValue":"qualified_name","TypeType":"self"},{"TypeValue":"import_prefix","TypeType":"os"},{"TypeValue":"imports_dict","TypeType":""},{"TypeValue":"final_internal_imports","TypeType":"self"},{"TypeValue":"imports_dict[ImportType.INTERNAL]","TypeType":"final_internal_imports"},{"TypeValue":"node.package","TypeType":"self"}],"Content":"def __common_node_processing(self, node: ChapiNode, local_workspace_path: str, source_directory: str):        \"\"\"Process common node attributes and imports.                Args:            node: The ChapiNode to process            local_workspace_path: Path like /Users/user/projects/myproject/src/code_confluence_flow_bridge            source_directory: Path like /Users/user/projects/myproject        \"\"\"        if node.node_name == \"default\":            node.node_name = os.path.basename(node.file_path).split(\".\")[0] if node.file_path else \"unknown\"        if node.node_name and node.file_path:  # Type guard for linter            qualified_name = self.qualified_name_strategy.get_qualified_name(                node_name=node.node_name,                 node_file_path=node.file_path,                 local_workspace_path=local_workspace_path,                 node_type=node.type            )                # Get the import prefix by finding the path from workspace to source root        # If local_workspace_path is /Users/user/projects/myproject/src/code_confluence_flow_bridge        # And source_directory is /Users/user/projects/myproject        # Then we want \"src.code_confluence_flow_bridge\" as the prefix        import_prefix = os.path.relpath(            local_workspace_path,  # From workspace path            source_directory      # To source root        ).replace(os.sep, \".\")                # segregating imports        imports_dict: Dict[ImportType, List[UnoplatImport]] = self.python_import_segregation_strategy.process_imports(            source_directory=import_prefix,  # Now correctly \"src.code_confluence_flow_bridge\"            class_metadata=node        )        # Extracting inheritance        if imports_dict and ImportType.INTERNAL in imports_dict:            final_internal_imports = self.python_extract_inheritance.extract_inheritance(                node,                 imports_dict[ImportType.INTERNAL]            )            imports_dict[ImportType.INTERNAL] = final_internal_imports        # Todo: Add dependent nodes        if node.file_path:  # Type guard for linter            node.package = self.package_naming_strategy.get_package_name(node.file_path, local_workspace_path)        # TODO: enable below when archguard fixes the formatting issues of code content - be it class or function        # node.fields = []        # if node is of type class do parse class variables and instance variables and add them to node.class_variables        # if node.type == \"CLASS\":        #     node.fields = self.node_variables_parser.parse_class_variables(node.content, node.functions)        # if node.functions:        #     self.python_function_calls.process_functions(node.functions)        return UnoplatChapiForgeNode.from_chapi_node(chapi_node=node, qualified_name=qualified_name, segregated_imports=imports_dict if imports_dict is not None else {})    "},{"Name":"parse_codebase","Parameters":[{"TypeValue":"codebase_name","TypeType":"str"},{"TypeValue":"json_data","TypeType":"dict"},{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"source_directory","TypeType":"str"},{"TypeValue":"programming_language_metadata","TypeType":"ProgrammingLanguageMetadata"}],"Position":{"StartLine":165,"StartLinePosition":4,"StopLine":227},"LocalVariables":[{"TypeValue":"self.python_extract_inheritance","TypeType":"PythonExtractInheritance"},{"TypeValue":"self.package_naming_strategy","TypeType":"PythonPackageNamingStrategy"},{"TypeValue":"self.qualified_name_strategy","TypeType":"PythonQualifiedNameStrategy"},{"TypeValue":"self.python_import_segregation_strategy","TypeType":"PythonImportSegregationStrategy"},{"TypeValue":"self.code_confluence_tree_sitter","TypeType":"CodeConfluenceTreeSitter"},{"TypeValue":"self.sort_function_dependencies","TypeType":"SortFunctionDependencies"},{"TypeValue":"self.python_node_dependency_processor","TypeType":"PythonNodeDependencyProcessor"},{"TypeValue":"self.node_variables_parser","TypeType":"NodeVariablesParser"},{"TypeValue":"file_path_nodes","TypeType":""},{"TypeValue":"qualified_names_dict","TypeType":""},{"TypeValue":"node","TypeType":""},{"TypeValue":"unoplat_node","TypeType":""},{"TypeValue":"file_path_nodes[unoplat_node.file_path]","TypeType":"[unoplat_node]"},{"TypeValue":"qualified_names_dict[unoplat_node.qualified_name]","TypeType":"unoplat_node"},{"TypeValue":"node.node_name","TypeType":"os"},{"TypeValue":"qualified_name","TypeType":"self"},{"TypeValue":"import_prefix","TypeType":"os"},{"TypeValue":"imports_dict","TypeType":""},{"TypeValue":"final_internal_imports","TypeType":"self"},{"TypeValue":"imports_dict[ImportType.INTERNAL]","TypeType":"final_internal_imports"},{"TypeValue":"node.package","TypeType":"self"},{"TypeValue":"","TypeType":"self"},{"TypeValue":"unoplat_package_dict","TypeType":""},{"TypeValue":"content_of_file","TypeType":"ProgrammingFileReader"},{"TypeValue":"global_variables","TypeType":""},{"TypeValue":"dependent_classes","TypeType":""},{"TypeValue":"sorted_functions","TypeType":""},{"TypeValue":"node.global_variables","TypeType":"global_variables"},{"TypeValue":"node.functions","TypeType":"sorted_functions"},{"TypeValue":"node.dependent_internal_classes","TypeType":"dependent_classes"},{"TypeValue":"package_parts","TypeType":"node"},{"TypeValue":"current_package","TypeType":"current_package"},{"TypeValue":"full_package_name","TypeType":"part"},{"TypeValue":"current_package[full_package_name]","TypeType":"UnoplatPackage"},{"TypeValue":"current_package[full_package_name].nodes[file_path]","TypeType":"[]"},{"TypeValue":"packages","TypeType":""}],"Content":"def parse_codebase(self, codebase_name: str, json_data: dict, local_workspace_path: str, source_directory: str, programming_language_metadata: ProgrammingLanguageMetadata) -> List[UnoplatPackage]:        \"\"\"Parse the entire codebase.        First preprocesses nodes to extract qualified names and segregate imports,        then processes dependencies for each node using that map.        \"\"\"        # Phase 1: Preprocess nodes        preprocessed_file_path_nodes, preprocessed_qualified_name_dict = self.__preprocess_nodes(json_data, local_workspace_path, source_directory, codebase_name)        # Phase 2: Process dependencies using the map        unoplat_package_dict: Dict[str, UnoplatPackage] = {}        for file_path, nodes in preprocessed_file_path_nodes.items():            try:                # TODO: enable when archguard fixes the formatting issues of code content - be it class or function                # The operation of generating global variables should be done once per file even if there are multiple nodes in the file                content_of_file = ProgrammingFileReader.read_file(file_path)                global_variables: List[ClassGlobalFieldModel] = self.node_variables_parser.parse_global_variables(content_of_file)                # The operation of figuring out dependent class should be done once per file even if there are multiple nodes in the file                dependent_classes: List[str] = self.python_node_dependency_processor.process_dependencies(nodes[0], preprocessed_qualified_name_dict)                # Process all nodes from file                for node in nodes:                    # TODO: check for interface and abstract class - what are types from chapi                    sorted_functions: List[UnoplatChapiForgeFunction] = self.sort_function_dependencies.sort_function_dependencies(functions=node.functions, node_type=node.type)                    # TODO: enable when archguard fixes the formatting issues of code content - be it class or function                    node.global_variables = global_variables                    if sorted_functions:                        node.functions = sorted_functions                    # Generate dependent classes                    # skip first node since it is already processed                    if node.node_name != nodes[0].node_name:                        node.dependent_internal_classes = dependent_classes                    # Build package structure                    if node.package:                        package_parts = node.package.split(\".\")                        current_package = unoplat_package_dict                        full_package_name = \"\"                        for i, part in enumerate(package_parts):                            full_package_name = part if i == 0 else f\"{full_package_name}.{part}\"                            if full_package_name not in current_package:                                current_package[full_package_name] = UnoplatPackage(name=full_package_name)                            if i == len(package_parts) - 1:                                # Add nodes to dict by file path                                if file_path not in current_package[full_package_name].nodes:                                    current_package[full_package_name].nodes[file_path] = []                                current_package[full_package_name].nodes[file_path].append(node)                            else:                                current_package = current_package[full_package_name].sub_packages  # type: ignore            except Exception as e:                logger.error(f\"Error processing node dependencies: {e}\")                packages: List[UnoplatPackage] = list(unoplat_package_dict.values()) if unoplat_package_dict else []                return packages"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_class_global_fieldmodel","UsageName":["ClassGlobalFieldModel"]},{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_node","UsageName":["ChapiNode"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_chapi_forge_function","UsageName":["UnoplatChapiForgeFunction"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_chapi_forge_node","UsageName":["UnoplatChapiForgeNode"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_import","UsageName":["UnoplatImport"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_import_type","UsageName":["ImportType"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package","UsageName":["UnoplatPackage"]},{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["ProgrammingLanguage","ProgrammingLanguageMetadata"]},{"Source":"src.code_confluence_flow_bridge.parser.codebase_parser_strategy","UsageName":["CodebaseParserStrategy"]},{"Source":"src.code_confluence_flow_bridge.parser.python.in_class_dependency.sort_function_dependencies","UsageName":["SortFunctionDependencies"]},{"Source":"src.code_confluence_flow_bridge.parser.python.node_variables.node_variables_parser","UsageName":["NodeVariablesParser"]},{"Source":"src.code_confluence_flow_bridge.parser.python.python_extract_inheritance","UsageName":["PythonExtractInheritance"]},{"Source":"src.code_confluence_flow_bridge.parser.python.python_import_segregation_strategy","UsageName":["PythonImportSegregationStrategy"]},{"Source":"src.code_confluence_flow_bridge.parser.python.python_node_dependency_processor","UsageName":["PythonNodeDependencyProcessor"]},{"Source":"src.code_confluence_flow_bridge.parser.python.python_package_naming_strategy","UsageName":["PythonPackageNamingStrategy"]},{"Source":"src.code_confluence_flow_bridge.parser.python.python_qualified_name_strategy","UsageName":["PythonQualifiedNameStrategy"]},{"Source":"src.code_confluence_flow_bridge.parser.python.utils.read_programming_file","UsageName":["ProgrammingFileReader"]},{"Source":"src.code_confluence_flow_bridge.parser.tree_sitter.code_confluence_tree_sitter","UsageName":["CodeConfluenceTreeSitter"]},{"Source":"os"},{"Source":"typing","UsageName":["Dict","List","Tuple"]},{"Source":"loguru","UsageName":["logger"]}],"Position":{"StartLine":65,"StopLine":227},"Content":"class PythonCodebaseParser(CodebaseParserStrategy):    def __init__(self):        self.python_extract_inheritance = PythonExtractInheritance()        self.package_naming_strategy = PythonPackageNamingStrategy()        self.qualified_name_strategy = PythonQualifiedNameStrategy()        self.python_import_segregation_strategy = PythonImportSegregationStrategy()        self.code_confluence_tree_sitter = CodeConfluenceTreeSitter(language=ProgrammingLanguage.PYTHON)        self.sort_function_dependencies = SortFunctionDependencies()        self.python_node_dependency_processor = PythonNodeDependencyProcessor()        self.node_variables_parser = NodeVariablesParser(code_confluence_tree_sitter=self.code_confluence_tree_sitter)    # we handle procedural , class and mix of procedural and class nodes.    def __preprocess_nodes(self, json_data: dict, local_workspace_path: str, source_directory: str, codebase_name: str) -> Tuple[Dict[str, List[UnoplatChapiForgeNode]], Dict[str, UnoplatChapiForgeNode]]:        \"\"\"Preprocess nodes to extract qualified names and segregate imports.\"\"\"        file_path_nodes: Dict[str, List[UnoplatChapiForgeNode]] = {}        qualified_names_dict: Dict[str, UnoplatChapiForgeNode] = {}        for item in json_data:            try:                node: ChapiNode = ChapiNode.model_validate(item)                unoplat_node: UnoplatChapiForgeNode = self.__common_node_processing(node, local_workspace_path, source_directory)                # Add debug logging                logger.debug(f\"Processing node: {node.node_name}\")                logger.debug(f\"Qualified name: {unoplat_node.qualified_name}\")                if unoplat_node.file_path not in file_path_nodes:                    file_path_nodes[unoplat_node.file_path] = [unoplat_node]  # type: ignore                else:                    file_path_nodes[unoplat_node.file_path].append(unoplat_node)                qualified_names_dict[unoplat_node.qualified_name] = unoplat_node            except Exception as e:                logger.error(f\"Error building qualified name map: {e}\")        # Add debug logging for final map        logger.debug(\"Qualified names in dict:\")        for qname in qualified_names_dict.keys():            logger.debug(f\"  {qname}\")        return file_path_nodes, qualified_names_dict    def __common_node_processing(self, node: ChapiNode, local_workspace_path: str, source_directory: str):        \"\"\"Process common node attributes and imports.                Args:            node: The ChapiNode to process            local_workspace_path: Path like /Users/user/projects/myproject/src/code_confluence_flow_bridge            source_directory: Path like /Users/user/projects/myproject        \"\"\"        if node.node_name == \"default\":            node.node_name = os.path.basename(node.file_path).split(\".\")[0] if node.file_path else \"unknown\"        if node.node_name and node.file_path:  # Type guard for linter            qualified_name = self.qualified_name_strategy.get_qualified_name(                node_name=node.node_name,                 node_file_path=node.file_path,                 local_workspace_path=local_workspace_path,                 node_type=node.type            )                # Get the import prefix by finding the path from workspace to source root        # If local_workspace_path is /Users/user/projects/myproject/src/code_confluence_flow_bridge        # And source_directory is /Users/user/projects/myproject        # Then we want \"src.code_confluence_flow_bridge\" as the prefix        import_prefix = os.path.relpath(            local_workspace_path,  # From workspace path            source_directory      # To source root        ).replace(os.sep, \".\")                # segregating imports        imports_dict: Dict[ImportType, List[UnoplatImport]] = self.python_import_segregation_strategy.process_imports(            source_directory=import_prefix,  # Now correctly \"src.code_confluence_flow_bridge\"            class_metadata=node        )        # Extracting inheritance        if imports_dict and ImportType.INTERNAL in imports_dict:            final_internal_imports = self.python_extract_inheritance.extract_inheritance(                node,                 imports_dict[ImportType.INTERNAL]            )            imports_dict[ImportType.INTERNAL] = final_internal_imports        # Todo: Add dependent nodes        if node.file_path:  # Type guard for linter            node.package = self.package_naming_strategy.get_package_name(node.file_path, local_workspace_path)        # TODO: enable below when archguard fixes the formatting issues of code content - be it class or function        # node.fields = []        # if node is of type class do parse class variables and instance variables and add them to node.class_variables        # if node.type == \"CLASS\":        #     node.fields = self.node_variables_parser.parse_class_variables(node.content, node.functions)        # if node.functions:        #     self.python_function_calls.process_functions(node.functions)        return UnoplatChapiForgeNode.from_chapi_node(chapi_node=node, qualified_name=qualified_name, segregated_imports=imports_dict if imports_dict is not None else {})    def parse_codebase(self, codebase_name: str, json_data: dict, local_workspace_path: str, source_directory: str, programming_language_metadata: ProgrammingLanguageMetadata) -> List[UnoplatPackage]:        \"\"\"Parse the entire codebase.        First preprocesses nodes to extract qualified names and segregate imports,        then processes dependencies for each node using that map.        \"\"\"        # Phase 1: Preprocess nodes        preprocessed_file_path_nodes, preprocessed_qualified_name_dict = self.__preprocess_nodes(json_data, local_workspace_path, source_directory, codebase_name)        # Phase 2: Process dependencies using the map        unoplat_package_dict: Dict[str, UnoplatPackage] = {}        for file_path, nodes in preprocessed_file_path_nodes.items():            try:                # TODO: enable when archguard fixes the formatting issues of code content - be it class or function                # The operation of generating global variables should be done once per file even if there are multiple nodes in the file                content_of_file = ProgrammingFileReader.read_file(file_path)                global_variables: List[ClassGlobalFieldModel] = self.node_variables_parser.parse_global_variables(content_of_file)                # The operation of figuring out dependent class should be done once per file even if there are multiple nodes in the file                dependent_classes: List[str] = self.python_node_dependency_processor.process_dependencies(nodes[0], preprocessed_qualified_name_dict)                # Process all nodes from file                for node in nodes:                    # TODO: check for interface and abstract class - what are types from chapi                    sorted_functions: List[UnoplatChapiForgeFunction] = self.sort_function_dependencies.sort_function_dependencies(functions=node.functions, node_type=node.type)                    # TODO: enable when archguard fixes the formatting issues of code content - be it class or function                    node.global_variables = global_variables                    if sorted_functions:                        node.functions = sorted_functions                    # Generate dependent classes                    # skip first node since it is already processed                    if node.node_name != nodes[0].node_name:                        node.dependent_internal_classes = dependent_classes                    # Build package structure                    if node.package:                        package_parts = node.package.split(\".\")                        current_package = unoplat_package_dict                        full_package_name = \"\"                        for i, part in enumerate(package_parts):                            full_package_name = part if i == 0 else f\"{full_package_name}.{part}\"                            if full_package_name not in current_package:                                current_package[full_package_name] = UnoplatPackage(name=full_package_name)                            if i == len(package_parts) - 1:                                # Add nodes to dict by file path                                if file_path not in current_package[full_package_name].nodes:                                    current_package[full_package_name].nodes[file_path] = []                                current_package[full_package_name].nodes[file_path].append(node)                            else:                                current_package = current_package[full_package_name].sub_packages  # type: ignore            except Exception as e:                logger.error(f\"Error processing node dependencies: {e}\")                packages: List[UnoplatPackage] = list(unoplat_package_dict.values()) if unoplat_package_dict else []                return packages"},{"NodeName":"NodeVariablesParser","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/python/node_variables/node_variables_parser.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"code_confluence_tree_sitter","TypeType":"CodeConfluenceTreeSitter"}],"FunctionCalls":[{"NodeName":"self","FunctionName":"variables_dict","Position":{"StartLine":28,"StartLinePosition":12,"StopLine":28,"StopLinePosition":13}}],"Position":{"StartLine":25,"StartLinePosition":4,"StopLine":31,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":""}],"Content":"def __init__(self, code_confluence_tree_sitter: CodeConfluenceTreeSitter):        self.parser = code_confluence_tree_sitter.get_parser()        # Change to simple string key since we no longer track scope        self.variables_dict: Dict[str, ClassGlobalFieldModel] = {}    # Here the content refers to content of a file where it can contain multiple classes/procedural functions and variables    "},{"Name":"parse_global_variables","Parameters":[{"TypeValue":"content_of_file","TypeType":"str"}],"FunctionCalls":[{"NodeName":"self","FunctionName":"__traverse_global_variables","Position":{"StartLine":38,"StartLinePosition":12,"StopLine":38,"StopLinePosition":47}}],"Position":{"StartLine":31,"StartLinePosition":4,"StopLine":44,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":"{}"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"cursor","TypeType":"tree"}],"Content":"def parse_global_variables(self, content_of_file: str) -> List[ClassGlobalFieldModel]:        \"\"\"Parse variables from Python content.\"\"\"        tree = self.parser.parse(bytes(content_of_file, \"utf8\"))        self.variables_dict = {}        cursor = tree.walk()        # this is for class and global variables        self.__traverse_global_variables(cursor)        return list(self.variables_dict.values())    # It should parse class variables and instance variables defined in a class but outside of any functions based on content of class    # and then should use list of functions to check if there are any instance variables or class run variables defined in the functions and add them. Make sure there is no duplication.    "},{"Name":"parse_class_variables","Parameters":[{"TypeValue":"content_of_class_code","TypeType":"str"},{"TypeValue":"list_of_functions","TypeType":"List[ChapiFunction]"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"debug","Position":{"StartLine":84,"StartLinePosition":18,"StopLine":84,"StopLinePosition":81}}],"Position":{"StartLine":44,"StartLinePosition":4,"StopLine":88,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":"{}"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"cursor","TypeType":"tree"},{"TypeValue":"seen_variables","TypeType":"{v.class_field_nameforvinself.variables_dict.values()}"},{"TypeValue":"func_tree","TypeType":"self"},{"TypeValue":"func_cursor","TypeType":"func_tree"}],"Content":"def parse_class_variables(self, content_of_class_code: str, list_of_functions: List[ChapiFunction]) -> List[ClassGlobalFieldModel]:        \"\"\"Parse class-level and instance variables from class content.        First parses variables defined at class level (outside functions),        then processes function contents for instance/runtime variables.        Args:            content_of_class_code: Content of just the class definition            list_of_functions: List of functions to check for instance/runtime variables        Returns:            List[ClassGlobalFieldModel]: All unique class variables including:                - ClassVar variables                - Instance variables defined at class level                - Instance variables from methods (self.x)                - Runtime class variables from @classmethod (cls.x)        \"\"\"        # Phase 1: Parse class-level variables        tree = self.parser.parse(bytes(content_of_class_code, \"utf8\"))        self.variables_dict = {}  # Reset for new parsing        cursor = tree.walk()        logger.debug(\"Before class variables: {}\", len(self.variables_dict))        self.__traverse_class_variables(cursor)        logger.debug(\"After class variables: {}\", len(self.variables_dict))        # Track seen variables to avoid duplicates        seen_variables = {v.class_field_name for v in self.variables_dict.values()}        logger.debug(\"Seen variables: {}\", seen_variables)        # Phase 2: Parse variables from function contents        for function in list_of_functions:            if not function.content:                continue            logger.debug(\"Processing function: {}\", function.name)            logger.debug(\"Function content: {}\", function.content)            func_tree = self.parser.parse(bytes(function.content, \"utf8\"))            func_cursor = func_tree.walk()            self.__traverse_function_variables(cursor=func_cursor, seen_variables=seen_variables)            logger.debug(\"Variables after function: {}\", len(self.variables_dict))        return list(self.variables_dict.values())    "},{"Name":"__traverse_global_variables","Parameters":[{"TypeValue":"cursor","TypeType":"TreeCursor"}],"Position":{"StartLine":88,"StartLinePosition":4,"StopLine":135,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":"{}"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"cursor","TypeType":"tree"},{"TypeValue":"seen_variables","TypeType":"{v.class_field_nameforvinself.variables_dict.values()}"},{"TypeValue":"func_tree","TypeType":"self"},{"TypeValue":"func_cursor","TypeType":"func_tree"},{"TypeValue":"inside_function","TypeType":"False"},{"TypeValue":"inside_class","TypeType":"False"},{"TypeValue":"current_decorators","TypeType":"[]"},{"TypeValue":"processed_decorators","TypeType":""},{"TypeValue":"should_traverse_children","TypeType":"cursor"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"assignment_node","TypeType":"node"}],"Content":"def __traverse_global_variables(self, cursor: TreeCursor) -> None:        \"\"\"Traverse AST to find only global variables (outside any class/function).\"\"\"        inside_function = False        inside_class = False        current_decorators: List[ChapiAnnotation] = []        processed_decorators: set[tuple[int, int]] = set()        # Move cursor to first child if exists        should_traverse_children = cursor.goto_first_child()        while should_traverse_children:            node = cursor.node            # Track scope            if node.type == \"function_definition\":                inside_function = True            elif node.type == \"class_definition\":                inside_class = True            # Process global variables            if node.type == \"expression_statement\":                if not inside_function and not inside_class and node.children and node.children[0].type == \"assignment\":                    assignment_node = node.children[0]                    self.__process_assignment(assignment_node, current_decorators)                    current_decorators = []                    processed_decorators.clear()            # Try to move to next sibling            if cursor.goto_next_sibling():                # If we moved to sibling, check if we're exiting a scope                if inside_function and node.type == \"function_definition\":                    inside_function = False                elif inside_class and node.type == \"class_definition\":                    inside_class = False                continue            # No more siblings, go back to parent            if not cursor.goto_parent():                # We've reached the root, stop traversal                break            # Reset scope flags when exiting their definitions            if inside_function and node.type == \"function_definition\":                inside_function = False            elif inside_class and node.type == \"class_definition\":                inside_class = False    "},{"Name":"__traverse_class_variables","Parameters":[{"TypeValue":"cursor","TypeType":"TreeCursor"}],"Position":{"StartLine":135,"StartLinePosition":4,"StopLine":187,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":"{}"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"cursor","TypeType":"tree"},{"TypeValue":"seen_variables","TypeType":"{v.class_field_nameforvinself.variables_dict.values()}"},{"TypeValue":"func_tree","TypeType":"self"},{"TypeValue":"func_cursor","TypeType":"func_tree"},{"TypeValue":"inside_function","TypeType":"True"},{"TypeValue":"inside_class","TypeType":"False"},{"TypeValue":"current_decorators","TypeType":"[]"},{"TypeValue":"processed_decorators","TypeType":""},{"TypeValue":"should_traverse_children","TypeType":"cursor"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"assignment_node","TypeType":"node"},{"TypeValue":"annotation","TypeType":"self"}],"Content":"def __traverse_class_variables(self, cursor: TreeCursor) -> None:        \"\"\"Parse variables defined at class level (outside any function).\"\"\"        inside_function = False        current_decorators: List[ChapiAnnotation] = []        # Move down the tree until we find a class_definition node        while cursor.node.type != \"class_definition\":            if not cursor.goto_first_child():                return        # Enter class_definition internals        if not cursor.goto_first_child():            return        # Skip until we hit 'block' (class body)        while cursor.node.type != \"block\":            if not cursor.goto_next_sibling():                return        # Enter the block        if not cursor.goto_first_child():            return        while True:            # Update node at the start of each iteration            node = cursor.node            # If we were inside a function previously, check if we've left it            if inside_function and node.type != \"function_definition\":                # Moved on from function definition node, reset inside_function                inside_function = False            if node.type == \"function_definition\":                inside_function = True            elif node.type == \"decorator\" and not inside_function:                annotation = self.__get_annotation(node)                if annotation:                    current_decorators.append(annotation)            elif not inside_function and node.type == \"expression_statement\":                if node.children and node.children[0].type == \"assignment\":                    self.__process_assignment(node.children[0], current_decorators)                    current_decorators = []            # Move to next sibling if possible            if cursor.goto_next_sibling():                # Don't update node here, rely on next iteration's node = cursor.node                continue            break  # No more siblings    "},{"Name":"__traverse_function_variables","Parameters":[{"TypeValue":"cursor","TypeType":"TreeCursor"},{"TypeValue":"seen_variables","TypeType":"set[str]"}],"FunctionCalls":[{"NodeName":"self","FunctionName":"__process_assignment","Position":{"StartLine":237,"StartLinePosition":36,"StopLine":237,"StopLinePosition":73}}],"Position":{"StartLine":187,"StartLinePosition":4,"StopLine":243,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":"{}"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"cursor","TypeType":"tree"},{"TypeValue":"seen_variables","TypeType":"{v.class_field_nameforvinself.variables_dict.values()}"},{"TypeValue":"func_tree","TypeType":"self"},{"TypeValue":"func_cursor","TypeType":"func_tree"},{"TypeValue":"inside_function","TypeType":"True"},{"TypeValue":"inside_class","TypeType":"False"},{"TypeValue":"current_decorators","TypeType":"[]"},{"TypeValue":"processed_decorators","TypeType":""},{"TypeValue":"should_traverse_children","TypeType":"cursor"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"assignment_node","TypeType":"node"},{"TypeValue":"annotation","TypeType":"self"},{"TypeValue":"first_child","TypeType":"node"},{"TypeValue":"lhs","TypeType":"first_child"},{"TypeValue":"attr_text","TypeType":"lhs"},{"TypeValue":"var_name","TypeType":"attr_text"}],"Content":"def __traverse_function_variables(self, cursor: TreeCursor, seen_variables: set[str]) -> None:        \"\"\"Parse variables from function body.\"\"\"        # Step 1: Navigate to function_definition or decorated_definition        while cursor.node.type not in [\"function_definition\", \"decorated_definition\"]:            if not cursor.goto_first_child():                logger.error(\"Failed to find function/decorated definition\")                return        # If decorated, move to the function_definition within        if cursor.node.type == \"decorated_definition\":            if not cursor.goto_first_child():                return            while cursor.node.type != \"function_definition\":                if not cursor.goto_next_sibling():                    return        # Step 2: Move down into the function_definition's children        if not cursor.goto_first_child():            logger.error(\"Failed to enter function internals\")            return        # Step 3: Iterate siblings until we find the block node        # The block node is the actual function body.        while cursor.node.type != \"block\":            if not cursor.goto_next_sibling():                logger.error(\"Failed to find block\")                return        # Now cursor.node is 'block', enter it        if not cursor.goto_first_child():            logger.error(\"Failed to enter block contents\")            return        # Step 4: We are now inside the function body. Iterate over statements.        while True:            node = cursor.node            if node.type == \"expression_statement\" and node.children:                first_child = node.children[0]                if first_child.type == \"assignment\":                    lhs = first_child.children[0] if first_child.children else None                    if lhs and lhs.type == \"attribute\":                        attr_text = lhs.text.decode(\"utf8\")                        # Check for instance/class variable assignments (self.x or cls.x)                        if attr_text.startswith((\"self.\", \"cls.\")):                            var_name = attr_text.split(\".\", 1)[1]                            if var_name not in seen_variables:                                seen_variables.add(var_name)                                # Process the assignment as needed                                self.__process_assignment(first_child, [])                                # If needed, rename the variable in self.variables_dict here            if not cursor.goto_next_sibling():                break  # No more siblings in block    "},{"Name":"__get_annotation","Parameters":[{"TypeValue":"node","TypeType":"Node"}],"FunctionCalls":[{"NodeName":"self","FunctionName":"__extract_annotation_arguments","Position":{"StartLine":259,"StartLinePosition":37,"StopLine":259,"StopLinePosition":74}}],"Position":{"StartLine":243,"StartLinePosition":4,"StopLine":263,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":"{}"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"cursor","TypeType":"tree"},{"TypeValue":"seen_variables","TypeType":"{v.class_field_nameforvinself.variables_dict.values()}"},{"TypeValue":"func_tree","TypeType":"self"},{"TypeValue":"func_cursor","TypeType":"func_tree"},{"TypeValue":"inside_function","TypeType":"True"},{"TypeValue":"inside_class","TypeType":"False"},{"TypeValue":"current_decorators","TypeType":"[]"},{"TypeValue":"processed_decorators","TypeType":""},{"TypeValue":"should_traverse_children","TypeType":"cursor"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"assignment_node","TypeType":"node"},{"TypeValue":"annotation","TypeType":"self"},{"TypeValue":"first_child","TypeType":"node"},{"TypeValue":"lhs","TypeType":"first_child"},{"TypeValue":"attr_text","TypeType":"lhs"},{"TypeValue":"var_name","TypeType":"attr_text"},{"TypeValue":"func_name","TypeType":"call_child"},{"TypeValue":"key_values","TypeType":"self"}],"Content":"def __get_annotation(self, node: Node) -> Optional[ChapiAnnotation]:        \"\"\"Extract annotation from decorator node.\"\"\"        # Skip @ symbol        for child in node.children:            if child.type == \"identifier\":                # Simple decorator without arguments                return ChapiAnnotation(Name=child.text.decode(\"utf8\"))            elif child.type == \"call\":                # Decorator with arguments                func_name = None                for call_child in child.children:                    if call_child.type == \"identifier\":                        func_name = call_child.text.decode(\"utf8\")                        break                if func_name:                    key_values = self.__extract_annotation_arguments(child)                    return ChapiAnnotation(Name=func_name, KeyValues=key_values if key_values else None)        return None    "},{"Name":"__extract_annotation_arguments","Parameters":[{"TypeValue":"call_node","TypeType":"Node"}],"Position":{"StartLine":263,"StartLinePosition":4,"StopLine":298,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":"{}"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"cursor","TypeType":"tree"},{"TypeValue":"seen_variables","TypeType":"{v.class_field_nameforvinself.variables_dict.values()}"},{"TypeValue":"func_tree","TypeType":"self"},{"TypeValue":"func_cursor","TypeType":"func_tree"},{"TypeValue":"inside_function","TypeType":"True"},{"TypeValue":"inside_class","TypeType":"False"},{"TypeValue":"current_decorators","TypeType":"[]"},{"TypeValue":"processed_decorators","TypeType":""},{"TypeValue":"should_traverse_children","TypeType":"cursor"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"assignment_node","TypeType":"node"},{"TypeValue":"annotation","TypeType":"self"},{"TypeValue":"first_child","TypeType":"node"},{"TypeValue":"lhs","TypeType":"first_child"},{"TypeValue":"attr_text","TypeType":"lhs"},{"TypeValue":"var_name","TypeType":"attr_text"},{"TypeValue":"func_name","TypeType":"call_child"},{"TypeValue":"key_values","TypeType":""},{"TypeValue":"pos_arg_index","TypeType":""},{"TypeValue":"key","TypeType":"arg"},{"TypeValue":"value","TypeType":"arg"}],"Content":"def __extract_annotation_arguments(self, call_node: Node) -> List[ChapiAnnotationKeyVal]:        \"\"\"Extract arguments from a decorator call node.        Examples:            @decorator(1, x=2)           -> [(\"0\", \"1\"), (\"x\", \"2\")]            @decorator(\"str\", y=\"test\")  -> [(\"0\", \"\\\"str\\\"\"), (\"y\", \"\\\"test\\\"\")]            @decorator(key=\"value\")      -> [(\"key\", \"\\\"value\\\"\")]            @decorator(1, 2, 3)          -> [(\"0\", \"1\"), (\"1\", \"2\"), (\"2\", \"3\")]        \"\"\"        key_values: List[ChapiAnnotationKeyVal] = []        # Find argument_list node        for child in call_node.children:            if child.type == \"argument_list\":                pos_arg_index = 0                # Process each argument                for arg in child.children:                    if arg.type == \"keyword_argument\":                        # Handle key=value style arguments                        for i, kw_child in enumerate(arg.children):                            if kw_child.type == \"=\":                                # Get key (everything before =)                                key = arg.children[i - 1].text.decode(\"utf8\")                                # Get value (everything after =)                                value = arg.children[i + 1].text.decode(\"utf8\")                                key_values.append(ChapiAnnotationKeyVal(Key=key, Value=value))                                break                    elif arg.type not in [\"(\", \")\", \",\"]:                        # Handle positional arguments                        key_values.append(ChapiAnnotationKeyVal(Key=str(pos_arg_index), Value=arg.text.decode(\"utf8\")))                        pos_arg_index += 1        return key_values    "},{"Name":"__process_assignment","Parameters":[{"TypeValue":"node","TypeType":"Node"},{"TypeValue":"decorators","TypeType":"List[ChapiAnnotation]"}],"Position":{"StartLine":298,"StartLinePosition":4,"StopLine":382},"LocalVariables":[{"TypeValue":"self.parser","TypeType":"code_confluence_tree_sitter"},{"TypeValue":"self.variables_dict","TypeType":"{}"},{"TypeValue":"tree","TypeType":"self"},{"TypeValue":"cursor","TypeType":"tree"},{"TypeValue":"seen_variables","TypeType":"{v.class_field_nameforvinself.variables_dict.values()}"},{"TypeValue":"func_tree","TypeType":"self"},{"TypeValue":"func_cursor","TypeType":"func_tree"},{"TypeValue":"inside_function","TypeType":"True"},{"TypeValue":"inside_class","TypeType":"False"},{"TypeValue":"current_decorators","TypeType":"[]"},{"TypeValue":"processed_decorators","TypeType":""},{"TypeValue":"should_traverse_children","TypeType":"cursor"},{"TypeValue":"node","TypeType":"cursor"},{"TypeValue":"assignment_node","TypeType":"node"},{"TypeValue":"annotation","TypeType":"self"},{"TypeValue":"first_child","TypeType":"node"},{"TypeValue":"lhs","TypeType":"node"},{"TypeValue":"attr_text","TypeType":"lhs"},{"TypeValue":"var_name","TypeType":"attr_text"},{"TypeValue":"func_name","TypeType":"call_child"},{"TypeValue":"key_values","TypeType":""},{"TypeValue":"pos_arg_index","TypeType":""},{"TypeValue":"key","TypeType":"arg"},{"TypeValue":"value","TypeType":"values"},{"TypeValue":"var_names","TypeType":"[]"},{"TypeValue":"start_point","TypeType":"var_name_node"},{"TypeValue":"end_point","TypeType":"var_name_node"},{"TypeValue":"prefix","TypeType":"lhs"},{"TypeValue":"var_name_node","TypeType":"lhs"},{"TypeValue":"type_hint","TypeType":"type_node"},{"TypeValue":"values","TypeType":"[]"},{"TypeValue":"type_node","TypeType":"node"},{"TypeValue":"value_node","TypeType":"node"},{"TypeValue":"position","TypeType":"Position"},{"TypeValue":"var","TypeType":"ClassGlobalFieldModel"},{"TypeValue":"self.variables_dict[var_name]","TypeType":"var"}],"Content":"def __process_assignment(self, node: Node, decorators: List[ChapiAnnotation]) -> None:        \"\"\"Process an assignment node to extract variable information.        Only captures variables on their first occurrence.\"\"\"        if not node.children:            return        lhs = node.children[0]  # Left-hand side of assignment        # Get variable names based on node type        var_names = []        if lhs.type == \"identifier\":            var_names.append(lhs.text.decode(\"utf8\"))            start_point = lhs.start_point            end_point = lhs.end_point        elif lhs.type == \"pattern_list\":            # Handle tuple unpacking (x, y = ...)            for pattern_child in lhs.children:                if pattern_child.type == \"identifier\":                    var_names.append(pattern_child.text.decode(\"utf8\"))                    # For tuple unpacking, use each identifier's position                    start_point = pattern_child.start_point                    end_point = pattern_child.end_point        elif lhs.type == \"attribute\":            # Handle self.x or cls.x attributes            if len(lhs.children) >= 3:  # Need at least 3 parts: self/cls, ., var_name                prefix = lhs.children[0].text.decode(\"utf8\")                if prefix in (\"self\", \"cls\"):                    var_name_node = lhs.children[2]                    var_names.append(var_name_node.text.decode(\"utf8\"))                    # For attributes, use the variable part's position                    start_point = var_name_node.start_point                    end_point = var_name_node.end_point        if not var_names:            return        # Get type hint if present        type_hint = None        values = []        # Look for type hint and values        for i, child in enumerate(node.children):            if child.type == \":\" and i + 1 < len(node.children):                type_node = node.children[i + 1]                type_hint = type_node.text.decode(\"utf8\")            elif child.type == \"=\" and i + 1 < len(node.children):                value_node = node.children[i + 1]                if value_node.type == \"expression_list\":                    # Handle tuple values (1, 2)                    for value_child in value_node.children:                        if value_child.type not in [\",\", \"(\", \")\"]:                            values.append(value_child.text.decode(\"utf8\").strip())                else:                    # Single value - normalize dictionary/object literals                    value = value_node.text.decode(\"utf8\")                    if value_node.type == \"dictionary\":                        # Remove newlines and extra whitespace                        value = \"\".join(value.split())                    values.append(value)        # Create variables for each name-value pair, but only if not already captured        for i, var_name in enumerate(var_names):            # Skip if variable already exists            if var_name in self.variables_dict:                continue            value = values[i] if i < len(values) else None            # Create position object            position = Position(                StartLine=start_point[0],  # Convert to 1-based line numbers                StartLinePosition=start_point[1],                StopLine=end_point[0],                StopLinePosition=end_point[1],            )            var = ClassGlobalFieldModel(                TypeValue=var_name,                TypeType=type_hint,                DefaultValue=value,                Annotations=decorators if decorators else None,                Position=position,  # Add position information            )            self.variables_dict[var_name] = var"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_annotation","UsageName":["ChapiAnnotation","ChapiAnnotationKeyVal"]},{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_class_global_fieldmodel","UsageName":["ClassGlobalFieldModel"]},{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_function","UsageName":["ChapiFunction"]},{"Source":"src.code_confluence_flow_bridge.models.chapi.chapi_position","UsageName":["Position"]},{"Source":"src.code_confluence_flow_bridge.parser.tree_sitter.code_confluence_tree_sitter","UsageName":["CodeConfluenceTreeSitter"]},{"Source":"typing","UsageName":["Dict","List","Optional"]},{"Source":"loguru","UsageName":["logger"]},{"Source":"tree_sitter","UsageName":["Node","TreeCursor"]}],"Position":{"StartLine":24,"StopLine":382},"Content":"class NodeVariablesParser:    def __init__(self, code_confluence_tree_sitter: CodeConfluenceTreeSitter):        self.parser = code_confluence_tree_sitter.get_parser()        # Change to simple string key since we no longer track scope        self.variables_dict: Dict[str, ClassGlobalFieldModel] = {}    # Here the content refers to content of a file where it can contain multiple classes/procedural functions and variables    def parse_global_variables(self, content_of_file: str) -> List[ClassGlobalFieldModel]:        \"\"\"Parse variables from Python content.\"\"\"        tree = self.parser.parse(bytes(content_of_file, \"utf8\"))        self.variables_dict = {}        cursor = tree.walk()        # this is for class and global variables        self.__traverse_global_variables(cursor)        return list(self.variables_dict.values())    # It should parse class variables and instance variables defined in a class but outside of any functions based on content of class    # and then should use list of functions to check if there are any instance variables or class run variables defined in the functions and add them. Make sure there is no duplication.    def parse_class_variables(self, content_of_class_code: str, list_of_functions: List[ChapiFunction]) -> List[ClassGlobalFieldModel]:        \"\"\"Parse class-level and instance variables from class content.        First parses variables defined at class level (outside functions),        then processes function contents for instance/runtime variables.        Args:            content_of_class_code: Content of just the class definition            list_of_functions: List of functions to check for instance/runtime variables        Returns:            List[ClassGlobalFieldModel]: All unique class variables including:                - ClassVar variables                - Instance variables defined at class level                - Instance variables from methods (self.x)                - Runtime class variables from @classmethod (cls.x)        \"\"\"        # Phase 1: Parse class-level variables        tree = self.parser.parse(bytes(content_of_class_code, \"utf8\"))        self.variables_dict = {}  # Reset for new parsing        cursor = tree.walk()        logger.debug(\"Before class variables: {}\", len(self.variables_dict))        self.__traverse_class_variables(cursor)        logger.debug(\"After class variables: {}\", len(self.variables_dict))        # Track seen variables to avoid duplicates        seen_variables = {v.class_field_name for v in self.variables_dict.values()}        logger.debug(\"Seen variables: {}\", seen_variables)        # Phase 2: Parse variables from function contents        for function in list_of_functions:            if not function.content:                continue            logger.debug(\"Processing function: {}\", function.name)            logger.debug(\"Function content: {}\", function.content)            func_tree = self.parser.parse(bytes(function.content, \"utf8\"))            func_cursor = func_tree.walk()            self.__traverse_function_variables(cursor=func_cursor, seen_variables=seen_variables)            logger.debug(\"Variables after function: {}\", len(self.variables_dict))        return list(self.variables_dict.values())    def __traverse_global_variables(self, cursor: TreeCursor) -> None:        \"\"\"Traverse AST to find only global variables (outside any class/function).\"\"\"        inside_function = False        inside_class = False        current_decorators: List[ChapiAnnotation] = []        processed_decorators: set[tuple[int, int]] = set()        # Move cursor to first child if exists        should_traverse_children = cursor.goto_first_child()        while should_traverse_children:            node = cursor.node            # Track scope            if node.type == \"function_definition\":                inside_function = True            elif node.type == \"class_definition\":                inside_class = True            # Process global variables            if node.type == \"expression_statement\":                if not inside_function and not inside_class and node.children and node.children[0].type == \"assignment\":                    assignment_node = node.children[0]                    self.__process_assignment(assignment_node, current_decorators)                    current_decorators = []                    processed_decorators.clear()            # Try to move to next sibling            if cursor.goto_next_sibling():                # If we moved to sibling, check if we're exiting a scope                if inside_function and node.type == \"function_definition\":                    inside_function = False                elif inside_class and node.type == \"class_definition\":                    inside_class = False                continue            # No more siblings, go back to parent            if not cursor.goto_parent():                # We've reached the root, stop traversal                break            # Reset scope flags when exiting their definitions            if inside_function and node.type == \"function_definition\":                inside_function = False            elif inside_class and node.type == \"class_definition\":                inside_class = False    def __traverse_class_variables(self, cursor: TreeCursor) -> None:        \"\"\"Parse variables defined at class level (outside any function).\"\"\"        inside_function = False        current_decorators: List[ChapiAnnotation] = []        # Move down the tree until we find a class_definition node        while cursor.node.type != \"class_definition\":            if not cursor.goto_first_child():                return        # Enter class_definition internals        if not cursor.goto_first_child():            return        # Skip until we hit 'block' (class body)        while cursor.node.type != \"block\":            if not cursor.goto_next_sibling():                return        # Enter the block        if not cursor.goto_first_child():            return        while True:            # Update node at the start of each iteration            node = cursor.node            # If we were inside a function previously, check if we've left it            if inside_function and node.type != \"function_definition\":                # Moved on from function definition node, reset inside_function                inside_function = False            if node.type == \"function_definition\":                inside_function = True            elif node.type == \"decorator\" and not inside_function:                annotation = self.__get_annotation(node)                if annotation:                    current_decorators.append(annotation)            elif not inside_function and node.type == \"expression_statement\":                if node.children and node.children[0].type == \"assignment\":                    self.__process_assignment(node.children[0], current_decorators)                    current_decorators = []            # Move to next sibling if possible            if cursor.goto_next_sibling():                # Don't update node here, rely on next iteration's node = cursor.node                continue            break  # No more siblings    def __traverse_function_variables(self, cursor: TreeCursor, seen_variables: set[str]) -> None:        \"\"\"Parse variables from function body.\"\"\"        # Step 1: Navigate to function_definition or decorated_definition        while cursor.node.type not in [\"function_definition\", \"decorated_definition\"]:            if not cursor.goto_first_child():                logger.error(\"Failed to find function/decorated definition\")                return        # If decorated, move to the function_definition within        if cursor.node.type == \"decorated_definition\":            if not cursor.goto_first_child():                return            while cursor.node.type != \"function_definition\":                if not cursor.goto_next_sibling():                    return        # Step 2: Move down into the function_definition's children        if not cursor.goto_first_child():            logger.error(\"Failed to enter function internals\")            return        # Step 3: Iterate siblings until we find the block node        # The block node is the actual function body.        while cursor.node.type != \"block\":            if not cursor.goto_next_sibling():                logger.error(\"Failed to find block\")                return        # Now cursor.node is 'block', enter it        if not cursor.goto_first_child():            logger.error(\"Failed to enter block contents\")            return        # Step 4: We are now inside the function body. Iterate over statements.        while True:            node = cursor.node            if node.type == \"expression_statement\" and node.children:                first_child = node.children[0]                if first_child.type == \"assignment\":                    lhs = first_child.children[0] if first_child.children else None                    if lhs and lhs.type == \"attribute\":                        attr_text = lhs.text.decode(\"utf8\")                        # Check for instance/class variable assignments (self.x or cls.x)                        if attr_text.startswith((\"self.\", \"cls.\")):                            var_name = attr_text.split(\".\", 1)[1]                            if var_name not in seen_variables:                                seen_variables.add(var_name)                                # Process the assignment as needed                                self.__process_assignment(first_child, [])                                # If needed, rename the variable in self.variables_dict here            if not cursor.goto_next_sibling():                break  # No more siblings in block    def __get_annotation(self, node: Node) -> Optional[ChapiAnnotation]:        \"\"\"Extract annotation from decorator node.\"\"\"        # Skip @ symbol        for child in node.children:            if child.type == \"identifier\":                # Simple decorator without arguments                return ChapiAnnotation(Name=child.text.decode(\"utf8\"))            elif child.type == \"call\":                # Decorator with arguments                func_name = None                for call_child in child.children:                    if call_child.type == \"identifier\":                        func_name = call_child.text.decode(\"utf8\")                        break                if func_name:                    key_values = self.__extract_annotation_arguments(child)                    return ChapiAnnotation(Name=func_name, KeyValues=key_values if key_values else None)        return None    def __extract_annotation_arguments(self, call_node: Node) -> List[ChapiAnnotationKeyVal]:        \"\"\"Extract arguments from a decorator call node.        Examples:            @decorator(1, x=2)           -> [(\"0\", \"1\"), (\"x\", \"2\")]            @decorator(\"str\", y=\"test\")  -> [(\"0\", \"\\\"str\\\"\"), (\"y\", \"\\\"test\\\"\")]            @decorator(key=\"value\")      -> [(\"key\", \"\\\"value\\\"\")]            @decorator(1, 2, 3)          -> [(\"0\", \"1\"), (\"1\", \"2\"), (\"2\", \"3\")]        \"\"\"        key_values: List[ChapiAnnotationKeyVal] = []        # Find argument_list node        for child in call_node.children:            if child.type == \"argument_list\":                pos_arg_index = 0                # Process each argument                for arg in child.children:                    if arg.type == \"keyword_argument\":                        # Handle key=value style arguments                        for i, kw_child in enumerate(arg.children):                            if kw_child.type == \"=\":                                # Get key (everything before =)                                key = arg.children[i - 1].text.decode(\"utf8\")                                # Get value (everything after =)                                value = arg.children[i + 1].text.decode(\"utf8\")                                key_values.append(ChapiAnnotationKeyVal(Key=key, Value=value))                                break                    elif arg.type not in [\"(\", \")\", \",\"]:                        # Handle positional arguments                        key_values.append(ChapiAnnotationKeyVal(Key=str(pos_arg_index), Value=arg.text.decode(\"utf8\")))                        pos_arg_index += 1        return key_values    def __process_assignment(self, node: Node, decorators: List[ChapiAnnotation]) -> None:        \"\"\"Process an assignment node to extract variable information.        Only captures variables on their first occurrence.\"\"\"        if not node.children:            return        lhs = node.children[0]  # Left-hand side of assignment        # Get variable names based on node type        var_names = []        if lhs.type == \"identifier\":            var_names.append(lhs.text.decode(\"utf8\"))            start_point = lhs.start_point            end_point = lhs.end_point        elif lhs.type == \"pattern_list\":            # Handle tuple unpacking (x, y = ...)            for pattern_child in lhs.children:                if pattern_child.type == \"identifier\":                    var_names.append(pattern_child.text.decode(\"utf8\"))                    # For tuple unpacking, use each identifier's position                    start_point = pattern_child.start_point                    end_point = pattern_child.end_point        elif lhs.type == \"attribute\":            # Handle self.x or cls.x attributes            if len(lhs.children) >= 3:  # Need at least 3 parts: self/cls, ., var_name                prefix = lhs.children[0].text.decode(\"utf8\")                if prefix in (\"self\", \"cls\"):                    var_name_node = lhs.children[2]                    var_names.append(var_name_node.text.decode(\"utf8\"))                    # For attributes, use the variable part's position                    start_point = var_name_node.start_point                    end_point = var_name_node.end_point        if not var_names:            return        # Get type hint if present        type_hint = None        values = []        # Look for type hint and values        for i, child in enumerate(node.children):            if child.type == \":\" and i + 1 < len(node.children):                type_node = node.children[i + 1]                type_hint = type_node.text.decode(\"utf8\")            elif child.type == \"=\" and i + 1 < len(node.children):                value_node = node.children[i + 1]                if value_node.type == \"expression_list\":                    # Handle tuple values (1, 2)                    for value_child in value_node.children:                        if value_child.type not in [\",\", \"(\", \")\"]:                            values.append(value_child.text.decode(\"utf8\").strip())                else:                    # Single value - normalize dictionary/object literals                    value = value_node.text.decode(\"utf8\")                    if value_node.type == \"dictionary\":                        # Remove newlines and extra whitespace                        value = \"\".join(value.split())                    values.append(value)        # Create variables for each name-value pair, but only if not already captured        for i, var_name in enumerate(var_names):            # Skip if variable already exists            if var_name in self.variables_dict:                continue            value = values[i] if i < len(values) else None            # Create position object            position = Position(                StartLine=start_point[0],  # Convert to 1-based line numbers                StartLinePosition=start_point[1],                StopLine=end_point[0],                StopLinePosition=end_point[1],            )            var = ClassGlobalFieldModel(                TypeValue=var_name,                TypeType=type_hint,                DefaultValue=value,                Annotations=decorators if decorators else None,                Position=position,  # Add position information            )            self.variables_dict[var_name] = var"},{"NodeName":"PythonQualifiedNameStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/python/python_qualified_name_strategy.py","Functions":[{"Name":"get_qualified_name","Parameters":[{"TypeValue":"node_name","TypeType":"str"},{"TypeValue":"node_file_path","TypeType":"str"},{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"node_type","TypeType":"str"}],"FunctionCalls":[{"NodeName":"\".\"","FunctionName":"join","Position":{"StartLine":33,"StartLinePosition":28,"StopLine":33,"StopLinePosition":44}}],"Position":{"StartLine":6,"StartLinePosition":4,"StopLine":36},"LocalVariables":[{"TypeValue":"relative_path","TypeType":"os"},{"TypeValue":"path_without_ext","TypeType":"os"},{"TypeValue":"path_parts","TypeType":"path_without_ext"},{"TypeValue":"qualified_name","TypeType":"\".\""}],"Content":"def get_qualified_name(self, node_name: str, node_file_path: str, local_workspace_path: str, node_type: str) -> str:        \"\"\"Get qualified name for Python nodes.        For Python, the qualified name follows the format:        package.subpackage.classname        where classname is the actual node_name rather than the filename        Args:            node_name: The actual name of the class/node            node_file_path: Path to the node's file            local_workspace_path: Base workspace path        Returns:            str: Qualified name in format package.subpackage.node_name        \"\"\"        # Get relative path and remove .py extension        relative_path = os.path.relpath(node_file_path, local_workspace_path)        path_without_ext = os.path.splitext(relative_path)[0]        # Split the path into parts        path_parts = path_without_ext.replace(\"/\", \".\").replace(\"\\\\\", \".\").split(\".\")        # Replace the last part (filename) with the actual node name        if node_type == \"CLASS\":            path_parts.append(node_name)        # Join the path parts without adding codebase_name since it's already in the path        qualified_name = \".\".join(path_parts)        return qualified_name"}],"Imports":[{"Source":"os"}],"Position":{"StartLine":5,"StopLine":36},"Content":"class PythonQualifiedNameStrategy:    def get_qualified_name(self, node_name: str, node_file_path: str, local_workspace_path: str, node_type: str) -> str:        \"\"\"Get qualified name for Python nodes.        For Python, the qualified name follows the format:        package.subpackage.classname        where classname is the actual node_name rather than the filename        Args:            node_name: The actual name of the class/node            node_file_path: Path to the node's file            local_workspace_path: Base workspace path        Returns:            str: Qualified name in format package.subpackage.node_name        \"\"\"        # Get relative path and remove .py extension        relative_path = os.path.relpath(node_file_path, local_workspace_path)        path_without_ext = os.path.splitext(relative_path)[0]        # Split the path into parts        path_parts = path_without_ext.replace(\"/\", \".\").replace(\"\\\\\", \".\").split(\".\")        # Replace the last part (filename) with the actual node name        if node_type == \"CLASS\":            path_parts.append(node_name)        # Join the path parts without adding codebase_name since it's already in the path        qualified_name = \".\".join(path_parts)        return qualified_name"},{"NodeName":"SortFunctionDependencies","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/python/in_class_dependency/sort_function_dependencies.py","Functions":[{"Name":"sort_function_dependencies","Parameters":[{"TypeValue":"functions","TypeType":"List[UnoplatChapiForgeFunction]"},{"TypeValue":"node_type","TypeType":"str"}],"Position":{"StartLine":12,"StartLinePosition":4,"StopLine":18,"StopLinePosition":4},"Content":"def sort_function_dependencies(self, functions: List[UnoplatChapiForgeFunction], node_type: str) -> List[UnoplatChapiForgeFunction]:        if node_type and node_type == \"CLASS\":            return self.__sort_function_dependencies_for_class(functions=functions)        else:            return self.__sort_function_dependencies_for_procedural(functions=functions)    "},{"Name":"__build_dependency_graph_for_class","Parameters":[{"TypeValue":"functions","TypeType":"List[UnoplatChapiForgeFunction]"}],"FunctionCalls":[{"FunctionName":"in_degree","Position":{"StartLine":44,"StartLinePosition":29,"StopLine":44,"StopLinePosition":39}}],"Position":{"StartLine":18,"StartLinePosition":4,"StopLine":49,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"graph","TypeType":""},{"TypeValue":"in_degree","TypeType":""},{"TypeValue":"function_name_map","TypeType":"{func.name:funcforfuncinfunctionsiffunc.nameisnotNone}"},{"TypeValue":"in_degree[func.name]","TypeType":""}],"Content":"def __build_dependency_graph_for_class(self, functions: List[UnoplatChapiForgeFunction]) -> tuple[Dict[str, Set[str]], Dict[str, int]]:        # Create adjacency list and in-degree count        graph: Dict[str, Set[str]] = defaultdict(set)        in_degree: Dict[str, int] = defaultdict(int)        function_name_map = {func.name: func for func in functions if func.name is not None}        # Initialize in-degree for all functions        for func in functions:            if func.name is not None:                in_degree[func.name] = 0        # Build the graph        for func in functions:            if not func.function_calls or func.name is None:                continue            for call in func.function_calls:                # Skip self-dependencies                if call.function_name == func.name:                    continue  # Skip adding an edge from a function to itself                # Check if it's an internal class method call                if call.function_name is not None and (call.node_name in (\"self\", \"cls\") or not call.node_name) and call.function_name in function_name_map:                    # Add edge from called function to calling function                    # This means: called_function must be processed before calling_function                    graph[call.function_name].add(func.name)                    in_degree[func.name] += 1        # Convert defaultdict to regular dict before returning        return dict(graph), dict(in_degree)    "},{"Name":"__sort_function_dependencies_for_class","Parameters":[{"TypeValue":"functions","TypeType":"List[UnoplatChapiForgeFunction]"}],"FunctionCalls":[{"NodeName":"queue","FunctionName":"append","Position":{"StartLine":75,"StartLinePosition":25,"StopLine":75,"StopLinePosition":41}}],"Position":{"StartLine":49,"StartLinePosition":4,"StopLine":85,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"graph","TypeType":""},{"TypeValue":"in_degree","TypeType":""},{"TypeValue":"function_name_map","TypeType":"{func.name:funcforfuncinfunctionsiffunc.nameisnotNone}"},{"TypeValue":"in_degree[func.name]","TypeType":""},{"TypeValue":"","TypeType":"self"},{"TypeValue":"queue","TypeType":""},{"TypeValue":"sorted_names","TypeType":""},{"TypeValue":"function_map","TypeType":"{func.name:funcforfuncinfunctionsiffunc.nameisnotNone}"},{"TypeValue":"current","TypeType":"queue"},{"TypeValue":"in_degree[neighbor]","TypeType":""}],"Content":"def __sort_function_dependencies_for_class(self, functions: List[UnoplatChapiForgeFunction]) -> List[UnoplatChapiForgeFunction]:        if not functions:            return []        # Build dependency graph        graph, in_degree = self.__build_dependency_graph_for_class(functions)        # Initialize queue with functions that have no incoming edges (in-degree == 0)        queue: Deque[str] = deque()        for func_name, degree in in_degree.items():            if degree == 0:                queue.append(func_name)        # Store function names in sorted order        sorted_names: List[str] = []        function_map = {func.name: func for func in functions if func.name is not None}        # Process queue (Kahn's algorithm)        while queue:            current = queue.popleft()            sorted_names.append(current)            # Reduce in-degree for all functions that the current function depends on            for neighbor in graph.get(current, []):                in_degree[neighbor] -= 1                if in_degree[neighbor] == 0:                    queue.append(neighbor)        # Check for cycles        if len(sorted_names) != len(function_map):            # If there's a cycle, return original list as fallback            return functions        # Convert sorted names back to functions        return [function_map[name] for name in sorted_names]    "},{"Name":"__build_dependency_graph_for_procedural","Parameters":[{"TypeValue":"functions","TypeType":"List[UnoplatChapiForgeFunction]"}],"FunctionCalls":[{"FunctionName":"in_degree","Position":{"StartLine":110,"StartLinePosition":29,"StopLine":110,"StopLinePosition":39}}],"Position":{"StartLine":85,"StartLinePosition":4,"StopLine":115,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"graph","TypeType":""},{"TypeValue":"in_degree","TypeType":""},{"TypeValue":"function_name_map","TypeType":"{func.name:funcforfuncinfunctionsiffunc.nameisnotNone}"},{"TypeValue":"in_degree[func.name]","TypeType":""},{"TypeValue":"","TypeType":"self"},{"TypeValue":"queue","TypeType":""},{"TypeValue":"sorted_names","TypeType":""},{"TypeValue":"function_map","TypeType":"{func.name:funcforfuncinfunctionsiffunc.nameisnotNone}"},{"TypeValue":"current","TypeType":"queue"},{"TypeValue":"in_degree[neighbor]","TypeType":""}],"Content":"def __build_dependency_graph_for_procedural(self, functions: List[UnoplatChapiForgeFunction]) -> tuple[Dict[str, Set[str]], Dict[str, int]]:        # Create adjacency list and in-degree count        graph: Dict[str, Set[str]] = defaultdict(set)        in_degree: Dict[str, int] = defaultdict(int)        function_name_map = {func.name: func for func in functions if func.name is not None}        # Initialize in_degree for all functions        for func in functions:            if func.name is not None:                in_degree[func.name] = 0        # Build the graph        for func in functions:            if not func.function_calls or func.name is None:                continue            for call in func.function_calls:                # Skip self-dependencies                if call.function_name == func.name:                    continue  # Skip adding an edge from a function to itself                # Check if it's a call to another procedural function                if call.function_name is not None and call.function_name in function_name_map:                    # Add edge from current function to called function                    graph[call.function_name].add(func.name)                    in_degree[func.name] += 1        # Convert defaultdict to regular dict before returning        return dict(graph), dict(in_degree)    "},{"Name":"__sort_function_dependencies_for_procedural","Parameters":[{"TypeValue":"functions","TypeType":"List[UnoplatChapiForgeFunction]"}],"FunctionCalls":[{"NodeName":"queue","FunctionName":"append","Position":{"StartLine":141,"StartLinePosition":25,"StopLine":141,"StopLinePosition":41}}],"Position":{"StartLine":115,"StartLinePosition":4,"StopLine":150},"LocalVariables":[{"TypeValue":"graph","TypeType":""},{"TypeValue":"in_degree","TypeType":""},{"TypeValue":"function_name_map","TypeType":"{func.name:funcforfuncinfunctionsiffunc.nameisnotNone}"},{"TypeValue":"in_degree[func.name]","TypeType":""},{"TypeValue":"","TypeType":"self"},{"TypeValue":"queue","TypeType":""},{"TypeValue":"sorted_names","TypeType":""},{"TypeValue":"function_map","TypeType":"{func.name:funcforfuncinfunctionsiffunc.nameisnotNone}"},{"TypeValue":"current","TypeType":"queue"},{"TypeValue":"in_degree[neighbor]","TypeType":""}],"Content":"def __sort_function_dependencies_for_procedural(self, functions: List[UnoplatChapiForgeFunction]) -> List[UnoplatChapiForgeFunction]:        if not functions:            return []        # Build dependency graph        graph, in_degree = self.__build_dependency_graph_for_procedural(functions)        # Initialize queue with functions that have no incoming edges (in-degree == 0)        queue: Deque[str] = deque()        for func_name, degree in in_degree.items():            if degree == 0:                queue.append(func_name)        # Store function names in sorted order        sorted_names: List[str] = []        function_map = {func.name: func for func in functions if func.name is not None}        # Process queue (Kahn's algorithm)        while queue:            current = queue.popleft()            sorted_names.append(current)            # Reduce in-degree for all functions that the current function depends on            for neighbor in graph.get(current, []):                in_degree[neighbor] -= 1                if in_degree[neighbor] == 0:                    queue.append(neighbor)        # Check for cycles        if len(sorted_names) != len(function_map):            # If there's a cycle, return original list as fallback            return functions        # Convert sorted names back to functions        return [function_map[name] for name in sorted_names]"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_chapi_forge_function","UsageName":["UnoplatChapiForgeFunction"]},{"Source":"collections","UsageName":["defaultdict","deque"]},{"Source":"typing","UsageName":["Deque","Dict","List","Set"]}],"Position":{"StartLine":11,"StopLine":150},"Content":"class SortFunctionDependencies:    def sort_function_dependencies(self, functions: List[UnoplatChapiForgeFunction], node_type: str) -> List[UnoplatChapiForgeFunction]:        if node_type and node_type == \"CLASS\":            return self.__sort_function_dependencies_for_class(functions=functions)        else:            return self.__sort_function_dependencies_for_procedural(functions=functions)    def __build_dependency_graph_for_class(self, functions: List[UnoplatChapiForgeFunction]) -> tuple[Dict[str, Set[str]], Dict[str, int]]:        # Create adjacency list and in-degree count        graph: Dict[str, Set[str]] = defaultdict(set)        in_degree: Dict[str, int] = defaultdict(int)        function_name_map = {func.name: func for func in functions if func.name is not None}        # Initialize in-degree for all functions        for func in functions:            if func.name is not None:                in_degree[func.name] = 0        # Build the graph        for func in functions:            if not func.function_calls or func.name is None:                continue            for call in func.function_calls:                # Skip self-dependencies                if call.function_name == func.name:                    continue  # Skip adding an edge from a function to itself                # Check if it's an internal class method call                if call.function_name is not None and (call.node_name in (\"self\", \"cls\") or not call.node_name) and call.function_name in function_name_map:                    # Add edge from called function to calling function                    # This means: called_function must be processed before calling_function                    graph[call.function_name].add(func.name)                    in_degree[func.name] += 1        # Convert defaultdict to regular dict before returning        return dict(graph), dict(in_degree)    def __sort_function_dependencies_for_class(self, functions: List[UnoplatChapiForgeFunction]) -> List[UnoplatChapiForgeFunction]:        if not functions:            return []        # Build dependency graph        graph, in_degree = self.__build_dependency_graph_for_class(functions)        # Initialize queue with functions that have no incoming edges (in-degree == 0)        queue: Deque[str] = deque()        for func_name, degree in in_degree.items():            if degree == 0:                queue.append(func_name)        # Store function names in sorted order        sorted_names: List[str] = []        function_map = {func.name: func for func in functions if func.name is not None}        # Process queue (Kahn's algorithm)        while queue:            current = queue.popleft()            sorted_names.append(current)            # Reduce in-degree for all functions that the current function depends on            for neighbor in graph.get(current, []):                in_degree[neighbor] -= 1                if in_degree[neighbor] == 0:                    queue.append(neighbor)        # Check for cycles        if len(sorted_names) != len(function_map):            # If there's a cycle, return original list as fallback            return functions        # Convert sorted names back to functions        return [function_map[name] for name in sorted_names]    def __build_dependency_graph_for_procedural(self, functions: List[UnoplatChapiForgeFunction]) -> tuple[Dict[str, Set[str]], Dict[str, int]]:        # Create adjacency list and in-degree count        graph: Dict[str, Set[str]] = defaultdict(set)        in_degree: Dict[str, int] = defaultdict(int)        function_name_map = {func.name: func for func in functions if func.name is not None}        # Initialize in_degree for all functions        for func in functions:            if func.name is not None:                in_degree[func.name] = 0        # Build the graph        for func in functions:            if not func.function_calls or func.name is None:                continue            for call in func.function_calls:                # Skip self-dependencies                if call.function_name == func.name:                    continue  # Skip adding an edge from a function to itself                # Check if it's a call to another procedural function                if call.function_name is not None and call.function_name in function_name_map:                    # Add edge from current function to called function                    graph[call.function_name].add(func.name)                    in_degree[func.name] += 1        # Convert defaultdict to regular dict before returning        return dict(graph), dict(in_degree)    def __sort_function_dependencies_for_procedural(self, functions: List[UnoplatChapiForgeFunction]) -> List[UnoplatChapiForgeFunction]:        if not functions:            return []        # Build dependency graph        graph, in_degree = self.__build_dependency_graph_for_procedural(functions)        # Initialize queue with functions that have no incoming edges (in-degree == 0)        queue: Deque[str] = deque()        for func_name, degree in in_degree.items():            if degree == 0:                queue.append(func_name)        # Store function names in sorted order        sorted_names: List[str] = []        function_map = {func.name: func for func in functions if func.name is not None}        # Process queue (Kahn's algorithm)        while queue:            current = queue.popleft()            sorted_names.append(current)            # Reduce in-degree for all functions that the current function depends on            for neighbor in graph.get(current, []):                in_degree[neighbor] -= 1                if in_degree[neighbor] == 0:                    queue.append(neighbor)        # Check for cycles        if len(sorted_names) != len(function_map):            # If there's a cycle, return original list as fallback            return functions        # Convert sorted names back to functions        return [function_map[name] for name in sorted_names]"},{"NodeName":"PythonNodeDependencyProcessor","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/python/python_node_dependency_processor.py","Functions":[{"Name":"process_dependencies","Parameters":[{"TypeValue":"node","TypeType":"UnoplatChapiForgeNode"},{"TypeValue":"qualified_dict","TypeType":"Dict[str,UnoplatChapiForgeNode]"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"opt","Position":{"StartLine":89,"StartLinePosition":18,"StopLine":89,"StopLinePosition":37}},{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":89,"StartLinePosition":38,"StopLine":97,"StopLinePosition":12}}],"Position":{"StartLine":18,"StartLinePosition":4,"StopLine":99},"LocalVariables":[{"TypeValue":"node.dependent_internal_classes","TypeType":"[]"},{"TypeValue":"procedural_nodes","TypeType":""},{"TypeValue":"internal_imports","TypeType":"node"},{"TypeValue":"qualified_name","TypeType":"f\"{imp.source}.{usage.original_name}\""},{"TypeValue":"procedural_dependent_node","TypeType":"qualified_dict"}],"Content":"def process_dependencies(self, node: UnoplatChapiForgeNode, qualified_dict: Dict[str, UnoplatChapiForgeNode]) -> List[str]:        \"\"\"Process dependencies for a given node.        We have internal imports with source and usage - original and alias both.        What we have to do is go through each import and identify class dependencies        based on import type. Classes use CamelCase naming convention.        For example:        from myproject.models import UserModel, BaseClass, helper_func        from myproject.utils import helper_function, HelperClass as helper        We identify:        - UserModel, BaseClass as class dependencies -> myproject.models.UserModel, myproject.models.BaseClass        - helper_func as function -> myproject.models        - HelperClass as class dependency -> myproject.utils.HelperClass        - helper_function as function -> myproject.utils        Args:            node: The ChapiUnoplatNode to process dependencies for        Note:            Modifies node.dependent_internal_classes in place:            - For classes: adds fully qualified name (source.original_name)            - For non-classes: adds only the source module path        \"\"\"        try:            logger.debug(\"Processing dependencies for node: {}\", node.node_name)            if not node.segregated_imports or ImportType.INTERNAL not in node.segregated_imports:                logger.debug(\"No internal imports found for node: {}\", node.node_name)                return []            node.dependent_internal_classes = []            procedural_nodes: Set[str] = set()            internal_imports = node.segregated_imports[ImportType.INTERNAL]            logger.debug(\"Found {} internal imports for node: {}\", len(internal_imports), node.node_name)            for imp in internal_imports:                if imp.usage_names:                    for usage in imp.usage_names:                        if usage.original_name:                            try:                                if IsClassName.is_python_class_name(usage.original_name):                                    # For classes, add the fully qualified name                                    qualified_name = f\"{imp.source}.{usage.original_name}\"                                    logger.debug(\"Found class dependency: {} -> {}\", node.node_name, qualified_name)                                    node.dependent_internal_classes.append(qualified_name)                                else:                                    # For non-classes, add only the source module path                                    try:                                        procedural_dependent_node = qualified_dict[imp.source]  # type: ignore                                    except KeyError:                                        logger.error(\"Missing source module in dictionary: {}\\n\" \"Available modules: {}\\n\" \"Current node: {}\", imp.source, list(qualified_dict.keys()), node.node_name)                                        continue                                    if procedural_dependent_node.node_name not in procedural_nodes:                                        logger.debug(\"Found procedural dependency: {} -> {}\", node.node_name, procedural_dependent_node.node_name)                                        node.dependent_internal_classes.append(imp.source)  # type: ignore                                        procedural_nodes.add(procedural_dependent_node.node_name)  # type: ignore                            except Exception as inner_e:                                logger.error(\"Error processing import usage: {}\\n\" \"Import source: {}\\n\" \"Error: {}\", usage.original_name, imp.source, str(inner_e))                                continue            logger.debug(\"Completed processing dependencies for node: {}\\n\" \"Found {} class dependencies and {} procedural dependencies\", node.node_name, len(node.dependent_internal_classes) - len(procedural_nodes), len(procedural_nodes))            return node.dependent_internal_classes        except Exception as e:            logger.opt(exception=True).error(                \"Error processing dependencies for node: {}\\n\" \"Node type: {}\\n\" \"Node package: {}\\n\" \"Internal imports: {}\\n\" \"Error: {}\\n\" \"Available qualified names: {}\",                node.node_name,                node.type,                node.package,                internal_imports if \"internal_imports\" in locals() else \"Not initialized\",                str(e),                list(qualified_dict.keys()),            )            return []"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_chapi_forge_node","UsageName":["UnoplatChapiForgeNode"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_import_type","UsageName":["ImportType"]},{"Source":"src.code_confluence_flow_bridge.utility.is_class_name","UsageName":["IsClassName"]},{"Source":"typing","UsageName":["Dict","List","Set"]},{"Source":"loguru","UsageName":["logger"]}],"Position":{"StartLine":17,"StopLine":99},"Content":"class PythonNodeDependencyProcessor:    def process_dependencies(self, node: UnoplatChapiForgeNode, qualified_dict: Dict[str, UnoplatChapiForgeNode]) -> List[str]:        \"\"\"Process dependencies for a given node.        We have internal imports with source and usage - original and alias both.        What we have to do is go through each import and identify class dependencies        based on import type. Classes use CamelCase naming convention.        For example:        from myproject.models import UserModel, BaseClass, helper_func        from myproject.utils import helper_function, HelperClass as helper        We identify:        - UserModel, BaseClass as class dependencies -> myproject.models.UserModel, myproject.models.BaseClass        - helper_func as function -> myproject.models        - HelperClass as class dependency -> myproject.utils.HelperClass        - helper_function as function -> myproject.utils        Args:            node: The ChapiUnoplatNode to process dependencies for        Note:            Modifies node.dependent_internal_classes in place:            - For classes: adds fully qualified name (source.original_name)            - For non-classes: adds only the source module path        \"\"\"        try:            logger.debug(\"Processing dependencies for node: {}\", node.node_name)            if not node.segregated_imports or ImportType.INTERNAL not in node.segregated_imports:                logger.debug(\"No internal imports found for node: {}\", node.node_name)                return []            node.dependent_internal_classes = []            procedural_nodes: Set[str] = set()            internal_imports = node.segregated_imports[ImportType.INTERNAL]            logger.debug(\"Found {} internal imports for node: {}\", len(internal_imports), node.node_name)            for imp in internal_imports:                if imp.usage_names:                    for usage in imp.usage_names:                        if usage.original_name:                            try:                                if IsClassName.is_python_class_name(usage.original_name):                                    # For classes, add the fully qualified name                                    qualified_name = f\"{imp.source}.{usage.original_name}\"                                    logger.debug(\"Found class dependency: {} -> {}\", node.node_name, qualified_name)                                    node.dependent_internal_classes.append(qualified_name)                                else:                                    # For non-classes, add only the source module path                                    try:                                        procedural_dependent_node = qualified_dict[imp.source]  # type: ignore                                    except KeyError:                                        logger.error(\"Missing source module in dictionary: {}\\n\" \"Available modules: {}\\n\" \"Current node: {}\", imp.source, list(qualified_dict.keys()), node.node_name)                                        continue                                    if procedural_dependent_node.node_name not in procedural_nodes:                                        logger.debug(\"Found procedural dependency: {} -> {}\", node.node_name, procedural_dependent_node.node_name)                                        node.dependent_internal_classes.append(imp.source)  # type: ignore                                        procedural_nodes.add(procedural_dependent_node.node_name)  # type: ignore                            except Exception as inner_e:                                logger.error(\"Error processing import usage: {}\\n\" \"Import source: {}\\n\" \"Error: {}\", usage.original_name, imp.source, str(inner_e))                                continue            logger.debug(\"Completed processing dependencies for node: {}\\n\" \"Found {} class dependencies and {} procedural dependencies\", node.node_name, len(node.dependent_internal_classes) - len(procedural_nodes), len(procedural_nodes))            return node.dependent_internal_classes        except Exception as e:            logger.opt(exception=True).error(                \"Error processing dependencies for node: {}\\n\" \"Node type: {}\\n\" \"Node package: {}\\n\" \"Internal imports: {}\\n\" \"Error: {}\\n\" \"Available qualified names: {}\",                node.node_name,                node.type,                node.package,                internal_imports if \"internal_imports\" in locals() else \"Not initialized\",                str(e),                list(qualified_dict.keys()),            )            return []"},{"NodeName":"UnsupportedLanguageError","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/codebase_parser_factory.py","MultipleExtend":["Exception"],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["ProgrammingLanguage"]},{"Source":"src.code_confluence_flow_bridge.parser.codebase_parser_strategy","UsageName":["CodebaseParserStrategy"]},{"Source":"src.code_confluence_flow_bridge.parser.python.python_codebase_parser","UsageName":["PythonCodebaseParser"]}],"Position":{"StartLine":13,"StopLine":17},"Content":"class UnsupportedLanguageError(Exception):    pass"},{"NodeName":"CodebaseParserFactory","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/codebase_parser_factory.py","Functions":[{"Name":"get_parser","Parameters":[{"TypeValue":"programming_language","TypeType":"str"}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":18,"StartLinePosition":4,"StopLine":19,"StopLinePosition":4}}],"Position":{"StartLine":19,"StartLinePosition":4,"StopLine":23},"Content":"def get_parser(programming_language: str) -> CodebaseParserStrategy:        if programming_language.lower() == ProgrammingLanguage.PYTHON.value:            return PythonCodebaseParser()        raise UnsupportedLanguageError(f\"No parser implementation for language: {programming_language}\")"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["ProgrammingLanguage"]},{"Source":"src.code_confluence_flow_bridge.parser.codebase_parser_strategy","UsageName":["CodebaseParserStrategy"]},{"Source":"src.code_confluence_flow_bridge.parser.python.python_codebase_parser","UsageName":["PythonCodebaseParser"]}],"Position":{"StartLine":17,"StopLine":23},"Content":"class CodebaseParserFactory:    @staticmethod    def get_parser(programming_language: str) -> CodebaseParserStrategy:        if programming_language.lower() == ProgrammingLanguage.PYTHON.value:            return PythonCodebaseParser()        raise UnsupportedLanguageError(f\"No parser implementation for language: {programming_language}\")"},{"NodeName":"PackageManagerStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/package_manager/package_manager_strategy.py","MultipleExtend":["ABC"],"Functions":[{"Name":"process_metadata","Parameters":[{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"metadata","TypeType":"ProgrammingLanguageMetadata"}],"Annotations":[{"Name":"abstractmethod","Position":{"StartLine":15,"StartLinePosition":4,"StopLine":16,"StopLinePosition":4}}],"Position":{"StartLine":16,"StartLinePosition":4,"StopLine":28},"Content":"def process_metadata(self, local_workspace_path: str, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"        Process package manager specific metadata        Args:            local_workspace_path: Path to the local workspace            metadata: Programming language metadata from config        Returns:            UnoplatPackageManagerMetadata: Processed package manager metadata        \"\"\"        pass"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["ProgrammingLanguageMetadata"]},{"Source":"abc","UsageName":["ABC","abstractmethod"]}],"Position":{"StartLine":14,"StopLine":28},"Content":"class PackageManagerStrategy(ABC):    @abstractmethod    def process_metadata(self, local_workspace_path: str, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"        Process package manager specific metadata        Args:            local_workspace_path: Path to the local workspace            metadata: Programming language metadata from config        Returns:            UnoplatPackageManagerMetadata: Processed package manager metadata        \"\"\"        pass"},{"NodeName":"SetupParser","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/package_manager/utils/setup_parser.py","Functions":[{"Name":"_extract_constant_value","Parameters":[{"TypeValue":"node","TypeType":"ast.AST"}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":28,"StartLinePosition":4,"StopLine":29,"StopLinePosition":4}}],"Position":{"StartLine":29,"StartLinePosition":4,"StopLine":35,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"SetupDictValue","TypeType":"Union"}],"Content":"def _extract_constant_value(node: ast.AST) -> Optional[str]:        \"\"\"Extract string value from an AST Constant node\"\"\"        if isinstance(node, ast.Constant) and isinstance(node.value, str):            return node.value        return None    "},{"Name":"_extract_list_values","Parameters":[{"TypeValue":"node","TypeType":"ast.List"}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":35,"StartLinePosition":4,"StopLine":36,"StopLinePosition":4}}],"Position":{"StartLine":36,"StartLinePosition":4,"StopLine":40,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"SetupDictValue","TypeType":"Union"}],"Content":"def _extract_list_values(node: ast.List) -> List[str]:        \"\"\"Extract string values from an AST List node\"\"\"        return [elt.value for elt in node.elts if isinstance(elt, ast.Constant) and isinstance(elt.value, str)]    "},{"Name":"_extract_dict_values","Parameters":[{"TypeValue":"node","TypeType":"ast.Dict"}],"FunctionCalls":[{"NodeName":"stack","FunctionName":"append","Position":{"StartLine":80,"StartLinePosition":25,"StopLine":80,"StopLinePosition":55}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":40,"StartLinePosition":4,"StopLine":41,"StopLinePosition":4}}],"Position":{"StartLine":41,"StartLinePosition":4,"StopLine":86,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"SetupDictValue","TypeType":"Union"},{"TypeValue":"result","TypeType":""},{"TypeValue":"stack","TypeType":"[(node,result)]"},{"TypeValue":"","TypeType":"stack"},{"TypeValue":"key_str","TypeType":"key_node"},{"TypeValue":"current_output_dict[key_str]","TypeType":"sub_dict"},{"TypeValue":"sub_dict","TypeType":""}],"Content":"def _extract_dict_values(node: ast.Dict) -> Dict[str, \"SetupParser.SetupDictValue\"]:        \"\"\"        Extract key-value pairs from an AST Dict node iteratively        (i.e., without recursion), handling:          - string constants          - lists of strings          - nested dictionaries        \"\"\"        # The dictionary for the top-level node        result: Dict[str, \"SetupParser.SetupDictValue\"] = {}        # Stack items are (dict_node, target_dict), where:        # - dict_node is the AST Dict to parse        # - target_dict is the Python dict we will fill at that level        stack = [(node, result)]        while stack:            current_ast_dict, current_output_dict = stack.pop()            for key_node, value_node in zip(current_ast_dict.keys, current_ast_dict.values):                # Only handle string keys                if not (isinstance(key_node, ast.Constant) and isinstance(key_node.value, str)):                    continue                key_str = key_node.value                if isinstance(value_node, ast.Constant) and isinstance(value_node.value, str):                    # A simple string                    current_output_dict[key_str] = value_node.value                elif isinstance(value_node, ast.List):                    # A list of strings                    current_output_dict[key_str] = SetupParser._extract_list_values(value_node)                elif isinstance(value_node, ast.Dict):                    # Nested dictionary: create a sub-dict in the current dictionary                    sub_dict: Dict[str, \"SetupParser.SetupDictValue\"] = {}                    current_output_dict[key_str] = sub_dict                    # Push it on the stack to parse                    stack.append((value_node, sub_dict))                # You can add more branches here for other node types if you need them        return result    "},{"Name":"_extract_setup_args_from_ast","Parameters":[{"TypeValue":"node","TypeType":"ast.AST"}],"FunctionCalls":[{"NodeName":"SetupParser","FunctionName":"_extract_dict_values","Position":{"StartLine":106,"StartLinePosition":52,"StopLine":106,"StopLinePosition":87}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":86,"StartLinePosition":4,"StopLine":87,"StopLinePosition":4}}],"Position":{"StartLine":87,"StartLinePosition":4,"StopLine":110,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"SetupDictValue","TypeType":"Union"},{"TypeValue":"result","TypeType":""},{"TypeValue":"stack","TypeType":"[(node,result)]"},{"TypeValue":"","TypeType":"stack"},{"TypeValue":"key_str","TypeType":"key_node"},{"TypeValue":"current_output_dict[key_str]","TypeType":"sub_dict"},{"TypeValue":"sub_dict","TypeType":""},{"TypeValue":"args_dict","TypeType":""},{"TypeValue":"value","TypeType":"SetupParser"},{"TypeValue":"args_dict[keyword.arg]","TypeType":"SetupParser"}],"Content":"def _extract_setup_args_from_ast(node: ast.AST) -> Optional[Dict[str, Any]]:        \"\"\"Extract setup() arguments from an AST node\"\"\"        if not (isinstance(node, ast.Call) and isinstance(node.func, ast.Name) and node.func.id == \"setup\"):            return None        args_dict: Dict[str, Any] = {}        for keyword in node.keywords:            if keyword.arg is None:  # Skip if no argument name                continue            if isinstance(keyword.value, ast.Constant):                value = SetupParser._extract_constant_value(keyword.value)                if value is not None:                    args_dict[keyword.arg] = value            elif isinstance(keyword.value, ast.List):                args_dict[keyword.arg] = SetupParser._extract_list_values(keyword.value)            elif isinstance(keyword.value, ast.Dict):                # Handle nested dictionary structures                args_dict[keyword.arg] = SetupParser._extract_dict_values(keyword.value)        return args_dict    "},{"Name":"_parse_entry_points","Parameters":[{"TypeValue":"entry_points","TypeType":"Union[Dict,str,List]"}],"FunctionCalls":[{"NodeName":"activity","FunctionName":"logger","Position":{"StartLine":146,"StartLinePosition":28,"StopLine":146,"StopLinePosition":29}},{"NodeName":"activity","FunctionName":"warning","Position":{"StartLine":146,"StartLinePosition":35,"StopLine":146,"StopLinePosition":90}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":110,"StartLinePosition":4,"StopLine":111,"StopLinePosition":4}}],"Position":{"StartLine":111,"StartLinePosition":4,"StopLine":150,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"SetupDictValue","TypeType":"Union"},{"TypeValue":"result","TypeType":"{}"},{"TypeValue":"stack","TypeType":"[(node,result)]"},{"TypeValue":"","TypeType":"[part.strip()forpartinentry.split(\"=\",1)]"},{"TypeValue":"key_str","TypeType":"key_node"},{"TypeValue":"current_output_dict[key_str]","TypeType":"sub_dict"},{"TypeValue":"sub_dict","TypeType":""},{"TypeValue":"args_dict","TypeType":""},{"TypeValue":"value","TypeType":"SetupParser"},{"TypeValue":"args_dict[keyword.arg]","TypeType":"SetupParser"},{"TypeValue":"console_scripts","TypeType":"entry_points"},{"TypeValue":"result[name]","TypeType":"path"},{"TypeValue":"config","TypeType":"configparser"},{"TypeValue":"entry_points","TypeType":""}],"Content":"def _parse_entry_points(entry_points: Union[Dict, str, List]) -> Dict[str, str]:        \"\"\"Parse entry_points data into a standardized dictionary format\"\"\"        result = {}        if isinstance(entry_points, dict):            console_scripts = entry_points.get(\"console_scripts\", [])            if isinstance(console_scripts, list):                for script in console_scripts:                    try:                        name, path = [part.strip() for part in script.split(\"=\", 1)]                        result[name] = path                    except ValueError:                        activity.logger.warning(\"Invalid entry point format\", {\"script\": script})            elif isinstance(console_scripts, dict):                result.update(console_scripts)        elif isinstance(entry_points, str):            try:                import configparser                config = configparser.ConfigParser()                if not entry_points.strip().startswith(\"[\"):                    entry_points = \"[console_scripts]\\n\" + entry_points                config.read_string(entry_points)                if \"console_scripts\" in config:                    result.update(dict(config[\"console_scripts\"]))            except Exception as e:                activity.logger.warning(\"Failed to parse entry_points string\", {\"error\": str(e), \"entry_points\": entry_points})        elif isinstance(entry_points, list):            for entry in entry_points:                try:                    name, path = [part.strip() for part in entry.split(\"=\", 1)]                    result[name] = path                except ValueError:                    activity.logger.warning(\"Invalid entry point format\", {\"entry\": entry})        return result    "},{"Name":"_update_metadata_from_setup_args","Parameters":[{"TypeValue":"metadata","TypeType":"UnoplatPackageManagerMetadata"},{"TypeValue":"setup_args","TypeType":"Dict[str,Any]"}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":150,"StartLinePosition":4,"StopLine":151,"StopLinePosition":4}}],"Position":{"StartLine":151,"StartLinePosition":4,"StopLine":215,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"SetupDictValue","TypeType":"Union"},{"TypeValue":"result","TypeType":"{}"},{"TypeValue":"stack","TypeType":"[(node,result)]"},{"TypeValue":"","TypeType":"[part.strip()forpartinentry.split(\"=\",1)]"},{"TypeValue":"key_str","TypeType":"key_node"},{"TypeValue":"current_output_dict[key_str]","TypeType":"sub_dict"},{"TypeValue":"sub_dict","TypeType":""},{"TypeValue":"args_dict","TypeType":""},{"TypeValue":"value","TypeType":"SetupParser"},{"TypeValue":"args_dict[keyword.arg]","TypeType":"SetupParser"},{"TypeValue":"console_scripts","TypeType":"entry_points"},{"TypeValue":"result[name]","TypeType":"path"},{"TypeValue":"config","TypeType":"configparser"},{"TypeValue":"entry_points","TypeType":""},{"TypeValue":"metadata.package_name","TypeType":"setup_args"},{"TypeValue":"metadata.project_version","TypeType":"setup_args"},{"TypeValue":"metadata.description","TypeType":"setup_args"},{"TypeValue":"authors","TypeType":"[setup_args[\"author\"]]"},{"TypeValue":"authors[0]","TypeType":"f\"{authors[0]} <{setup_args['author_email']}>\""},{"TypeValue":"metadata.authors","TypeType":"authors"},{"TypeValue":"metadata.license","TypeType":"setup_args"},{"TypeValue":"version_str","TypeType":"setup_args"},{"TypeValue":"metadata.programming_language_version","TypeType":"version_str"},{"TypeValue":"metadata.entry_points","TypeType":"SetupParser"},{"TypeValue":"req","TypeType":"Requirement"},{"TypeValue":"version","TypeType":"UnoplatVersion"},{"TypeValue":"dep","TypeType":"UnoplatProjectDependency"},{"TypeValue":"metadata.dependencies[req.name]","TypeType":"dep"},{"TypeValue":"existing_dep","TypeType":"metadata"},{"TypeValue":"existing_dep.extras","TypeType":"[extra_name]"}],"Content":"def _update_metadata_from_setup_args(metadata: UnoplatPackageManagerMetadata, setup_args: Dict[str, Any]) -> UnoplatPackageManagerMetadata:        \"\"\"Update metadata instance with setup arguments\"\"\"        if \"name\" in setup_args:            metadata.package_name = setup_args[\"name\"]        if \"version\" in setup_args:            metadata.project_version = setup_args[\"version\"]        if \"description\" in setup_args:            metadata.description = setup_args[\"description\"]        if \"author\" in setup_args:            authors = [setup_args[\"author\"]]            if \"author_email\" in setup_args:                authors[0] = f\"{authors[0]} <{setup_args['author_email']}>\"            metadata.authors = authors        if \"license\" in setup_args:            metadata.license = setup_args[\"license\"]        if \"python_requires\" in setup_args:            version_str = setup_args[\"python_requires\"]            metadata.programming_language_version = version_str        if \"entry_points\" in setup_args:            metadata.entry_points = SetupParser._parse_entry_points(setup_args[\"entry_points\"])        # Process install_requires        if \"install_requires\" in setup_args:            for req_str in setup_args[\"install_requires\"]:                req = Requirement(req_str)                version = UnoplatVersion()                # Handle version specifiers                if req.specifier:                    version = UnoplatVersion(specifier=str(req.specifier))                dep = UnoplatProjectDependency(version=version, extras=list(req.extras) if req.extras else None, environment_marker=str(req.marker) if req.marker else None)                metadata.dependencies[req.name] = dep        # Process extras_require        if \"extras_require\" in setup_args:            for extra_name, req_list in setup_args[\"extras_require\"].items():                for req_str in req_list:                    req = Requirement(req_str)                    version = UnoplatVersion()                    # Handle version specifiers                    if req.specifier:                        version = UnoplatVersion(specifier=str(req.specifier))                    dep = UnoplatProjectDependency(version=version, extras=[extra_name] + list(req.extras) if req.extras else [extra_name], environment_marker=str(req.marker) if req.marker else None, group=extra_name)                    if req.name not in metadata.dependencies:                        metadata.dependencies[req.name] = dep                    else:                        existing_dep = metadata.dependencies[req.name]                        if existing_dep.extras:                            existing_dep.extras.append(extra_name)                        else:                            existing_dep.extras = [extra_name]        return metadata    "},{"Name":"parse_setup_file","Parameters":[{"TypeValue":"root_dir","TypeType":"str"},{"TypeValue":"metadata","TypeType":"UnoplatPackageManagerMetadata"}],"FunctionCalls":[{"NodeName":"activity","FunctionName":"logger","Position":{"StartLine":255,"StartLinePosition":20,"StopLine":255,"StopLinePosition":21}},{"NodeName":"activity","FunctionName":"error","Position":{"StartLine":255,"StartLinePosition":27,"StopLine":255,"StopLinePosition":102}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":215,"StartLinePosition":4,"StopLine":216,"StopLinePosition":4}}],"Position":{"StartLine":216,"StartLinePosition":4,"StopLine":257},"LocalVariables":[{"TypeValue":"SetupDictValue","TypeType":"Union"},{"TypeValue":"result","TypeType":"{}"},{"TypeValue":"stack","TypeType":"[(node,result)]"},{"TypeValue":"","TypeType":"[part.strip()forpartinentry.split(\"=\",1)]"},{"TypeValue":"key_str","TypeType":"key_node"},{"TypeValue":"current_output_dict[key_str]","TypeType":"sub_dict"},{"TypeValue":"sub_dict","TypeType":""},{"TypeValue":"args_dict","TypeType":""},{"TypeValue":"value","TypeType":"SetupParser"},{"TypeValue":"args_dict[keyword.arg]","TypeType":"SetupParser"},{"TypeValue":"console_scripts","TypeType":"entry_points"},{"TypeValue":"result[name]","TypeType":"path"},{"TypeValue":"config","TypeType":"configparser"},{"TypeValue":"entry_points","TypeType":""},{"TypeValue":"metadata.package_name","TypeType":"setup_args"},{"TypeValue":"metadata.project_version","TypeType":"setup_args"},{"TypeValue":"metadata.description","TypeType":"setup_args"},{"TypeValue":"authors","TypeType":"[setup_args[\"author\"]]"},{"TypeValue":"authors[0]","TypeType":"f\"{authors[0]} <{setup_args['author_email']}>\""},{"TypeValue":"metadata.authors","TypeType":"authors"},{"TypeValue":"metadata.license","TypeType":"setup_args"},{"TypeValue":"version_str","TypeType":"setup_args"},{"TypeValue":"metadata.programming_language_version","TypeType":"version_str"},{"TypeValue":"metadata.entry_points","TypeType":"SetupParser"},{"TypeValue":"req","TypeType":"Requirement"},{"TypeValue":"version","TypeType":"UnoplatVersion"},{"TypeValue":"dep","TypeType":"UnoplatProjectDependency"},{"TypeValue":"metadata.dependencies[req.name]","TypeType":"dep"},{"TypeValue":"existing_dep","TypeType":"metadata"},{"TypeValue":"existing_dep.extras","TypeType":"[extra_name]"},{"TypeValue":"setup_file_path","TypeType":"os"},{"TypeValue":"setup_content","TypeType":"file"},{"TypeValue":"tree","TypeType":"ast"},{"TypeValue":"setup_args","TypeType":"args"},{"TypeValue":"args","TypeType":"SetupParser"},{"TypeValue":"metadata","TypeType":"SetupParser"}],"Content":"def parse_setup_file(root_dir: str, metadata: UnoplatPackageManagerMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Parse a setup.py file and update the UnoplatPackageManagerMetadata instance        Args:            root_dir: Path to the directory containing setup.py            metadata: Existing UnoplatPackageManagerMetadata instance to update        Returns:            Updated UnoplatPackageManagerMetadata instance        \"\"\"        try:            setup_file_path = os.path.join(root_dir, \"setup.py\")            if not os.path.exists(setup_file_path):                raise FileNotFoundError(f\"setup.py not found at {setup_file_path}\")            with open(setup_file_path, \"r\", encoding=\"utf-8\") as file:                setup_content = file.read()            # Parse the AST            tree = ast.parse(setup_content)            # Find the setup() call            setup_args = None            for node in ast.walk(tree):                args = SetupParser._extract_setup_args_from_ast(node)                if args:                    setup_args = args                    break            if setup_args:                # Log when python_requires is found                if \"python_requires\" in setup_args:                    activity.logger.info(\"Found python_requires in setup.py\", {\"version\": setup_args[\"python_requires\"], \"path\": setup_file_path})                metadata = SetupParser._update_metadata_from_setup_args(metadata, setup_args)            return metadata        except Exception as e:            activity.logger.error(\"Error parsing setup.py\", {\"error\": str(e), \"path\": setup_file_path})            return metadata"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_project_dependency","UsageName":["UnoplatProjectDependency"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_version","UsageName":["UnoplatVersion"]},{"Source":"os"},{"Source":"ast"},{"Source":"typing","UsageName":["Any","Dict","List","Optional","Union"]},{"Source":"packaging.requirements","UsageName":["Requirement"]},{"Source":"temporalio","UsageName":["activity"]},{"Source":"configparser"}],"Position":{"StartLine":23,"StopLine":257},"Content":"class SetupParser:    \"\"\"Parser for Python setup.py files to extract package metadata\"\"\"    SetupDictValue = Union[str, List[str], Dict[str, Any]]    @staticmethod    def _extract_constant_value(node: ast.AST) -> Optional[str]:        \"\"\"Extract string value from an AST Constant node\"\"\"        if isinstance(node, ast.Constant) and isinstance(node.value, str):            return node.value        return None    @staticmethod    def _extract_list_values(node: ast.List) -> List[str]:        \"\"\"Extract string values from an AST List node\"\"\"        return [elt.value for elt in node.elts if isinstance(elt, ast.Constant) and isinstance(elt.value, str)]    @staticmethod    def _extract_dict_values(node: ast.Dict) -> Dict[str, \"SetupParser.SetupDictValue\"]:        \"\"\"        Extract key-value pairs from an AST Dict node iteratively        (i.e., without recursion), handling:          - string constants          - lists of strings          - nested dictionaries        \"\"\"        # The dictionary for the top-level node        result: Dict[str, \"SetupParser.SetupDictValue\"] = {}        # Stack items are (dict_node, target_dict), where:        # - dict_node is the AST Dict to parse        # - target_dict is the Python dict we will fill at that level        stack = [(node, result)]        while stack:            current_ast_dict, current_output_dict = stack.pop()            for key_node, value_node in zip(current_ast_dict.keys, current_ast_dict.values):                # Only handle string keys                if not (isinstance(key_node, ast.Constant) and isinstance(key_node.value, str)):                    continue                key_str = key_node.value                if isinstance(value_node, ast.Constant) and isinstance(value_node.value, str):                    # A simple string                    current_output_dict[key_str] = value_node.value                elif isinstance(value_node, ast.List):                    # A list of strings                    current_output_dict[key_str] = SetupParser._extract_list_values(value_node)                elif isinstance(value_node, ast.Dict):                    # Nested dictionary: create a sub-dict in the current dictionary                    sub_dict: Dict[str, \"SetupParser.SetupDictValue\"] = {}                    current_output_dict[key_str] = sub_dict                    # Push it on the stack to parse                    stack.append((value_node, sub_dict))                # You can add more branches here for other node types if you need them        return result    @staticmethod    def _extract_setup_args_from_ast(node: ast.AST) -> Optional[Dict[str, Any]]:        \"\"\"Extract setup() arguments from an AST node\"\"\"        if not (isinstance(node, ast.Call) and isinstance(node.func, ast.Name) and node.func.id == \"setup\"):            return None        args_dict: Dict[str, Any] = {}        for keyword in node.keywords:            if keyword.arg is None:  # Skip if no argument name                continue            if isinstance(keyword.value, ast.Constant):                value = SetupParser._extract_constant_value(keyword.value)                if value is not None:                    args_dict[keyword.arg] = value            elif isinstance(keyword.value, ast.List):                args_dict[keyword.arg] = SetupParser._extract_list_values(keyword.value)            elif isinstance(keyword.value, ast.Dict):                # Handle nested dictionary structures                args_dict[keyword.arg] = SetupParser._extract_dict_values(keyword.value)        return args_dict    @staticmethod    def _parse_entry_points(entry_points: Union[Dict, str, List]) -> Dict[str, str]:        \"\"\"Parse entry_points data into a standardized dictionary format\"\"\"        result = {}        if isinstance(entry_points, dict):            console_scripts = entry_points.get(\"console_scripts\", [])            if isinstance(console_scripts, list):                for script in console_scripts:                    try:                        name, path = [part.strip() for part in script.split(\"=\", 1)]                        result[name] = path                    except ValueError:                        activity.logger.warning(\"Invalid entry point format\", {\"script\": script})            elif isinstance(console_scripts, dict):                result.update(console_scripts)        elif isinstance(entry_points, str):            try:                import configparser                config = configparser.ConfigParser()                if not entry_points.strip().startswith(\"[\"):                    entry_points = \"[console_scripts]\\n\" + entry_points                config.read_string(entry_points)                if \"console_scripts\" in config:                    result.update(dict(config[\"console_scripts\"]))            except Exception as e:                activity.logger.warning(\"Failed to parse entry_points string\", {\"error\": str(e), \"entry_points\": entry_points})        elif isinstance(entry_points, list):            for entry in entry_points:                try:                    name, path = [part.strip() for part in entry.split(\"=\", 1)]                    result[name] = path                except ValueError:                    activity.logger.warning(\"Invalid entry point format\", {\"entry\": entry})        return result    @staticmethod    def _update_metadata_from_setup_args(metadata: UnoplatPackageManagerMetadata, setup_args: Dict[str, Any]) -> UnoplatPackageManagerMetadata:        \"\"\"Update metadata instance with setup arguments\"\"\"        if \"name\" in setup_args:            metadata.package_name = setup_args[\"name\"]        if \"version\" in setup_args:            metadata.project_version = setup_args[\"version\"]        if \"description\" in setup_args:            metadata.description = setup_args[\"description\"]        if \"author\" in setup_args:            authors = [setup_args[\"author\"]]            if \"author_email\" in setup_args:                authors[0] = f\"{authors[0]} <{setup_args['author_email']}>\"            metadata.authors = authors        if \"license\" in setup_args:            metadata.license = setup_args[\"license\"]        if \"python_requires\" in setup_args:            version_str = setup_args[\"python_requires\"]            metadata.programming_language_version = version_str        if \"entry_points\" in setup_args:            metadata.entry_points = SetupParser._parse_entry_points(setup_args[\"entry_points\"])        # Process install_requires        if \"install_requires\" in setup_args:            for req_str in setup_args[\"install_requires\"]:                req = Requirement(req_str)                version = UnoplatVersion()                # Handle version specifiers                if req.specifier:                    version = UnoplatVersion(specifier=str(req.specifier))                dep = UnoplatProjectDependency(version=version, extras=list(req.extras) if req.extras else None, environment_marker=str(req.marker) if req.marker else None)                metadata.dependencies[req.name] = dep        # Process extras_require        if \"extras_require\" in setup_args:            for extra_name, req_list in setup_args[\"extras_require\"].items():                for req_str in req_list:                    req = Requirement(req_str)                    version = UnoplatVersion()                    # Handle version specifiers                    if req.specifier:                        version = UnoplatVersion(specifier=str(req.specifier))                    dep = UnoplatProjectDependency(version=version, extras=[extra_name] + list(req.extras) if req.extras else [extra_name], environment_marker=str(req.marker) if req.marker else None, group=extra_name)                    if req.name not in metadata.dependencies:                        metadata.dependencies[req.name] = dep                    else:                        existing_dep = metadata.dependencies[req.name]                        if existing_dep.extras:                            existing_dep.extras.append(extra_name)                        else:                            existing_dep.extras = [extra_name]        return metadata    @staticmethod    def parse_setup_file(root_dir: str, metadata: UnoplatPackageManagerMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Parse a setup.py file and update the UnoplatPackageManagerMetadata instance        Args:            root_dir: Path to the directory containing setup.py            metadata: Existing UnoplatPackageManagerMetadata instance to update        Returns:            Updated UnoplatPackageManagerMetadata instance        \"\"\"        try:            setup_file_path = os.path.join(root_dir, \"setup.py\")            if not os.path.exists(setup_file_path):                raise FileNotFoundError(f\"setup.py not found at {setup_file_path}\")            with open(setup_file_path, \"r\", encoding=\"utf-8\") as file:                setup_content = file.read()            # Parse the AST            tree = ast.parse(setup_content)            # Find the setup() call            setup_args = None            for node in ast.walk(tree):                args = SetupParser._extract_setup_args_from_ast(node)                if args:                    setup_args = args                    break            if setup_args:                # Log when python_requires is found                if \"python_requires\" in setup_args:                    activity.logger.info(\"Found python_requires in setup.py\", {\"version\": setup_args[\"python_requires\"], \"path\": setup_file_path})                metadata = SetupParser._update_metadata_from_setup_args(metadata, setup_args)            return metadata        except Exception as e:            activity.logger.error(\"Error parsing setup.py\", {\"error\": str(e), \"path\": setup_file_path})            return metadata"},{"NodeName":"RequirementsUtils","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/package_manager/utils/requirements_utils.py","Functions":[{"Name":"parse_requirements_folder","Parameters":[{"TypeValue":"workspace_path","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":95,"StartLinePosition":22,"StopLine":95,"StopLinePosition":66}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":40,"StartLinePosition":4,"StopLine":41,"StopLinePosition":4}}],"Position":{"StartLine":41,"StartLinePosition":4,"StopLine":99,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"_SKIP_OPTIONS","TypeType":"{\"-c\",\"--constraint\",\"-r\",\"--requirement\",\"--no-binary\",\"--only-binary\",\"--prefer-binary\",\"--require-hashes\",\"--pre\",\"--trusted-host\",\"--use-feature\",\"-Z\",\"--always-unzip\",}"},{"TypeValue":"requirements_paths","TypeType":"[]"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"req_folder","TypeType":"os"},{"TypeValue":"default_path","TypeType":"os"},{"TypeValue":"req_txt_path","TypeType":"os"},{"TypeValue":"root_req_txt","TypeType":"os"},{"TypeValue":"tuple_dependency","TypeType":"RequirementsUtils"},{"TypeValue":"dependencies[tuple_dependency[0]]","TypeType":"tuple_dependency"}],"Content":"def parse_requirements_folder(workspace_path: str) -> Dict[str, UnoplatProjectDependency]:        \"\"\"Parse requirements files from the requirements folder.        Args:            workspace_path: Path to project workspace        Returns:            Dict of parsed dependencies with package name as key        Note:            Looks for requirements files in this order:            1. requirements/default.txt            2. requirements/requirements.txt            3. requirements/*.txt            4. requirements.txt (in workspace root)        \"\"\"        requirements_paths = []        dependencies: Dict[str, UnoplatProjectDependency] = {}        # Check requirements folder first        req_folder = os.path.join(workspace_path, \"requirements\")        if os.path.exists(req_folder):            # Priority 1: default.txt            default_path = os.path.join(req_folder, \"default.txt\")            if os.path.exists(default_path):                requirements_paths.append(default_path)            # Priority 2: requirements.txt in requirements folder            req_txt_path = os.path.join(req_folder, \"requirements.txt\")            if os.path.exists(req_txt_path):                requirements_paths.append(req_txt_path)            # Priority 3: All .txt files in requirements folder            if not requirements_paths:                requirements_paths.extend([os.path.join(req_folder, f) for f in os.listdir(req_folder) if f.endswith(\".txt\")])        # Priority 4: requirements.txt in workspace root        root_req_txt = os.path.join(workspace_path, \"requirements.txt\")        if os.path.exists(root_req_txt):            requirements_paths.append(root_req_txt)        if not requirements_paths:            logger.warning(f\"No requirements files found in {workspace_path}\")            return {}        # Parse all found requirement files using requirements-parser        for req_file in requirements_paths:            try:                with open(req_file, \"r\") as f:                    for req in requirements.parse(f):                        tuple_dependency = RequirementsUtils._convert_requirement_to_dependency(req)                        if tuple_dependency:                            dependencies[tuple_dependency[0]] = tuple_dependency[1]            except Exception as e:                logger.error(f\"Error parsing {req_file}: {str(e)}\")        return dependencies    "},{"Name":"_convert_requirement_to_dependency","Parameters":[{"TypeValue":"req","TypeType":"Requirement"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":168,"StartLinePosition":18,"StopLine":168,"StopLinePosition":77}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":99,"StartLinePosition":4,"StopLine":100,"StopLinePosition":4}}],"Position":{"StartLine":100,"StartLinePosition":4,"StopLine":170},"LocalVariables":[{"TypeValue":"_SKIP_OPTIONS","TypeType":"{\"-c\",\"--constraint\",\"-r\",\"--requirement\",\"--no-binary\",\"--only-binary\",\"--prefer-binary\",\"--require-hashes\",\"--pre\",\"--trusted-host\",\"--use-feature\",\"-Z\",\"--always-unzip\",}"},{"TypeValue":"requirements_paths","TypeType":"[]"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"req_folder","TypeType":"os"},{"TypeValue":"default_path","TypeType":"os"},{"TypeValue":"req_txt_path","TypeType":"os"},{"TypeValue":"root_req_txt","TypeType":"os"},{"TypeValue":"tuple_dependency","TypeType":"(name,UnoplatProjectDependency(version=version,extras=sorted(req.extras)ifreq.extraselseNone,source=source,source_url=source_url,source_reference=req.revisionifreq.vcselseNone,subdirectory=req.subdirectoryifhasattr(req,\"subdirectory\")elseNone,hash_info=hash_info))"},{"TypeValue":"dependencies[tuple_dependency[0]]","TypeType":"tuple_dependency"},{"TypeValue":"name","TypeType":"req"},{"TypeValue":"version","TypeType":"UnoplatVersion"},{"TypeValue":"sorted_specs","TypeType":"sorted"},{"TypeValue":"version.minimum_version","TypeType":"f\">={ver}\""},{"TypeValue":"version.maximum_version","TypeType":"f\"<{'.'.join(parts)}\""},{"TypeValue":"version.current_version","TypeType":"f\"=={ver}\""},{"TypeValue":"parts","TypeType":"ver"},{"TypeValue":"parts[0]","TypeType":"str"},{"TypeValue":"parts[-2]","TypeType":"str"},{"TypeValue":"parts[-1]","TypeType":"\"0\""},{"TypeValue":"source","TypeType":"\"path\""},{"TypeValue":"source_url","TypeType":"req"},{"TypeValue":"hash_info","TypeType":"f\"{req.hash_name}:{req.hash}\""}],"Content":"def _convert_requirement_to_dependency(req: Requirement) -> Optional[Tuple[str, UnoplatProjectDependency]]:        \"\"\"Convert requirements-parser Requirement to UnoplatProjectDependency.\"\"\"        try:            # Get package name, handling both VCS and regular requirements            name = req.name            if not name and req.uri:                # Try to get name from URI fragment                if hasattr(req, \"fragment\") and req.fragment:                    name = req.fragment.get(\"egg\")            if not name:                logger.warning(f\"Could not determine package name from: {req.line}\")                return None            # Handle version specs            version = UnoplatVersion()            if req.specs:                # Sort specs for consistent output                sorted_specs = sorted(req.specs, key=lambda x: (x[0], x[1]))                # Parse version constraints into min/max versions                for op, ver in sorted_specs:                    if op == \">=\":                        version.minimum_version = f\">={ver}\"                    elif op == \">\":                        version.minimum_version = f\">{ver}\"                    elif op == \"<=\":                        version.maximum_version = f\"<={ver}\"                    elif op == \"<\":                        version.maximum_version = f\"<{ver}\"                    elif op == \"==\":                        version.current_version = f\"=={ver}\"                    elif op == \"~=\":  # Compatible release operator                        version.minimum_version = f\">={ver}\"                        # Handle compatible release based on version components                        parts = ver.split(\".\")                        if len(parts) >= 2:                            if len(parts) == 2:                                # For X.Y format, allow up to next major                                parts[0] = str(int(parts[0]) + 1)                                version.maximum_version = f\"<{'.'.join(parts)}\"                            else:                                # For X.Y.Z... format, allow up to next minor                                parts[-2] = str(int(parts[-2]) + 1)                                parts[-1] = \"0\"                                version.maximum_version = f\"<{'.'.join(parts)}\"            # Determine source info            source = None            source_url = None            if req.vcs:                source = req.vcs                source_url = req.uri            elif req.uri:                source = \"url\"                source_url = req.uri            elif req.local_file:                source = \"path\"                source_url = req.path            # Get hash info if present            hash_info = None            if hasattr(req, \"hash_name\") and req.hash_name:                hash_info = f\"{req.hash_name}:{req.hash}\"            tuple_dependency = (name, UnoplatProjectDependency(version=version, extras=sorted(req.extras) if req.extras else None, source=source, source_url=source_url, source_reference=req.revision if req.vcs else None, subdirectory=req.subdirectory if hasattr(req, \"subdirectory\") else None, hash_info=hash_info))            return tuple_dependency        except Exception as e:            logger.error(f\"Error converting requirement {req.line}: {str(e)}\")            return None"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_project_dependency","UsageName":["UnoplatProjectDependency"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_version","UsageName":["UnoplatVersion"]},{"Source":"os"},{"Source":"typing","UsageName":["Dict","Optional","Tuple"]},{"Source":"loguru","UsageName":["logger"]},{"Source":"requirements"},{"Source":"requirements.requirement","UsageName":["Requirement"]}],"Position":{"StartLine":20,"StopLine":170},"Content":"class RequirementsUtils:    \"\"\"Utility class for parsing requirements files using requirements-parser library.\"\"\"    # Add constants for unsupported options that we want to skip    _SKIP_OPTIONS = {        \"-c\",        \"--constraint\",  # Constraint files        \"-r\",        \"--requirement\",  # Recursive requirements        \"--no-binary\",        \"--only-binary\",        \"--prefer-binary\",        \"--require-hashes\",        \"--pre\",        \"--trusted-host\",        \"--use-feature\",        \"-Z\",        \"--always-unzip\",    }    @staticmethod    def parse_requirements_folder(workspace_path: str) -> Dict[str, UnoplatProjectDependency]:        \"\"\"Parse requirements files from the requirements folder.        Args:            workspace_path: Path to project workspace        Returns:            Dict of parsed dependencies with package name as key        Note:            Looks for requirements files in this order:            1. requirements/default.txt            2. requirements/requirements.txt            3. requirements/*.txt            4. requirements.txt (in workspace root)        \"\"\"        requirements_paths = []        dependencies: Dict[str, UnoplatProjectDependency] = {}        # Check requirements folder first        req_folder = os.path.join(workspace_path, \"requirements\")        if os.path.exists(req_folder):            # Priority 1: default.txt            default_path = os.path.join(req_folder, \"default.txt\")            if os.path.exists(default_path):                requirements_paths.append(default_path)            # Priority 2: requirements.txt in requirements folder            req_txt_path = os.path.join(req_folder, \"requirements.txt\")            if os.path.exists(req_txt_path):                requirements_paths.append(req_txt_path)            # Priority 3: All .txt files in requirements folder            if not requirements_paths:                requirements_paths.extend([os.path.join(req_folder, f) for f in os.listdir(req_folder) if f.endswith(\".txt\")])        # Priority 4: requirements.txt in workspace root        root_req_txt = os.path.join(workspace_path, \"requirements.txt\")        if os.path.exists(root_req_txt):            requirements_paths.append(root_req_txt)        if not requirements_paths:            logger.warning(f\"No requirements files found in {workspace_path}\")            return {}        # Parse all found requirement files using requirements-parser        for req_file in requirements_paths:            try:                with open(req_file, \"r\") as f:                    for req in requirements.parse(f):                        tuple_dependency = RequirementsUtils._convert_requirement_to_dependency(req)                        if tuple_dependency:                            dependencies[tuple_dependency[0]] = tuple_dependency[1]            except Exception as e:                logger.error(f\"Error parsing {req_file}: {str(e)}\")        return dependencies    @staticmethod    def _convert_requirement_to_dependency(req: Requirement) -> Optional[Tuple[str, UnoplatProjectDependency]]:        \"\"\"Convert requirements-parser Requirement to UnoplatProjectDependency.\"\"\"        try:            # Get package name, handling both VCS and regular requirements            name = req.name            if not name and req.uri:                # Try to get name from URI fragment                if hasattr(req, \"fragment\") and req.fragment:                    name = req.fragment.get(\"egg\")            if not name:                logger.warning(f\"Could not determine package name from: {req.line}\")                return None            # Handle version specs            version = UnoplatVersion()            if req.specs:                # Sort specs for consistent output                sorted_specs = sorted(req.specs, key=lambda x: (x[0], x[1]))                # Parse version constraints into min/max versions                for op, ver in sorted_specs:                    if op == \">=\":                        version.minimum_version = f\">={ver}\"                    elif op == \">\":                        version.minimum_version = f\">{ver}\"                    elif op == \"<=\":                        version.maximum_version = f\"<={ver}\"                    elif op == \"<\":                        version.maximum_version = f\"<{ver}\"                    elif op == \"==\":                        version.current_version = f\"=={ver}\"                    elif op == \"~=\":  # Compatible release operator                        version.minimum_version = f\">={ver}\"                        # Handle compatible release based on version components                        parts = ver.split(\".\")                        if len(parts) >= 2:                            if len(parts) == 2:                                # For X.Y format, allow up to next major                                parts[0] = str(int(parts[0]) + 1)                                version.maximum_version = f\"<{'.'.join(parts)}\"                            else:                                # For X.Y.Z... format, allow up to next minor                                parts[-2] = str(int(parts[-2]) + 1)                                parts[-1] = \"0\"                                version.maximum_version = f\"<{'.'.join(parts)}\"            # Determine source info            source = None            source_url = None            if req.vcs:                source = req.vcs                source_url = req.uri            elif req.uri:                source = \"url\"                source_url = req.uri            elif req.local_file:                source = \"path\"                source_url = req.path            # Get hash info if present            hash_info = None            if hasattr(req, \"hash_name\") and req.hash_name:                hash_info = f\"{req.hash_name}:{req.hash}\"            tuple_dependency = (name, UnoplatProjectDependency(version=version, extras=sorted(req.extras) if req.extras else None, source=source, source_url=source_url, source_reference=req.revision if req.vcs else None, subdirectory=req.subdirectory if hasattr(req, \"subdirectory\") else None, hash_info=hash_info))            return tuple_dependency        except Exception as e:            logger.error(f\"Error converting requirement {req.line}: {str(e)}\")            return None"},{"NodeName":"PipStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/package_manager/pip/pip_strategy.py","MultipleExtend":["PackageManagerStrategy"],"Functions":[{"Name":"process_metadata","Parameters":[{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"metadata","TypeType":"ProgrammingLanguageMetadata"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":79,"StartLinePosition":18,"StopLine":79,"StopLinePosition":67}}],"Position":{"StartLine":37,"StartLinePosition":4,"StopLine":82,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"REQUIREMENT_PATTERNS","TypeType":""},{"TypeValue":"package_metadata","TypeType":"SetupParser"},{"TypeValue":"workspace","TypeType":"Path"},{"TypeValue":"requirement_files","TypeType":"self"},{"TypeValue":"requirements","TypeType":"self"},{"TypeValue":"existing","TypeType":"package_metadata"},{"TypeValue":"existing.version","TypeType":"req"},{"TypeValue":"existing.extras","TypeType":"req"},{"TypeValue":"existing.environment_marker","TypeType":"req"},{"TypeValue":"existing.source","TypeType":"req"},{"TypeValue":"existing.source_url","TypeType":"req"},{"TypeValue":"existing.source_reference","TypeType":"req"},{"TypeValue":"existing.subdirectory","TypeType":"req"},{"TypeValue":"package_metadata.dependencies[name]","TypeType":"req"}],"Content":"def process_metadata(self, local_workspace_path: str, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Process pip specific metadata from setup.py and requirements files.\"\"\"        try:            # Create initial metadata            package_metadata = UnoplatPackageManagerMetadata(dependencies={}, programming_language=metadata.language.value, package_manager=metadata.package_manager.value, programming_language_version=metadata.language_version)            workspace = Path(local_workspace_path)            # First parse setup.py for core metadata and dependencies            try:                package_metadata = SetupParser.parse_setup_file(str(workspace), package_metadata)            except FileNotFoundError:                logger.warning(\"setup.py not found\")            except Exception as e:                logger.error(f\"Error parsing setup.py: {str(e)}\")            # Then parse requirements files for additional dependencies            requirement_files = self._find_requirement_files(workspace)            if requirement_files:                requirements = self._parse_all_requirements(str(workspace))                # Merge with existing dependencies                for name, req in requirements.items():                    if name in package_metadata.dependencies:                        # Update existing dependency                        existing = package_metadata.dependencies[name]                        if req.version:                            existing.version = req.version                        if req.extras:                            existing.extras = req.extras                        if req.environment_marker:                            existing.environment_marker = req.environment_marker                        if req.source:                            existing.source = req.source                            existing.source_url = req.source_url                            existing.source_reference = req.source_reference                            existing.subdirectory = req.subdirectory                    else:                        package_metadata.dependencies[name] = req            return package_metadata        except Exception as e:            logger.error(f\"Error processing pip metadata: {str(e)}\")            return self._create_empty_metadata(metadata)    "},{"Name":"_parse_all_requirements","Parameters":[{"TypeValue":"workspace_path","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"warning","Position":{"StartLine":93,"StartLinePosition":22,"StopLine":93,"StopLinePosition":68}}],"Position":{"StartLine":82,"StartLinePosition":4,"StopLine":97,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"REQUIREMENT_PATTERNS","TypeType":""},{"TypeValue":"package_metadata","TypeType":"SetupParser"},{"TypeValue":"workspace","TypeType":"Path"},{"TypeValue":"requirement_files","TypeType":"self"},{"TypeValue":"requirements","TypeType":"self"},{"TypeValue":"existing","TypeType":"package_metadata"},{"TypeValue":"existing.version","TypeType":"req"},{"TypeValue":"existing.extras","TypeType":"req"},{"TypeValue":"existing.environment_marker","TypeType":"req"},{"TypeValue":"existing.source","TypeType":"req"},{"TypeValue":"existing.source_url","TypeType":"req"},{"TypeValue":"existing.source_reference","TypeType":"req"},{"TypeValue":"existing.subdirectory","TypeType":"req"},{"TypeValue":"package_metadata.dependencies[name]","TypeType":"req"},{"TypeValue":"all_dependencies","TypeType":""},{"TypeValue":"file_deps","TypeType":"self"}],"Content":"def _parse_all_requirements(self, workspace_path: str) -> Dict[str, UnoplatProjectDependency]:        \"\"\"Parse all requirement files in the workspace.\"\"\"        all_dependencies: Dict[str, UnoplatProjectDependency] = {}        workspace = Path(workspace_path)        requirement_files = self._find_requirement_files(workspace)        for req_file in requirement_files:            try:                file_deps = self._parse_requirement_file(req_file)                all_dependencies.update(file_deps)            except Exception as e:                logger.warning(f\"Error parsing {req_file}: {str(e)}\")        return all_dependencies    "},{"Name":"_parse_requirement_file","Parameters":[{"TypeValue":"file_path","TypeType":"Path"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":150,"StartLinePosition":18,"StopLine":150,"StopLinePosition":63}}],"Position":{"StartLine":97,"StartLinePosition":4,"StopLine":154,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"REQUIREMENT_PATTERNS","TypeType":""},{"TypeValue":"package_metadata","TypeType":"SetupParser"},{"TypeValue":"workspace","TypeType":"Path"},{"TypeValue":"requirement_files","TypeType":"self"},{"TypeValue":"requirements","TypeType":"self"},{"TypeValue":"existing","TypeType":"package_metadata"},{"TypeValue":"existing.version","TypeType":"req"},{"TypeValue":"existing.extras","TypeType":"req"},{"TypeValue":"existing.environment_marker","TypeType":"req"},{"TypeValue":"existing.source","TypeType":"req"},{"TypeValue":"existing.source_url","TypeType":"req"},{"TypeValue":"existing.source_reference","TypeType":"req"},{"TypeValue":"existing.subdirectory","TypeType":"req"},{"TypeValue":"package_metadata.dependencies[name]","TypeType":"req"},{"TypeValue":"all_dependencies","TypeType":""},{"TypeValue":"file_deps","TypeType":"self"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"group","TypeType":"file_path"},{"TypeValue":"line","TypeType":"line"},{"TypeValue":"vcs_result","TypeType":"self"},{"TypeValue":"","TypeType":"vcs_result"},{"TypeValue":"dep.group","TypeType":"group"},{"TypeValue":"dependencies[name]","TypeType":"dep"},{"TypeValue":"req","TypeType":"Requirement"},{"TypeValue":"dep","TypeType":"self"},{"TypeValue":"dep.environment_marker","TypeType":"str"},{"TypeValue":"dependencies[req.name]","TypeType":"dep"}],"Content":"def _parse_requirement_file(self, file_path: Path) -> Dict[str, UnoplatProjectDependency]:        \"\"\"Parse a single requirement file.\"\"\"        dependencies: Dict[str, UnoplatProjectDependency] = {}        try:            # Determine group from filename            group = None            if \"requirements-\" in file_path.name:                group = file_path.name.replace(\"requirements-\", \"\").replace(\".txt\", \"\")            with open(file_path, \"r\") as f:                for line in f:                    line = line.strip()                    if not line or line.startswith(\"#\"):                        continue                    if line.startswith(\"-e\") or line.startswith(\"-r\"):                        continue  # Skip editable installs and requirements includes                    try:                        # Check for VCS URLs first                        if any(line.startswith(f\"{vcs}+\") for vcs in (\"git\", \"hg\", \"svn\", \"bzr\")):                            vcs_result = self._parse_vcs_line(line)                            if vcs_result:                                name, dep = vcs_result                                if group:                                    dep.group = group                                dependencies[name] = dep                            continue                        # Handle regular requirements                        req = Requirement(line)                        # Create appropriate dependency                        if req.url:                            dep = self._create_url_dependency(req)                        else:                            dep = self._create_version_dependency(req)                        # Add environment marker if present                        if req.marker:                            dep.environment_marker = str(req.marker)                        # Add group if from a group-specific file                        if group:                            dep.group = group                        dependencies[req.name] = dep                    except Exception as e:                        logger.warning(f\"Error parsing requirement '{line}': {str(e)}\")        except Exception as e:            logger.error(f\"Error reading {file_path}: {str(e)}\")        return dependencies    "},{"Name":"_parse_vcs_line","Parameters":[{"TypeValue":"line","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"warning","Position":{"StartLine":189,"StartLinePosition":18,"StopLine":189,"StopLinePosition":71}}],"Position":{"StartLine":154,"StartLinePosition":4,"StopLine":192,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"REQUIREMENT_PATTERNS","TypeType":""},{"TypeValue":"package_metadata","TypeType":"SetupParser"},{"TypeValue":"workspace","TypeType":"Path"},{"TypeValue":"requirement_files","TypeType":"self"},{"TypeValue":"requirements","TypeType":"self"},{"TypeValue":"existing","TypeType":"package_metadata"},{"TypeValue":"existing.version","TypeType":"req"},{"TypeValue":"existing.extras","TypeType":"req"},{"TypeValue":"existing.environment_marker","TypeType":"req"},{"TypeValue":"existing.source","TypeType":"req"},{"TypeValue":"existing.source_url","TypeType":"req"},{"TypeValue":"existing.source_reference","TypeType":"req"},{"TypeValue":"existing.subdirectory","TypeType":"req"},{"TypeValue":"package_metadata.dependencies[name]","TypeType":"req"},{"TypeValue":"all_dependencies","TypeType":""},{"TypeValue":"file_deps","TypeType":"self"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"group","TypeType":"file_path"},{"TypeValue":"line","TypeType":"line"},{"TypeValue":"vcs_result","TypeType":"self"},{"TypeValue":"","TypeType":"repo_url"},{"TypeValue":"dep.group","TypeType":"group"},{"TypeValue":"dependencies[name]","TypeType":"dep"},{"TypeValue":"req","TypeType":"Requirement"},{"TypeValue":"dep","TypeType":"self"},{"TypeValue":"dep.environment_marker","TypeType":"str"},{"TypeValue":"dependencies[req.name]","TypeType":"dep"},{"TypeValue":"params","TypeType":"dict"},{"TypeValue":"name","TypeType":"params"},{"TypeValue":"reference","TypeType":"None"},{"TypeValue":"dependency","TypeType":"UnoplatProjectDependency"}],"Content":"def _parse_vcs_line(self, line: str) -> Optional[tuple[str, UnoplatProjectDependency]]:        \"\"\"        Parse a VCS URL line into a tuple of (package_name, dependency_object).        Returns:            Optional[tuple[str, UnoplatProjectDependency]]: Tuple of (name, dependency) or None if parsing fails        \"\"\"        try:            # Extract package name from egg= parameter            if \"#egg=\" not in line:                return None            url, egg_part = line.split(\"#\", 1)            params = dict(p.split(\"=\") for p in egg_part.split(\"&\"))            name = params.get(\"egg\")            if not name:                return None            # Extract VCS type and URL            vcs_type, repo_url = url.split(\"+\", 1)            # Extract reference if present (@branch, @tag, @commit)            reference = None            if \"@\" in repo_url:                repo_url, reference = repo_url.rsplit(\"@\", 1)            dependency = UnoplatProjectDependency(                version=UnoplatVersion(),  # VCS deps don't have version constraints                source=vcs_type,                source_url=repo_url,                source_reference=reference,                subdirectory=params.get(\"subdirectory\"),            )            return name, dependency        except Exception as e:            logger.warning(f\"Error parsing VCS line '{line}': {str(e)}\")            return None    "},{"Name":"_create_url_dependency","Parameters":[{"TypeValue":"req","TypeType":"Requirement"}],"FunctionCalls":[{"FunctionName":"str","Position":{"StartLine":230,"StartLinePosition":27,"StopLine":230,"StopLinePosition":41}}],"Position":{"StartLine":192,"StartLinePosition":4,"StopLine":234,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"REQUIREMENT_PATTERNS","TypeType":""},{"TypeValue":"package_metadata","TypeType":"SetupParser"},{"TypeValue":"workspace","TypeType":"Path"},{"TypeValue":"requirement_files","TypeType":"self"},{"TypeValue":"requirements","TypeType":"self"},{"TypeValue":"existing","TypeType":"package_metadata"},{"TypeValue":"existing.version","TypeType":"req"},{"TypeValue":"existing.extras","TypeType":"req"},{"TypeValue":"existing.environment_marker","TypeType":"req"},{"TypeValue":"existing.source","TypeType":"req"},{"TypeValue":"existing.source_url","TypeType":"req"},{"TypeValue":"existing.source_reference","TypeType":"req"},{"TypeValue":"existing.subdirectory","TypeType":"req"},{"TypeValue":"package_metadata.dependencies[name]","TypeType":"req"},{"TypeValue":"all_dependencies","TypeType":""},{"TypeValue":"file_deps","TypeType":"self"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"group","TypeType":"file_path"},{"TypeValue":"line","TypeType":"line"},{"TypeValue":"vcs_result","TypeType":"self"},{"TypeValue":"","TypeType":"source_url"},{"TypeValue":"dep.group","TypeType":"group"},{"TypeValue":"dependencies[name]","TypeType":"dep"},{"TypeValue":"req","TypeType":"Requirement"},{"TypeValue":"dep","TypeType":"self"},{"TypeValue":"dep.environment_marker","TypeType":"str"},{"TypeValue":"dependencies[req.name]","TypeType":"dep"},{"TypeValue":"params","TypeType":"dict"},{"TypeValue":"name","TypeType":"params"},{"TypeValue":"reference","TypeType":"None"},{"TypeValue":"dependency","TypeType":"UnoplatProjectDependency"},{"TypeValue":"source","TypeType":"vcs"},{"TypeValue":"source_url","TypeType":"source_url"},{"TypeValue":"source_reference","TypeType":"None"},{"TypeValue":"subdirectory","TypeType":"None"},{"TypeValue":"specifier","TypeType":"str"},{"TypeValue":"prefix","TypeType":""}],"Content":"def _create_url_dependency(self, req: Requirement) -> UnoplatProjectDependency:        \"\"\"        Create a UnoplatProjectDependency for a URL-based requirement (including VCS).        Args:            req: Package requirement with URL        Returns:            UnoplatProjectDependency with source, URL and version information        Example:            \"flask @ git+https://github.com/pallets/flask.git@2.0.0\"            -> source=\"git\", source_url=\"https://github.com/pallets/flask.git\",               source_reference=\"2.0.0\", specifier=\"==2.0.0\"        \"\"\"        source = \"url\"        source_url = str(req.url)        source_reference = None        subdirectory = None        specifier = None  # Initialize specifier        # Check for VCS prefixes (git+, hg+, etc.)        for vcs in (\"git\", \"hg\", \"svn\", \"bzr\"):            prefix = vcs + \"+\"            if source_url.startswith(prefix):                source = vcs                source_url = source_url[len(prefix) :]  # remove \"git+\" etc.                break        # If there's an @, assume it indicates a branch/tag/commit        if \"@\" in source_url:            source_url, source_reference = source_url.rsplit(\"@\", 1)            # If it looks like a version number, set it as specifier            if source_reference and any(c.isdigit() for c in source_reference):                specifier = f\"=={source_reference}\"        # Only update version from req.specifier if we haven't set it from reference        if req.specifier and not specifier:            specifier = str(req.specifier)        return UnoplatProjectDependency(version=UnoplatVersion(specifier=specifier), source=source, source_url=source_url, source_reference=source_reference, extras=list(req.extras) if req.extras else None, environment_marker=str(req.marker) if req.marker else None, subdirectory=subdirectory)    "},{"Name":"_create_version_dependency","Parameters":[{"TypeValue":"req","TypeType":"Requirement"}],"FunctionCalls":[{"FunctionName":"UnoplatVersion","Position":{"StartLine":249,"StartLinePosition":32,"StopLine":249,"StopLinePosition":88}}],"Position":{"StartLine":234,"StartLinePosition":4,"StopLine":253,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"REQUIREMENT_PATTERNS","TypeType":""},{"TypeValue":"package_metadata","TypeType":"SetupParser"},{"TypeValue":"workspace","TypeType":"Path"},{"TypeValue":"requirement_files","TypeType":"self"},{"TypeValue":"requirements","TypeType":"self"},{"TypeValue":"existing","TypeType":"package_metadata"},{"TypeValue":"existing.version","TypeType":"req"},{"TypeValue":"existing.extras","TypeType":"req"},{"TypeValue":"existing.environment_marker","TypeType":"req"},{"TypeValue":"existing.source","TypeType":"req"},{"TypeValue":"existing.source_url","TypeType":"req"},{"TypeValue":"existing.source_reference","TypeType":"req"},{"TypeValue":"existing.subdirectory","TypeType":"req"},{"TypeValue":"package_metadata.dependencies[name]","TypeType":"req"},{"TypeValue":"all_dependencies","TypeType":""},{"TypeValue":"file_deps","TypeType":"self"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"group","TypeType":"file_path"},{"TypeValue":"line","TypeType":"line"},{"TypeValue":"vcs_result","TypeType":"self"},{"TypeValue":"","TypeType":"source_url"},{"TypeValue":"dep.group","TypeType":"group"},{"TypeValue":"dependencies[name]","TypeType":"dep"},{"TypeValue":"req","TypeType":"Requirement"},{"TypeValue":"dep","TypeType":"self"},{"TypeValue":"dep.environment_marker","TypeType":"str"},{"TypeValue":"dependencies[req.name]","TypeType":"dep"},{"TypeValue":"params","TypeType":"dict"},{"TypeValue":"name","TypeType":"params"},{"TypeValue":"reference","TypeType":"None"},{"TypeValue":"dependency","TypeType":"UnoplatProjectDependency"},{"TypeValue":"source","TypeType":"vcs"},{"TypeValue":"source_url","TypeType":"source_url"},{"TypeValue":"source_reference","TypeType":"None"},{"TypeValue":"subdirectory","TypeType":"None"},{"TypeValue":"specifier","TypeType":"str"},{"TypeValue":"prefix","TypeType":""},{"TypeValue":"version","TypeType":"UnoplatVersion"}],"Content":"def _create_version_dependency(self, req: Requirement) -> UnoplatProjectDependency:        \"\"\"Create dependency object for version-based requirements.        Args:            req: Package requirement with version specifiers        Returns:            UnoplatProjectDependency with version specifier as-is        Example:            req: \"requests>=2.0.0\"            -> version.specifier=\">=2.0.0\"            req: \"black==22.3.0\"            -> version.specifier=\"==22.3.0\"        \"\"\"        version = UnoplatVersion(specifier=str(req.specifier) if req.specifier else None)        return UnoplatProjectDependency(version=version, extras=list(req.extras) if req.extras else None)    "},{"Name":"_create_empty_metadata","Parameters":[{"TypeValue":"metadata","TypeType":"ProgrammingLanguageMetadata"}],"Position":{"StartLine":253,"StartLinePosition":4,"StopLine":257,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"REQUIREMENT_PATTERNS","TypeType":""},{"TypeValue":"package_metadata","TypeType":"SetupParser"},{"TypeValue":"workspace","TypeType":"Path"},{"TypeValue":"requirement_files","TypeType":"self"},{"TypeValue":"requirements","TypeType":"self"},{"TypeValue":"existing","TypeType":"package_metadata"},{"TypeValue":"existing.version","TypeType":"req"},{"TypeValue":"existing.extras","TypeType":"req"},{"TypeValue":"existing.environment_marker","TypeType":"req"},{"TypeValue":"existing.source","TypeType":"req"},{"TypeValue":"existing.source_url","TypeType":"req"},{"TypeValue":"existing.source_reference","TypeType":"req"},{"TypeValue":"existing.subdirectory","TypeType":"req"},{"TypeValue":"package_metadata.dependencies[name]","TypeType":"req"},{"TypeValue":"all_dependencies","TypeType":""},{"TypeValue":"file_deps","TypeType":"self"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"group","TypeType":"file_path"},{"TypeValue":"line","TypeType":"line"},{"TypeValue":"vcs_result","TypeType":"self"},{"TypeValue":"","TypeType":"source_url"},{"TypeValue":"dep.group","TypeType":"group"},{"TypeValue":"dependencies[name]","TypeType":"dep"},{"TypeValue":"req","TypeType":"Requirement"},{"TypeValue":"dep","TypeType":"self"},{"TypeValue":"dep.environment_marker","TypeType":"str"},{"TypeValue":"dependencies[req.name]","TypeType":"dep"},{"TypeValue":"params","TypeType":"dict"},{"TypeValue":"name","TypeType":"params"},{"TypeValue":"reference","TypeType":"None"},{"TypeValue":"dependency","TypeType":"UnoplatProjectDependency"},{"TypeValue":"source","TypeType":"vcs"},{"TypeValue":"source_url","TypeType":"source_url"},{"TypeValue":"source_reference","TypeType":"None"},{"TypeValue":"subdirectory","TypeType":"None"},{"TypeValue":"specifier","TypeType":"str"},{"TypeValue":"prefix","TypeType":""},{"TypeValue":"version","TypeType":"UnoplatVersion"}],"Content":"def _create_empty_metadata(self, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Create empty metadata with basic information.\"\"\"        return UnoplatPackageManagerMetadata(dependencies={}, programming_language=metadata.language.value, package_manager=metadata.package_manager, programming_language_version=metadata.language_version)    "},{"Name":"_find_requirement_files","Parameters":[{"TypeValue":"workspace","TypeType":"Path"}],"FunctionCalls":[{"NodeName":"requirement_files","FunctionName":"append","Position":{"StartLine":266,"StartLinePosition":37,"StopLine":266,"StopLinePosition":54}}],"Position":{"StartLine":257,"StartLinePosition":4,"StopLine":268},"LocalVariables":[{"TypeValue":"REQUIREMENT_PATTERNS","TypeType":""},{"TypeValue":"package_metadata","TypeType":"SetupParser"},{"TypeValue":"workspace","TypeType":"Path"},{"TypeValue":"requirement_files","TypeType":""},{"TypeValue":"requirements","TypeType":"self"},{"TypeValue":"existing","TypeType":"package_metadata"},{"TypeValue":"existing.version","TypeType":"req"},{"TypeValue":"existing.extras","TypeType":"req"},{"TypeValue":"existing.environment_marker","TypeType":"req"},{"TypeValue":"existing.source","TypeType":"req"},{"TypeValue":"existing.source_url","TypeType":"req"},{"TypeValue":"existing.source_reference","TypeType":"req"},{"TypeValue":"existing.subdirectory","TypeType":"req"},{"TypeValue":"package_metadata.dependencies[name]","TypeType":"req"},{"TypeValue":"all_dependencies","TypeType":""},{"TypeValue":"file_deps","TypeType":"self"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"group","TypeType":"file_path"},{"TypeValue":"line","TypeType":"line"},{"TypeValue":"vcs_result","TypeType":"self"},{"TypeValue":"","TypeType":"source_url"},{"TypeValue":"dep.group","TypeType":"group"},{"TypeValue":"dependencies[name]","TypeType":"dep"},{"TypeValue":"req","TypeType":"Requirement"},{"TypeValue":"dep","TypeType":"self"},{"TypeValue":"dep.environment_marker","TypeType":"str"},{"TypeValue":"dependencies[req.name]","TypeType":"dep"},{"TypeValue":"params","TypeType":"dict"},{"TypeValue":"name","TypeType":"params"},{"TypeValue":"reference","TypeType":"None"},{"TypeValue":"dependency","TypeType":"UnoplatProjectDependency"},{"TypeValue":"source","TypeType":"vcs"},{"TypeValue":"source_url","TypeType":"source_url"},{"TypeValue":"source_reference","TypeType":"None"},{"TypeValue":"subdirectory","TypeType":"None"},{"TypeValue":"specifier","TypeType":"str"},{"TypeValue":"prefix","TypeType":""},{"TypeValue":"version","TypeType":"UnoplatVersion"},{"TypeValue":"candidate","TypeType":""}],"Content":"def _find_requirement_files(self, workspace: Path) -> List[Path]:        \"\"\"Find all requirement files in the workspace, matching any of the REQUIREMENT_PATTERNS.\"\"\"        requirement_files: List[Path] = []        for pattern in self.REQUIREMENT_PATTERNS:            if \"*\" in pattern:                requirement_files.extend(workspace.glob(pattern))            else:                candidate = workspace / pattern                if candidate.exists():                    requirement_files.append(candidate)        return requirement_files"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_project_dependency","UsageName":["UnoplatProjectDependency"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_version","UsageName":["UnoplatVersion"]},{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["ProgrammingLanguageMetadata"]},{"Source":"src.code_confluence_flow_bridge.parser.package_manager.package_manager_strategy","UsageName":["PackageManagerStrategy"]},{"Source":"src.code_confluence_flow_bridge.parser.package_manager.utils.setup_parser","UsageName":["SetupParser"]},{"Source":"pathlib","UsageName":["Path"]},{"Source":"typing","UsageName":["Dict","List","Optional"]},{"Source":"loguru","UsageName":["logger"]},{"Source":"packaging.requirements","UsageName":["Requirement"]}],"Position":{"StartLine":30,"StopLine":268},"Content":"class PipStrategy(PackageManagerStrategy):    # Standard requirement file patterns    REQUIREMENT_PATTERNS: List[str] = [        \"requirements.txt\",  # Base requirements        \"requirements-*.txt\",  # Environment-specific requirements    ]    def process_metadata(self, local_workspace_path: str, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Process pip specific metadata from setup.py and requirements files.\"\"\"        try:            # Create initial metadata            package_metadata = UnoplatPackageManagerMetadata(dependencies={}, programming_language=metadata.language.value, package_manager=metadata.package_manager.value, programming_language_version=metadata.language_version)            workspace = Path(local_workspace_path)            # First parse setup.py for core metadata and dependencies            try:                package_metadata = SetupParser.parse_setup_file(str(workspace), package_metadata)            except FileNotFoundError:                logger.warning(\"setup.py not found\")            except Exception as e:                logger.error(f\"Error parsing setup.py: {str(e)}\")            # Then parse requirements files for additional dependencies            requirement_files = self._find_requirement_files(workspace)            if requirement_files:                requirements = self._parse_all_requirements(str(workspace))                # Merge with existing dependencies                for name, req in requirements.items():                    if name in package_metadata.dependencies:                        # Update existing dependency                        existing = package_metadata.dependencies[name]                        if req.version:                            existing.version = req.version                        if req.extras:                            existing.extras = req.extras                        if req.environment_marker:                            existing.environment_marker = req.environment_marker                        if req.source:                            existing.source = req.source                            existing.source_url = req.source_url                            existing.source_reference = req.source_reference                            existing.subdirectory = req.subdirectory                    else:                        package_metadata.dependencies[name] = req            return package_metadata        except Exception as e:            logger.error(f\"Error processing pip metadata: {str(e)}\")            return self._create_empty_metadata(metadata)    def _parse_all_requirements(self, workspace_path: str) -> Dict[str, UnoplatProjectDependency]:        \"\"\"Parse all requirement files in the workspace.\"\"\"        all_dependencies: Dict[str, UnoplatProjectDependency] = {}        workspace = Path(workspace_path)        requirement_files = self._find_requirement_files(workspace)        for req_file in requirement_files:            try:                file_deps = self._parse_requirement_file(req_file)                all_dependencies.update(file_deps)            except Exception as e:                logger.warning(f\"Error parsing {req_file}: {str(e)}\")        return all_dependencies    def _parse_requirement_file(self, file_path: Path) -> Dict[str, UnoplatProjectDependency]:        \"\"\"Parse a single requirement file.\"\"\"        dependencies: Dict[str, UnoplatProjectDependency] = {}        try:            # Determine group from filename            group = None            if \"requirements-\" in file_path.name:                group = file_path.name.replace(\"requirements-\", \"\").replace(\".txt\", \"\")            with open(file_path, \"r\") as f:                for line in f:                    line = line.strip()                    if not line or line.startswith(\"#\"):                        continue                    if line.startswith(\"-e\") or line.startswith(\"-r\"):                        continue  # Skip editable installs and requirements includes                    try:                        # Check for VCS URLs first                        if any(line.startswith(f\"{vcs}+\") for vcs in (\"git\", \"hg\", \"svn\", \"bzr\")):                            vcs_result = self._parse_vcs_line(line)                            if vcs_result:                                name, dep = vcs_result                                if group:                                    dep.group = group                                dependencies[name] = dep                            continue                        # Handle regular requirements                        req = Requirement(line)                        # Create appropriate dependency                        if req.url:                            dep = self._create_url_dependency(req)                        else:                            dep = self._create_version_dependency(req)                        # Add environment marker if present                        if req.marker:                            dep.environment_marker = str(req.marker)                        # Add group if from a group-specific file                        if group:                            dep.group = group                        dependencies[req.name] = dep                    except Exception as e:                        logger.warning(f\"Error parsing requirement '{line}': {str(e)}\")        except Exception as e:            logger.error(f\"Error reading {file_path}: {str(e)}\")        return dependencies    def _parse_vcs_line(self, line: str) -> Optional[tuple[str, UnoplatProjectDependency]]:        \"\"\"        Parse a VCS URL line into a tuple of (package_name, dependency_object).        Returns:            Optional[tuple[str, UnoplatProjectDependency]]: Tuple of (name, dependency) or None if parsing fails        \"\"\"        try:            # Extract package name from egg= parameter            if \"#egg=\" not in line:                return None            url, egg_part = line.split(\"#\", 1)            params = dict(p.split(\"=\") for p in egg_part.split(\"&\"))            name = params.get(\"egg\")            if not name:                return None            # Extract VCS type and URL            vcs_type, repo_url = url.split(\"+\", 1)            # Extract reference if present (@branch, @tag, @commit)            reference = None            if \"@\" in repo_url:                repo_url, reference = repo_url.rsplit(\"@\", 1)            dependency = UnoplatProjectDependency(                version=UnoplatVersion(),  # VCS deps don't have version constraints                source=vcs_type,                source_url=repo_url,                source_reference=reference,                subdirectory=params.get(\"subdirectory\"),            )            return name, dependency        except Exception as e:            logger.warning(f\"Error parsing VCS line '{line}': {str(e)}\")            return None    def _create_url_dependency(self, req: Requirement) -> UnoplatProjectDependency:        \"\"\"        Create a UnoplatProjectDependency for a URL-based requirement (including VCS).        Args:            req: Package requirement with URL        Returns:            UnoplatProjectDependency with source, URL and version information        Example:            \"flask @ git+https://github.com/pallets/flask.git@2.0.0\"            -> source=\"git\", source_url=\"https://github.com/pallets/flask.git\",               source_reference=\"2.0.0\", specifier=\"==2.0.0\"        \"\"\"        source = \"url\"        source_url = str(req.url)        source_reference = None        subdirectory = None        specifier = None  # Initialize specifier        # Check for VCS prefixes (git+, hg+, etc.)        for vcs in (\"git\", \"hg\", \"svn\", \"bzr\"):            prefix = vcs + \"+\"            if source_url.startswith(prefix):                source = vcs                source_url = source_url[len(prefix) :]  # remove \"git+\" etc.                break        # If there's an @, assume it indicates a branch/tag/commit        if \"@\" in source_url:            source_url, source_reference = source_url.rsplit(\"@\", 1)            # If it looks like a version number, set it as specifier            if source_reference and any(c.isdigit() for c in source_reference):                specifier = f\"=={source_reference}\"        # Only update version from req.specifier if we haven't set it from reference        if req.specifier and not specifier:            specifier = str(req.specifier)        return UnoplatProjectDependency(version=UnoplatVersion(specifier=specifier), source=source, source_url=source_url, source_reference=source_reference, extras=list(req.extras) if req.extras else None, environment_marker=str(req.marker) if req.marker else None, subdirectory=subdirectory)    def _create_version_dependency(self, req: Requirement) -> UnoplatProjectDependency:        \"\"\"Create dependency object for version-based requirements.        Args:            req: Package requirement with version specifiers        Returns:            UnoplatProjectDependency with version specifier as-is        Example:            req: \"requests>=2.0.0\"            -> version.specifier=\">=2.0.0\"            req: \"black==22.3.0\"            -> version.specifier=\"==22.3.0\"        \"\"\"        version = UnoplatVersion(specifier=str(req.specifier) if req.specifier else None)        return UnoplatProjectDependency(version=version, extras=list(req.extras) if req.extras else None)    def _create_empty_metadata(self, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Create empty metadata with basic information.\"\"\"        return UnoplatPackageManagerMetadata(dependencies={}, programming_language=metadata.language.value, package_manager=metadata.package_manager, programming_language_version=metadata.language_version)    def _find_requirement_files(self, workspace: Path) -> List[Path]:        \"\"\"Find all requirement files in the workspace, matching any of the REQUIREMENT_PATTERNS.\"\"\"        requirement_files: List[Path] = []        for pattern in self.REQUIREMENT_PATTERNS:            if \"*\" in pattern:                requirement_files.extend(workspace.glob(pattern))            else:                candidate = workspace / pattern                if candidate.exists():                    requirement_files.append(candidate)        return requirement_files"},{"NodeName":"UvStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/package_manager/uv/uv_strategy.py","MultipleExtend":["PackageManagerStrategy"],"Functions":[{"Name":"process_metadata","Parameters":[{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"metadata","TypeType":"ProgrammingLanguageMetadata"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"warning","Position":{"StartLine":144,"StartLinePosition":26,"StopLine":144,"StopLinePosition":95}}],"Position":{"StartLine":29,"StartLinePosition":4,"StopLine":166,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":""},{"TypeValue":"pyproject_data","TypeType":"tomllib"},{"TypeValue":"project_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"","TypeType":"self"},{"TypeValue":"version","TypeType":"self"},{"TypeValue":"dependencies[name]","TypeType":"UnoplatProjectDependency"},{"TypeValue":"optional_deps","TypeType":"project_data"},{"TypeValue":"existing_dep","TypeType":"dependencies"},{"TypeValue":"existing_groups","TypeType":"[existing_dep.group]"},{"TypeValue":"existing_dep.group","TypeType":"\",\""},{"TypeValue":"existing_extras","TypeType":""},{"TypeValue":"existing_dep.extras","TypeType":"list"},{"TypeValue":"uv_config","TypeType":"pyproject_data"},{"TypeValue":"sources","TypeType":"uv_config"},{"TypeValue":"dependencies[pkg_name].source","TypeType":"\"index\""},{"TypeValue":"dependencies[pkg_name].source_url","TypeType":"self"},{"TypeValue":"dep_metadata","TypeType":"uv_config"},{"TypeValue":"name","TypeType":"metadata_entry"},{"TypeValue":"dependencies[name].environment_marker","TypeType":"f\"python_version {metadata_entry['requires-python']}\""}],"Content":"def process_metadata(self, local_workspace_path: str, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Process UV project metadata from pyproject.toml file.        Parses a pyproject.toml file to extract package metadata and dependencies.        Handles main dependencies, optional dependencies, git sources, and environment markers.        Example pyproject.toml:            [project]            name = \"my-package\"            version = \"1.0.0\"            dependencies = [                \"requests>=2.25.0\",                \"pandas[excel]>=1.2.0\"            ]            [project.optional-dependencies]            test = [\"pytest>=6.0.0\"]            [tool.uv.sources]            my-pkg = { git = \"https://github.com/user/repo.git\", tag = \"v1.0.0\" }        Args:            local_workspace_path: Path to the directory containing pyproject.toml            metadata: Programming language metadata configuration        Returns:            UnoplatPackageManagerMetadata containing all parsed project information        Raises:            ValueError: If pyproject.toml exists but cannot be parsed        \"\"\"        pyproject_path = Path(local_workspace_path) / \"pyproject.toml\"        if not pyproject_path.exists():            return UnoplatPackageManagerMetadata(programming_language=\"python\", package_manager=\"uv\")        try:            with open(pyproject_path, \"rb\") as f:                pyproject_data = tomllib.load(f)        except Exception as e:            raise ValueError(f\"Failed to parse pyproject.toml: {str(e)}\")        # Extract project metadata        project_data = pyproject_data.get(\"project\", {})        # Parse dependencies        dependencies: Dict[str, UnoplatProjectDependency] = {}        # Process main dependencies        for dep in project_data.get(\"dependencies\", []):            try:                name, version_str, extras = self._parse_dependency(dep)                version = self._parse_version_constraint(version_str) if version_str else UnoplatVersion()                dependencies[name] = UnoplatProjectDependency(                    version=version,                    extras=extras,  # Now properly handling extras                )            except Exception as e:                logger.warning(f\"Error processing dependency {dep}: {str(e)}\")                continue        # Process optional dependencies (extras)        optional_deps = project_data.get(\"optional-dependencies\", {})        for group_name, deps in optional_deps.items():            for dep in deps:                try:                    name, version_str, extras = self._parse_dependency(dep)                    version = self._parse_version_constraint(version_str) if version_str else UnoplatVersion()                    if name not in dependencies:                        dependencies[name] = UnoplatProjectDependency(                            version=version,                            group=group_name,                            extras=extras,  # Include extras                        )                    else:                        # Update existing dependency                        existing_dep = dependencies[name]                        if existing_dep.group != group_name:                            existing_groups = [existing_dep.group] if existing_dep.group else []                            existing_groups.append(group_name)                            existing_dep.group = \",\".join(existing_groups)                        if extras:                            existing_extras = existing_dep.extras or []                            existing_dep.extras = list(set(existing_extras + extras))                except Exception as e:                    logger.warning(f\"Error processing optional dependency {dep} in group {group_name}: {str(e)}\")                    continue        # Process UV-specific sources        uv_config = pyproject_data.get(\"tool\", {}).get(\"uv\", {})        sources = uv_config.get(\"sources\", {})        for pkg_name, source_info in sources.items():            if pkg_name in dependencies:                try:                    if \"git\" in source_info:                        self._process_git_source(dependencies, pkg_name, source_info)                    elif \"index\" in source_info:                        dependencies[pkg_name].source = \"index\"                        dependencies[pkg_name].source_url = self._get_index_url(pyproject_data, source_info[\"index\"])                except Exception as e:                    logger.warning(f\"Error processing source for {pkg_name}: {str(e)}\")                    continue        # Process dependency metadata for environment markers        dep_metadata = uv_config.get(\"dependency-metadata\", [])        for metadata_entry in dep_metadata:            name = metadata_entry.get(\"name\")            if name in dependencies:                try:                    # Add environment markers if specified                    if \"requires-python\" in metadata_entry:                        dependencies[name].environment_marker = f\"python_version {metadata_entry['requires-python']}\"                except Exception as e:                    logger.warning(f\"Error processing dependency metadata for {name}: {str(e)}\")                    continue        return UnoplatPackageManagerMetadata(            dependencies=dependencies,            package_name=project_data.get(\"name\"),            programming_language=\"python\",            package_manager=\"uv\",            programming_language_version=project_data.get(\"requires-python\"),            project_version=project_data.get(\"version\"),            description=project_data.get(\"description\"),            authors=project_data.get(\"authors\", []),            license=project_data.get(\"license\"),            entry_points=self._parse_entry_points(project_data),            homepage=project_data.get(\"homepage\"),            repository=project_data.get(\"repository\"),            documentation=project_data.get(\"documentation\"),            keywords=project_data.get(\"keywords\", []),            maintainers=project_data.get(\"maintainers\", []),            readme=project_data.get(\"readme\"),        )    "},{"Name":"_parse_dependency","Parameters":[{"TypeValue":"dep_string","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"warning","Position":{"StartLine":213,"StartLinePosition":18,"StopLine":213,"StopLinePosition":79}}],"Position":{"StartLine":166,"StartLinePosition":4,"StopLine":216,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":""},{"TypeValue":"pyproject_data","TypeType":"tomllib"},{"TypeValue":"project_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"","TypeType":"name"},{"TypeValue":"version","TypeType":"self"},{"TypeValue":"dependencies[name]","TypeType":"UnoplatProjectDependency"},{"TypeValue":"optional_deps","TypeType":"project_data"},{"TypeValue":"existing_dep","TypeType":"dependencies"},{"TypeValue":"existing_groups","TypeType":"[existing_dep.group]"},{"TypeValue":"existing_dep.group","TypeType":"\",\""},{"TypeValue":"existing_extras","TypeType":""},{"TypeValue":"existing_dep.extras","TypeType":"list"},{"TypeValue":"uv_config","TypeType":"pyproject_data"},{"TypeValue":"sources","TypeType":"uv_config"},{"TypeValue":"dependencies[pkg_name].source","TypeType":"\"index\""},{"TypeValue":"dependencies[pkg_name].source_url","TypeType":"self"},{"TypeValue":"dep_metadata","TypeType":"uv_config"},{"TypeValue":"name","TypeType":"name"},{"TypeValue":"dependencies[name].environment_marker","TypeType":"f\"python_version {metadata_entry['requires-python']}\""},{"TypeValue":"package_part","TypeType":"dep_string"},{"TypeValue":"version_part","TypeType":"f\"{operator}{version_part.strip()}\""},{"TypeValue":"extras","TypeType":"[e.strip()foreinextras_str.split(\",\")]"},{"TypeValue":"extras_str","TypeType":"extras_str"}],"Content":"def _parse_dependency(self, dep_string: str) -> tuple[str, Optional[str], Optional[List[str]]]:        \"\"\"Parse a dependency string into name, version, and extras.        Handles various dependency formats including:        - Simple version specifiers: \"requests>=2.25.0\"        - Multiple version specifiers: \"requests>=2.25.0,<3.0.0\"        - Extras: \"pandas[excel,csv]>=1.2.0\"        Examples:            >>> _parse_dependency(\"requests>=2.25.0\")            (\"requests\", \">=2.25.0\", None)            >>> _parse_dependency(\"pandas[excel,csv]>=1.2.0\")            (\"pandas\", \">=1.2.0\", [\"excel\", \"csv\"])            >>> _parse_dependency(\"flask\")            (\"flask\", None, None)        Args:            dep_string: The dependency string to parse        Returns:            tuple containing:            - str: Package name            - Optional[str]: Version specifier if present            - Optional[List[str]]: List of extras if present        \"\"\"        try:            # First split on version operators to get the package part            package_part = dep_string            version_part = None            for operator in (\">=\", \"==\", \"<=\", \"<\", \">\"):                if operator in dep_string:                    package_part, version_part = dep_string.split(operator, 1)                    version_part = f\"{operator}{version_part.strip()}\"                    break            # Now handle extras in the package part            extras = None            name = package_part.strip()            if \"[\" in name and \"]\" in name:                name, extras_str = name.split(\"[\", 1)                extras_str = extras_str.split(\"]\")[0]                extras = [e.strip() for e in extras_str.split(\",\")]                name = name.strip()            return name, version_part, extras        except Exception as e:            logger.warning(f\"Error parsing dependency '{dep_string}': {str(e)}\")            return dep_string.strip(), None, None    "},{"Name":"_parse_version_constraint","Parameters":[{"TypeValue":"constraint","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"warning","Position":{"StartLine":246,"StartLinePosition":18,"StopLine":246,"StopLinePosition":87}}],"Position":{"StartLine":216,"StartLinePosition":4,"StopLine":249,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":""},{"TypeValue":"pyproject_data","TypeType":"tomllib"},{"TypeValue":"project_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"","TypeType":"name"},{"TypeValue":"version","TypeType":"self"},{"TypeValue":"dependencies[name]","TypeType":"UnoplatProjectDependency"},{"TypeValue":"optional_deps","TypeType":"project_data"},{"TypeValue":"existing_dep","TypeType":"dependencies"},{"TypeValue":"existing_groups","TypeType":"[existing_dep.group]"},{"TypeValue":"existing_dep.group","TypeType":"\",\""},{"TypeValue":"existing_extras","TypeType":""},{"TypeValue":"existing_dep.extras","TypeType":"list"},{"TypeValue":"uv_config","TypeType":"pyproject_data"},{"TypeValue":"sources","TypeType":"uv_config"},{"TypeValue":"dependencies[pkg_name].source","TypeType":"\"index\""},{"TypeValue":"dependencies[pkg_name].source_url","TypeType":"self"},{"TypeValue":"dep_metadata","TypeType":"uv_config"},{"TypeValue":"name","TypeType":"name"},{"TypeValue":"dependencies[name].environment_marker","TypeType":"f\"python_version {metadata_entry['requires-python']}\""},{"TypeValue":"package_part","TypeType":"dep_string"},{"TypeValue":"version_part","TypeType":"f\"{operator}{version_part.strip()}\""},{"TypeValue":"extras","TypeType":"[e.strip()foreinextras_str.split(\",\")]"},{"TypeValue":"extras_str","TypeType":"extras_str"}],"Content":"def _parse_version_constraint(self, constraint: str) -> UnoplatVersion:        \"\"\"Parse version constraint into a specifier string.        Handles various version constraint formats:        - Simple constraints: \"==1.0.0\", \">=2.0.0\"        - Complex constraints: \">=1.0.0,<2.0.0\"        - Compatible release: \"~=1.0.0\"        Examples:            >>> _parse_version_constraint(\">=1.0.0\")            UnoplatVersion(specifier=\">=1.0.0\")            >>> _parse_version_constraint(\">=1.0.0,<2.0.0\")            UnoplatVersion(specifier=\">=1.0.0,<2.0.0\")            >>> _parse_version_constraint(\"==1.0.0\")            UnoplatVersion(specifier=\"==1.0.0\")        Args:            constraint: Version constraint string        Returns:            UnoplatVersion with specifier field set        \"\"\"        if not constraint or constraint == \"*\":            return UnoplatVersion()        try:            # Simply store the raw version constraint as specifier            return UnoplatVersion(specifier=constraint)        except Exception as e:            logger.warning(f\"Error parsing version constraint '{constraint}': {str(e)}\")            return UnoplatVersion()    "},{"Name":"_get_index_url","Parameters":[{"TypeValue":"pyproject_data","TypeType":"dict"},{"TypeValue":"index_name","TypeType":"str"}],"FunctionCalls":[{"NodeName":"pyproject_data","FunctionName":"get","Position":{"StartLine":264,"StartLinePosition":34,"StopLine":264,"StopLinePosition":49}},{"NodeName":"pyproject_data","FunctionName":"get","Position":{"StartLine":264,"StartLinePosition":50,"StopLine":264,"StopLinePosition":63}}],"Position":{"StartLine":249,"StartLinePosition":4,"StopLine":270,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":""},{"TypeValue":"pyproject_data","TypeType":"tomllib"},{"TypeValue":"project_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"","TypeType":"name"},{"TypeValue":"version","TypeType":"self"},{"TypeValue":"dependencies[name]","TypeType":"UnoplatProjectDependency"},{"TypeValue":"optional_deps","TypeType":"project_data"},{"TypeValue":"existing_dep","TypeType":"dependencies"},{"TypeValue":"existing_groups","TypeType":"[existing_dep.group]"},{"TypeValue":"existing_dep.group","TypeType":"\",\""},{"TypeValue":"existing_extras","TypeType":""},{"TypeValue":"existing_dep.extras","TypeType":"list"},{"TypeValue":"uv_config","TypeType":"pyproject_data"},{"TypeValue":"sources","TypeType":"uv_config"},{"TypeValue":"dependencies[pkg_name].source","TypeType":"\"index\""},{"TypeValue":"dependencies[pkg_name].source_url","TypeType":"self"},{"TypeValue":"dep_metadata","TypeType":"uv_config"},{"TypeValue":"name","TypeType":"name"},{"TypeValue":"dependencies[name].environment_marker","TypeType":"f\"python_version {metadata_entry['requires-python']}\""},{"TypeValue":"package_part","TypeType":"dep_string"},{"TypeValue":"version_part","TypeType":"f\"{operator}{version_part.strip()}\""},{"TypeValue":"extras","TypeType":"[e.strip()foreinextras_str.split(\",\")]"},{"TypeValue":"extras_str","TypeType":"extras_str"}],"Content":"def _get_index_url(self, pyproject_data: dict, index_name: str) -> Optional[str]:        \"\"\"Get the URL for a named custom package index from UV configuration.        Example pyproject.toml:            [[tool.uv.index]]            name = \"private-pypi\"            url = \"https://pypi.internal.company.com/simple\"        Args:            pyproject_data: Parsed pyproject.toml data            index_name: Name of the index to look up        Returns:            URL string if index is found, None otherwise        \"\"\"        uv_config = pyproject_data.get(\"tool\", {}).get(\"uv\", {})        for index in uv_config.get(\"index\", []):            if index.get(\"name\") == index_name:                return index.get(\"url\")        return None    "},{"Name":"_parse_entry_points","Parameters":[{"TypeValue":"project_data","TypeType":"dict"}],"FunctionCalls":[{"NodeName":"entry_points","FunctionName":"update","Position":{"StartLine":297,"StartLinePosition":20,"StopLine":297,"StopLinePosition":43}}],"Position":{"StartLine":270,"StartLinePosition":4,"StopLine":301,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":""},{"TypeValue":"pyproject_data","TypeType":"tomllib"},{"TypeValue":"project_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"","TypeType":"name"},{"TypeValue":"version","TypeType":"self"},{"TypeValue":"dependencies[name]","TypeType":"UnoplatProjectDependency"},{"TypeValue":"optional_deps","TypeType":"project_data"},{"TypeValue":"existing_dep","TypeType":"dependencies"},{"TypeValue":"existing_groups","TypeType":"[existing_dep.group]"},{"TypeValue":"existing_dep.group","TypeType":"\",\""},{"TypeValue":"existing_extras","TypeType":""},{"TypeValue":"existing_dep.extras","TypeType":"list"},{"TypeValue":"uv_config","TypeType":"pyproject_data"},{"TypeValue":"sources","TypeType":"uv_config"},{"TypeValue":"dependencies[pkg_name].source","TypeType":"\"index\""},{"TypeValue":"dependencies[pkg_name].source_url","TypeType":"self"},{"TypeValue":"dep_metadata","TypeType":"uv_config"},{"TypeValue":"name","TypeType":"name"},{"TypeValue":"dependencies[name].environment_marker","TypeType":"f\"python_version {metadata_entry['requires-python']}\""},{"TypeValue":"package_part","TypeType":"dep_string"},{"TypeValue":"version_part","TypeType":"f\"{operator}{version_part.strip()}\""},{"TypeValue":"extras","TypeType":"[e.strip()foreinextras_str.split(\",\")]"},{"TypeValue":"extras_str","TypeType":"extras_str"},{"TypeValue":"entry_points","TypeType":"{}"},{"TypeValue":"scripts","TypeType":"project_data"},{"TypeValue":"console_scripts","TypeType":"project_data"}],"Content":"def _parse_entry_points(self, project_data: dict) -> Dict[str, str]:        \"\"\"Parse entry points from project configuration.        Handles both script entries and console_scripts entry points.        Example pyproject.toml:            [project.scripts]            my-cli = \"my_package.cli:main\"            [project.entry-points.\"console_scripts\"]            another-cli = \"my_package.another:main\"        Args:            project_data: Project section from pyproject.toml        Returns:            Dictionary mapping script names to their entry points            Example: {\"my-cli\": \"my_package.cli:main\", \"another-cli\": \"my_package.another:main\"}        \"\"\"        entry_points = {}        # Parse scripts section        scripts = project_data.get(\"scripts\", {})        entry_points.update(scripts)        # Parse console_scripts entry points        console_scripts = project_data.get(\"entry-points\", {}).get(\"console_scripts\", {})        entry_points.update(console_scripts)        return entry_points    "},{"Name":"_parse_vcs_url","Parameters":[{"TypeValue":"url","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"warning","Position":{"StartLine":371,"StartLinePosition":18,"StopLine":371,"StopLinePosition":69}}],"Position":{"StartLine":301,"StartLinePosition":4,"StopLine":374,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":""},{"TypeValue":"pyproject_data","TypeType":"tomllib"},{"TypeValue":"project_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"","TypeType":"parsed"},{"TypeValue":"version","TypeType":"self"},{"TypeValue":"dependencies[name]","TypeType":"UnoplatProjectDependency"},{"TypeValue":"optional_deps","TypeType":"project_data"},{"TypeValue":"existing_dep","TypeType":"dependencies"},{"TypeValue":"existing_groups","TypeType":"[existing_dep.group]"},{"TypeValue":"existing_dep.group","TypeType":"\",\""},{"TypeValue":"existing_extras","TypeType":""},{"TypeValue":"existing_dep.extras","TypeType":"list"},{"TypeValue":"uv_config","TypeType":"pyproject_data"},{"TypeValue":"sources","TypeType":"uv_config"},{"TypeValue":"dependencies[pkg_name].source","TypeType":"\"index\""},{"TypeValue":"dependencies[pkg_name].source_url","TypeType":"self"},{"TypeValue":"dep_metadata","TypeType":"uv_config"},{"TypeValue":"name","TypeType":"name"},{"TypeValue":"dependencies[name].environment_marker","TypeType":"f\"python_version {metadata_entry['requires-python']}\""},{"TypeValue":"package_part","TypeType":"dep_string"},{"TypeValue":"version_part","TypeType":"f\"{operator}{version_part.strip()}\""},{"TypeValue":"extras","TypeType":"[e.strip()foreinextras_str.split(\",\")]"},{"TypeValue":"extras_str","TypeType":"extras_str"},{"TypeValue":"entry_points","TypeType":"{}"},{"TypeValue":"scripts","TypeType":"project_data"},{"TypeValue":"console_scripts","TypeType":"project_data"},{"TypeValue":"parsed","TypeType":"urlsplit"},{"TypeValue":"base_url","TypeType":"f\"{parsed.scheme}://{parsed.netloc}{path_parts[0]}\""},{"TypeValue":"source_reference","TypeType":"path_parts"},{"TypeValue":"path_parts","TypeType":"parsed"},{"TypeValue":"subdirectory","TypeType":"params"},{"TypeValue":"params","TypeType":"parse_qs"},{"TypeValue":"egg","TypeType":"params"}],"Content":"def _parse_vcs_url(self, url: str) -> tuple[Optional[str], Optional[str], Optional[str], Optional[str]]:        \"\"\"Parse VCS URL into components using urllib.parse.        Handles various VCS URL formats:        - Simple git URLs: \"git+https://github.com/user/repo.git\"        - URLs with references: \"git+https://github.com/user/repo.git@main\"        - URLs with extras: \"git+https://github.com/user/repo.git#egg=package&subdirectory=src\"        Examples:            >>> _parse_vcs_url(\"git+https://github.com/user/repo.git\")            (\"https://github.com/user/repo\", None, None, \"repo\")            >>> _parse_vcs_url(\"git+https://github.com/user/repo.git@main#egg=mypackage\")            (\"https://github.com/user/repo\", \"main\", None, \"mypackage\")            >>> _parse_vcs_url(\"git+https://github.com/user/repo.git#egg=pkg&subdirectory=src\")            (\"https://github.com/user/repo\", None, \"src\", \"pkg\")        Args:            url: VCS URL to parse        Returns:            tuple containing:            - Optional[str]: Base URL without VCS prefix            - Optional[str]: Reference (branch, tag, or commit)            - Optional[str]: Subdirectory path if specified            - Optional[str]: Package name from egg fragment or URL path        \"\"\"        try:            # Parse URL into components            parsed = urlsplit(url)            # Get base URL without VCS prefix            if \"+\" in parsed.scheme:                _, base_scheme = parsed.scheme.split(\"+\", 1)                base_url = f\"{base_scheme}://{parsed.netloc}{parsed.path}\"            else:                base_url = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"            # Remove .git suffix if present            if base_url.endswith(\".git\"):                base_url = base_url[:-4]            # Extract reference from path if present            source_reference = None            if \"@\" in parsed.path:                path_parts = parsed.path.rsplit(\"@\", 1)                base_url = f\"{parsed.scheme}://{parsed.netloc}{path_parts[0]}\"                source_reference = path_parts[1]            # Parse fragment for egg and subdirectory            name = None            subdirectory = None            if parsed.fragment:                params = parse_qs(parsed.fragment)                if \"egg\" in params:                    # Get package name from egg fragment                    egg = params[\"egg\"][0]                    # Remove any version specifier if present                    name = egg.split(\"-\")[0] if \"-\" in egg else egg                if \"subdirectory\" in params:                    subdirectory = params[\"subdirectory\"][0]            # If no name found in egg, try to get it from the path            if not name:                name = parsed.path.rstrip(\"/\").split(\"/\")[-1]                if name.endswith(\".git\"):                    name = name[:-4]            return base_url, source_reference, subdirectory, name        except Exception as e:            logger.warning(f\"Error parsing VCS URL '{url}': {str(e)}\")            return None, None, None, None    "},{"Name":"_process_git_source","Parameters":[{"TypeValue":"dependencies","TypeType":"Dict[str,UnoplatProjectDependency]"},{"TypeValue":"pkg_name","TypeType":"str"},{"TypeValue":"source_info","TypeType":"dict"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"warning","Position":{"StartLine":423,"StartLinePosition":18,"StopLine":423,"StopLinePosition":82}}],"Position":{"StartLine":374,"StartLinePosition":4,"StopLine":424},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":""},{"TypeValue":"pyproject_data","TypeType":"tomllib"},{"TypeValue":"project_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":""},{"TypeValue":"","TypeType":"self"},{"TypeValue":"version","TypeType":"self"},{"TypeValue":"dependencies[name]","TypeType":"UnoplatProjectDependency"},{"TypeValue":"optional_deps","TypeType":"project_data"},{"TypeValue":"existing_dep","TypeType":"dependencies"},{"TypeValue":"existing_groups","TypeType":"[existing_dep.group]"},{"TypeValue":"existing_dep.group","TypeType":"\",\""},{"TypeValue":"existing_extras","TypeType":""},{"TypeValue":"existing_dep.extras","TypeType":"list"},{"TypeValue":"uv_config","TypeType":"pyproject_data"},{"TypeValue":"sources","TypeType":"uv_config"},{"TypeValue":"dependencies[pkg_name].source","TypeType":"\"git\""},{"TypeValue":"dependencies[pkg_name].source_url","TypeType":"source_url"},{"TypeValue":"dep_metadata","TypeType":"uv_config"},{"TypeValue":"name","TypeType":"name"},{"TypeValue":"dependencies[name].environment_marker","TypeType":"f\"python_version {metadata_entry['requires-python']}\""},{"TypeValue":"package_part","TypeType":"dep_string"},{"TypeValue":"version_part","TypeType":"f\"{operator}{version_part.strip()}\""},{"TypeValue":"extras","TypeType":"[e.strip()foreinextras_str.split(\",\")]"},{"TypeValue":"extras_str","TypeType":"extras_str"},{"TypeValue":"entry_points","TypeType":"{}"},{"TypeValue":"scripts","TypeType":"project_data"},{"TypeValue":"console_scripts","TypeType":"project_data"},{"TypeValue":"parsed","TypeType":"urlsplit"},{"TypeValue":"base_url","TypeType":"f\"{parsed.scheme}://{parsed.netloc}{path_parts[0]}\""},{"TypeValue":"source_reference","TypeType":"path_parts"},{"TypeValue":"path_parts","TypeType":"parsed"},{"TypeValue":"subdirectory","TypeType":"params"},{"TypeValue":"params","TypeType":"parse_qs"},{"TypeValue":"egg","TypeType":"params"},{"TypeValue":"git_url","TypeType":"source_info"},{"TypeValue":"dependencies[pkg_name].source_reference","TypeType":"source_reference"},{"TypeValue":"dependencies[pkg_name].version","TypeType":"self"},{"TypeValue":"dependencies[pkg_name].subdirectory","TypeType":"subdirectory"}],"Content":"def _process_git_source(self, dependencies: Dict[str, UnoplatProjectDependency], pkg_name: str, source_info: dict) -> None:        \"\"\"Process git source information for a dependency.        Updates the dependency object with git source information including URL,        reference (tag/branch/commit), and subdirectory if specified.        Example source_info:            {                \"git\": \"https://github.com/user/repo.git\",                \"tag\": \"v1.0.0\",                \"subdirectory\": \"python\"            }        Args:            dependencies: Dictionary of dependencies to update            pkg_name: Name of the package to update            source_info: Git source configuration from pyproject.toml        Note:            Modifies the dependency object in place with git source information            If a tag is specified, also updates the version information        \"\"\"        try:            git_url = source_info[\"git\"]            source_url, source_reference, subdirectory, _ = self._parse_vcs_url(git_url)            if source_url:                dependencies[pkg_name].source = \"git\"                dependencies[pkg_name].source_url = source_url                # Use explicit tag/branch/rev if provided, otherwise use parsed reference                if \"tag\" in source_info:                    dependencies[pkg_name].source_reference = source_info[\"tag\"]                    # Update version if tag is specified                    dependencies[pkg_name].version = self._parse_version_constraint(f\"=={source_info['tag']}\")                elif \"branch\" in source_info:                    dependencies[pkg_name].source_reference = source_info[\"branch\"]                elif \"rev\" in source_info:                    dependencies[pkg_name].source_reference = source_info[\"rev\"]                elif source_reference:                    dependencies[pkg_name].source_reference = source_reference                # Use explicit subdirectory if provided, otherwise use parsed subdirectory                if \"subdirectory\" in source_info:                    dependencies[pkg_name].subdirectory = source_info[\"subdirectory\"]                elif subdirectory:                    dependencies[pkg_name].subdirectory = subdirectory        except Exception as e:            logger.warning(f\"Error processing git source for {pkg_name}: {str(e)}\")"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_project_dependency","UsageName":["UnoplatProjectDependency"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_version","UsageName":["UnoplatVersion"]},{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["ProgrammingLanguageMetadata"]},{"Source":"src.code_confluence_flow_bridge.parser.package_manager.package_manager_strategy","UsageName":["PackageManagerStrategy"]},{"Source":"pathlib","UsageName":["Path"]},{"Source":"tomllib"},{"Source":"typing","UsageName":["Dict","List","Optional"]},{"Source":"urllib.parse","UsageName":["parse_qs","urlsplit"]},{"Source":"loguru","UsageName":["logger"]}],"Position":{"StartLine":28,"StopLine":424},"Content":"class UvStrategy(PackageManagerStrategy):    def process_metadata(self, local_workspace_path: str, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Process UV project metadata from pyproject.toml file.        Parses a pyproject.toml file to extract package metadata and dependencies.        Handles main dependencies, optional dependencies, git sources, and environment markers.        Example pyproject.toml:            [project]            name = \"my-package\"            version = \"1.0.0\"            dependencies = [                \"requests>=2.25.0\",                \"pandas[excel]>=1.2.0\"            ]            [project.optional-dependencies]            test = [\"pytest>=6.0.0\"]            [tool.uv.sources]            my-pkg = { git = \"https://github.com/user/repo.git\", tag = \"v1.0.0\" }        Args:            local_workspace_path: Path to the directory containing pyproject.toml            metadata: Programming language metadata configuration        Returns:            UnoplatPackageManagerMetadata containing all parsed project information        Raises:            ValueError: If pyproject.toml exists but cannot be parsed        \"\"\"        pyproject_path = Path(local_workspace_path) / \"pyproject.toml\"        if not pyproject_path.exists():            return UnoplatPackageManagerMetadata(programming_language=\"python\", package_manager=\"uv\")        try:            with open(pyproject_path, \"rb\") as f:                pyproject_data = tomllib.load(f)        except Exception as e:            raise ValueError(f\"Failed to parse pyproject.toml: {str(e)}\")        # Extract project metadata        project_data = pyproject_data.get(\"project\", {})        # Parse dependencies        dependencies: Dict[str, UnoplatProjectDependency] = {}        # Process main dependencies        for dep in project_data.get(\"dependencies\", []):            try:                name, version_str, extras = self._parse_dependency(dep)                version = self._parse_version_constraint(version_str) if version_str else UnoplatVersion()                dependencies[name] = UnoplatProjectDependency(                    version=version,                    extras=extras,  # Now properly handling extras                )            except Exception as e:                logger.warning(f\"Error processing dependency {dep}: {str(e)}\")                continue        # Process optional dependencies (extras)        optional_deps = project_data.get(\"optional-dependencies\", {})        for group_name, deps in optional_deps.items():            for dep in deps:                try:                    name, version_str, extras = self._parse_dependency(dep)                    version = self._parse_version_constraint(version_str) if version_str else UnoplatVersion()                    if name not in dependencies:                        dependencies[name] = UnoplatProjectDependency(                            version=version,                            group=group_name,                            extras=extras,  # Include extras                        )                    else:                        # Update existing dependency                        existing_dep = dependencies[name]                        if existing_dep.group != group_name:                            existing_groups = [existing_dep.group] if existing_dep.group else []                            existing_groups.append(group_name)                            existing_dep.group = \",\".join(existing_groups)                        if extras:                            existing_extras = existing_dep.extras or []                            existing_dep.extras = list(set(existing_extras + extras))                except Exception as e:                    logger.warning(f\"Error processing optional dependency {dep} in group {group_name}: {str(e)}\")                    continue        # Process UV-specific sources        uv_config = pyproject_data.get(\"tool\", {}).get(\"uv\", {})        sources = uv_config.get(\"sources\", {})        for pkg_name, source_info in sources.items():            if pkg_name in dependencies:                try:                    if \"git\" in source_info:                        self._process_git_source(dependencies, pkg_name, source_info)                    elif \"index\" in source_info:                        dependencies[pkg_name].source = \"index\"                        dependencies[pkg_name].source_url = self._get_index_url(pyproject_data, source_info[\"index\"])                except Exception as e:                    logger.warning(f\"Error processing source for {pkg_name}: {str(e)}\")                    continue        # Process dependency metadata for environment markers        dep_metadata = uv_config.get(\"dependency-metadata\", [])        for metadata_entry in dep_metadata:            name = metadata_entry.get(\"name\")            if name in dependencies:                try:                    # Add environment markers if specified                    if \"requires-python\" in metadata_entry:                        dependencies[name].environment_marker = f\"python_version {metadata_entry['requires-python']}\"                except Exception as e:                    logger.warning(f\"Error processing dependency metadata for {name}: {str(e)}\")                    continue        return UnoplatPackageManagerMetadata(            dependencies=dependencies,            package_name=project_data.get(\"name\"),            programming_language=\"python\",            package_manager=\"uv\",            programming_language_version=project_data.get(\"requires-python\"),            project_version=project_data.get(\"version\"),            description=project_data.get(\"description\"),            authors=project_data.get(\"authors\", []),            license=project_data.get(\"license\"),            entry_points=self._parse_entry_points(project_data),            homepage=project_data.get(\"homepage\"),            repository=project_data.get(\"repository\"),            documentation=project_data.get(\"documentation\"),            keywords=project_data.get(\"keywords\", []),            maintainers=project_data.get(\"maintainers\", []),            readme=project_data.get(\"readme\"),        )    def _parse_dependency(self, dep_string: str) -> tuple[str, Optional[str], Optional[List[str]]]:        \"\"\"Parse a dependency string into name, version, and extras.        Handles various dependency formats including:        - Simple version specifiers: \"requests>=2.25.0\"        - Multiple version specifiers: \"requests>=2.25.0,<3.0.0\"        - Extras: \"pandas[excel,csv]>=1.2.0\"        Examples:            >>> _parse_dependency(\"requests>=2.25.0\")            (\"requests\", \">=2.25.0\", None)            >>> _parse_dependency(\"pandas[excel,csv]>=1.2.0\")            (\"pandas\", \">=1.2.0\", [\"excel\", \"csv\"])            >>> _parse_dependency(\"flask\")            (\"flask\", None, None)        Args:            dep_string: The dependency string to parse        Returns:            tuple containing:            - str: Package name            - Optional[str]: Version specifier if present            - Optional[List[str]]: List of extras if present        \"\"\"        try:            # First split on version operators to get the package part            package_part = dep_string            version_part = None            for operator in (\">=\", \"==\", \"<=\", \"<\", \">\"):                if operator in dep_string:                    package_part, version_part = dep_string.split(operator, 1)                    version_part = f\"{operator}{version_part.strip()}\"                    break            # Now handle extras in the package part            extras = None            name = package_part.strip()            if \"[\" in name and \"]\" in name:                name, extras_str = name.split(\"[\", 1)                extras_str = extras_str.split(\"]\")[0]                extras = [e.strip() for e in extras_str.split(\",\")]                name = name.strip()            return name, version_part, extras        except Exception as e:            logger.warning(f\"Error parsing dependency '{dep_string}': {str(e)}\")            return dep_string.strip(), None, None    def _parse_version_constraint(self, constraint: str) -> UnoplatVersion:        \"\"\"Parse version constraint into a specifier string.        Handles various version constraint formats:        - Simple constraints: \"==1.0.0\", \">=2.0.0\"        - Complex constraints: \">=1.0.0,<2.0.0\"        - Compatible release: \"~=1.0.0\"        Examples:            >>> _parse_version_constraint(\">=1.0.0\")            UnoplatVersion(specifier=\">=1.0.0\")            >>> _parse_version_constraint(\">=1.0.0,<2.0.0\")            UnoplatVersion(specifier=\">=1.0.0,<2.0.0\")            >>> _parse_version_constraint(\"==1.0.0\")            UnoplatVersion(specifier=\"==1.0.0\")        Args:            constraint: Version constraint string        Returns:            UnoplatVersion with specifier field set        \"\"\"        if not constraint or constraint == \"*\":            return UnoplatVersion()        try:            # Simply store the raw version constraint as specifier            return UnoplatVersion(specifier=constraint)        except Exception as e:            logger.warning(f\"Error parsing version constraint '{constraint}': {str(e)}\")            return UnoplatVersion()    def _get_index_url(self, pyproject_data: dict, index_name: str) -> Optional[str]:        \"\"\"Get the URL for a named custom package index from UV configuration.        Example pyproject.toml:            [[tool.uv.index]]            name = \"private-pypi\"            url = \"https://pypi.internal.company.com/simple\"        Args:            pyproject_data: Parsed pyproject.toml data            index_name: Name of the index to look up        Returns:            URL string if index is found, None otherwise        \"\"\"        uv_config = pyproject_data.get(\"tool\", {}).get(\"uv\", {})        for index in uv_config.get(\"index\", []):            if index.get(\"name\") == index_name:                return index.get(\"url\")        return None    def _parse_entry_points(self, project_data: dict) -> Dict[str, str]:        \"\"\"Parse entry points from project configuration.        Handles both script entries and console_scripts entry points.        Example pyproject.toml:            [project.scripts]            my-cli = \"my_package.cli:main\"            [project.entry-points.\"console_scripts\"]            another-cli = \"my_package.another:main\"        Args:            project_data: Project section from pyproject.toml        Returns:            Dictionary mapping script names to their entry points            Example: {\"my-cli\": \"my_package.cli:main\", \"another-cli\": \"my_package.another:main\"}        \"\"\"        entry_points = {}        # Parse scripts section        scripts = project_data.get(\"scripts\", {})        entry_points.update(scripts)        # Parse console_scripts entry points        console_scripts = project_data.get(\"entry-points\", {}).get(\"console_scripts\", {})        entry_points.update(console_scripts)        return entry_points    def _parse_vcs_url(self, url: str) -> tuple[Optional[str], Optional[str], Optional[str], Optional[str]]:        \"\"\"Parse VCS URL into components using urllib.parse.        Handles various VCS URL formats:        - Simple git URLs: \"git+https://github.com/user/repo.git\"        - URLs with references: \"git+https://github.com/user/repo.git@main\"        - URLs with extras: \"git+https://github.com/user/repo.git#egg=package&subdirectory=src\"        Examples:            >>> _parse_vcs_url(\"git+https://github.com/user/repo.git\")            (\"https://github.com/user/repo\", None, None, \"repo\")            >>> _parse_vcs_url(\"git+https://github.com/user/repo.git@main#egg=mypackage\")            (\"https://github.com/user/repo\", \"main\", None, \"mypackage\")            >>> _parse_vcs_url(\"git+https://github.com/user/repo.git#egg=pkg&subdirectory=src\")            (\"https://github.com/user/repo\", None, \"src\", \"pkg\")        Args:            url: VCS URL to parse        Returns:            tuple containing:            - Optional[str]: Base URL without VCS prefix            - Optional[str]: Reference (branch, tag, or commit)            - Optional[str]: Subdirectory path if specified            - Optional[str]: Package name from egg fragment or URL path        \"\"\"        try:            # Parse URL into components            parsed = urlsplit(url)            # Get base URL without VCS prefix            if \"+\" in parsed.scheme:                _, base_scheme = parsed.scheme.split(\"+\", 1)                base_url = f\"{base_scheme}://{parsed.netloc}{parsed.path}\"            else:                base_url = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"            # Remove .git suffix if present            if base_url.endswith(\".git\"):                base_url = base_url[:-4]            # Extract reference from path if present            source_reference = None            if \"@\" in parsed.path:                path_parts = parsed.path.rsplit(\"@\", 1)                base_url = f\"{parsed.scheme}://{parsed.netloc}{path_parts[0]}\"                source_reference = path_parts[1]            # Parse fragment for egg and subdirectory            name = None            subdirectory = None            if parsed.fragment:                params = parse_qs(parsed.fragment)                if \"egg\" in params:                    # Get package name from egg fragment                    egg = params[\"egg\"][0]                    # Remove any version specifier if present                    name = egg.split(\"-\")[0] if \"-\" in egg else egg                if \"subdirectory\" in params:                    subdirectory = params[\"subdirectory\"][0]            # If no name found in egg, try to get it from the path            if not name:                name = parsed.path.rstrip(\"/\").split(\"/\")[-1]                if name.endswith(\".git\"):                    name = name[:-4]            return base_url, source_reference, subdirectory, name        except Exception as e:            logger.warning(f\"Error parsing VCS URL '{url}': {str(e)}\")            return None, None, None, None    def _process_git_source(self, dependencies: Dict[str, UnoplatProjectDependency], pkg_name: str, source_info: dict) -> None:        \"\"\"Process git source information for a dependency.        Updates the dependency object with git source information including URL,        reference (tag/branch/commit), and subdirectory if specified.        Example source_info:            {                \"git\": \"https://github.com/user/repo.git\",                \"tag\": \"v1.0.0\",                \"subdirectory\": \"python\"            }        Args:            dependencies: Dictionary of dependencies to update            pkg_name: Name of the package to update            source_info: Git source configuration from pyproject.toml        Note:            Modifies the dependency object in place with git source information            If a tag is specified, also updates the version information        \"\"\"        try:            git_url = source_info[\"git\"]            source_url, source_reference, subdirectory, _ = self._parse_vcs_url(git_url)            if source_url:                dependencies[pkg_name].source = \"git\"                dependencies[pkg_name].source_url = source_url                # Use explicit tag/branch/rev if provided, otherwise use parsed reference                if \"tag\" in source_info:                    dependencies[pkg_name].source_reference = source_info[\"tag\"]                    # Update version if tag is specified                    dependencies[pkg_name].version = self._parse_version_constraint(f\"=={source_info['tag']}\")                elif \"branch\" in source_info:                    dependencies[pkg_name].source_reference = source_info[\"branch\"]                elif \"rev\" in source_info:                    dependencies[pkg_name].source_reference = source_info[\"rev\"]                elif source_reference:                    dependencies[pkg_name].source_reference = source_reference                # Use explicit subdirectory if provided, otherwise use parsed subdirectory                if \"subdirectory\" in source_info:                    dependencies[pkg_name].subdirectory = source_info[\"subdirectory\"]                elif subdirectory:                    dependencies[pkg_name].subdirectory = subdirectory        except Exception as e:            logger.warning(f\"Error processing git source for {pkg_name}: {str(e)}\")"},{"NodeName":"PythonPoetryStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/package_manager/poetry/poetry_strategy.py","MultipleExtend":["PackageManagerStrategy"],"Functions":[{"Name":"process_metadata","Parameters":[{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"metadata","TypeType":"ProgrammingLanguageMetadata"}],"FunctionCalls":[{"NodeName":"activity","FunctionName":"logger","Position":{"StartLine":95,"StartLinePosition":20,"StopLine":95,"StopLinePosition":21}},{"NodeName":"activity","FunctionName":"error","Position":{"StartLine":95,"StartLinePosition":27,"StopLine":95,"StopLinePosition":107}}],"Position":{"StartLine":37,"StartLinePosition":4,"StopLine":98,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":"os"},{"TypeValue":"pyproject_content","TypeType":"f"},{"TypeValue":"pyproject_data","TypeType":"tomlkit"},{"TypeValue":"validator","TypeType":"validator_api"},{"TypeValue":"poetry_data","TypeType":"pyproject_data"},{"TypeValue":"project_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":"self"},{"TypeValue":"programming_language_version","TypeType":"(project_data.get(\"requires-python\")orpoetry_data.get(\"dependencies\",{}).get(\"python\")ormetadata.language_version)"}],"Content":"def process_metadata(self, local_workspace_path: str, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Process poetry specific metadata from pyproject.toml\"\"\"        pyproject_path = os.path.join(local_workspace_path, \"pyproject.toml\")        if not os.path.exists(pyproject_path):            activity.logger.warning(\"pyproject.toml not found\", {\"path\": pyproject_path})            return self._create_empty_metadata(metadata)        try:            with open(pyproject_path, \"r\") as f:                pyproject_content = f.read()                pyproject_data = tomlkit.parse(pyproject_content)            # Validate PEP 621 compliance            try:                validator = validator_api.Validator()                validator(pyproject_data)            except ValidationError as e:                activity.logger.warning(\"pyproject.toml validation failed\", {\"error\": e.message, \"path\": pyproject_path})                # Continue processing even if validation fails, but log the warning            poetry_data = pyproject_data.get(\"tool\", {}).get(\"poetry\", {})            project_data = pyproject_data.get(\"project\", {})            if not poetry_data:                return self._handle_fallback(local_workspace_path, metadata)            # Parse dependencies from all groups            dependencies = self._parse_all_dependency_groups(poetry_data)            # Get Python version requirement - check PEP 621 first, then Poetry            programming_language_version = (                project_data.get(\"requires-python\")  # PEP 621 format                or poetry_data.get(\"dependencies\", {}).get(\"python\")  # Poetry format                or metadata.language_version  # Fallback            )            # Create metadata object with all available fields            return UnoplatPackageManagerMetadata(                dependencies=dependencies,                package_name=poetry_data.get(\"name\"),                programming_language=metadata.language.value,                package_manager=metadata.package_manager,                programming_language_version=programming_language_version,                project_version=poetry_data.get(\"version\"),                description=poetry_data.get(\"description\"),                authors=poetry_data.get(\"authors\"),                entry_points=self._get_entry_points(poetry_data.get(\"scripts\", {})),                license=poetry_data.get(\"license\"),                homepage=poetry_data.get(\"homepage\"),                repository=poetry_data.get(\"repository\"),                documentation=poetry_data.get(\"documentation\"),                keywords=poetry_data.get(\"keywords\", []),                maintainers=poetry_data.get(\"maintainers\", []),                readme=poetry_data.get(\"readme\"),            )        except Exception as e:            activity.logger.error(\"Error parsing pyproject.toml\", {\"error\": str(e), \"path\": pyproject_path})            return self._create_empty_metadata(metadata)    "},{"Name":"_handle_fallback","Parameters":[{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"metadata","TypeType":"ProgrammingLanguageMetadata"}],"FunctionCalls":[{"NodeName":"activity","FunctionName":"logger","Position":{"StartLine":106,"StartLinePosition":20,"StopLine":106,"StopLinePosition":21}},{"NodeName":"activity","FunctionName":"warning","Position":{"StartLine":106,"StartLinePosition":27,"StopLine":106,"StopLinePosition":83}}],"Position":{"StartLine":98,"StartLinePosition":4,"StopLine":109,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":"os"},{"TypeValue":"pyproject_content","TypeType":"f"},{"TypeValue":"pyproject_data","TypeType":"tomlkit"},{"TypeValue":"validator","TypeType":"validator_api"},{"TypeValue":"poetry_data","TypeType":"pyproject_data"},{"TypeValue":"project_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":"RequirementsUtils"},{"TypeValue":"programming_language_version","TypeType":"(project_data.get(\"requires-python\")orpoetry_data.get(\"dependencies\",{}).get(\"python\")ormetadata.language_version)"},{"TypeValue":"package_manager","TypeType":"UnoplatPackageManagerMetadata"}],"Content":"def _handle_fallback(self, local_workspace_path: str, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Handle fallback to requirements.txt when no poetry config is found\"\"\"        activity.logger.warning(\"No poetry configuration found, falling back to requirements\", {\"path\": local_workspace_path, \"package_manager\": metadata.package_manager.value})        dependencies = RequirementsUtils.parse_requirements_folder(local_workspace_path)        package_manager = UnoplatPackageManagerMetadata(dependencies=dependencies, programming_language=metadata.language.value, package_manager=PackageManagerType.PIP.value)        try:            return SetupParser.parse_setup_file(local_workspace_path, package_manager)        except FileNotFoundError:            activity.logger.warning(\"setup.py not found, skipping setup.py parsing\")            return package_manager    "},{"Name":"_parse_all_dependency_groups","Parameters":[{"TypeValue":"poetry_data","TypeType":"Dict"}],"FunctionCalls":[{"NodeName":"all_dependencies","FunctionName":"update","Position":{"StartLine":127,"StartLinePosition":32,"StopLine":127,"StopLinePosition":50}}],"Position":{"StartLine":109,"StartLinePosition":4,"StopLine":131,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":"os"},{"TypeValue":"pyproject_content","TypeType":"f"},{"TypeValue":"pyproject_data","TypeType":"tomlkit"},{"TypeValue":"validator","TypeType":"validator_api"},{"TypeValue":"poetry_data","TypeType":"pyproject_data"},{"TypeValue":"project_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":"RequirementsUtils"},{"TypeValue":"programming_language_version","TypeType":"(project_data.get(\"requires-python\")orpoetry_data.get(\"dependencies\",{}).get(\"python\")ormetadata.language_version)"},{"TypeValue":"package_manager","TypeType":"UnoplatPackageManagerMetadata"},{"TypeValue":"all_dependencies","TypeType":"{}"},{"TypeValue":"main_deps","TypeType":"poetry_data"},{"TypeValue":"dev_deps","TypeType":"poetry_data"},{"TypeValue":"groups","TypeType":"poetry_data"},{"TypeValue":"group_deps","TypeType":"self"}],"Content":"def _parse_all_dependency_groups(self, poetry_data: Dict) -> Dict[str, UnoplatProjectDependency]:        \"\"\"Parse dependencies from all groups including main, dev, and custom groups\"\"\"        all_dependencies = {}        # Parse main dependencies        main_deps = poetry_data.get(\"dependencies\", {})        all_dependencies.update(self._parse_dependencies(main_deps))        # Parse dev dependencies (legacy format)        dev_deps = poetry_data.get(\"dev-dependencies\", {})        if dev_deps:            all_dependencies.update(self._parse_dependencies(dev_deps, group=\"dev\"))        # Parse group dependencies (Poetry 1.2+ format)        groups = poetry_data.get(\"group\", {})        for group_name, group_data in groups.items():            if isinstance(group_data, dict) and \"dependencies\" in group_data:                group_deps = self._parse_dependencies(group_data[\"dependencies\"], group=group_name)                all_dependencies.update(group_deps)        return all_dependencies    "},{"Name":"_parse_version_constraint","Parameters":[{"TypeValue":"constraint","TypeType":"str"}],"FunctionCalls":[{"NodeName":"activity","FunctionName":"logger","Position":{"StartLine":141,"StartLinePosition":20,"StopLine":141,"StopLinePosition":21}},{"NodeName":"activity","FunctionName":"warning","Position":{"StartLine":141,"StartLinePosition":27,"StopLine":141,"StopLinePosition":96}}],"Position":{"StartLine":131,"StartLinePosition":4,"StopLine":144,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":"os"},{"TypeValue":"pyproject_content","TypeType":"f"},{"TypeValue":"pyproject_data","TypeType":"tomlkit"},{"TypeValue":"validator","TypeType":"validator_api"},{"TypeValue":"poetry_data","TypeType":"pyproject_data"},{"TypeValue":"project_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":"RequirementsUtils"},{"TypeValue":"programming_language_version","TypeType":"(project_data.get(\"requires-python\")orpoetry_data.get(\"dependencies\",{}).get(\"python\")ormetadata.language_version)"},{"TypeValue":"package_manager","TypeType":"UnoplatPackageManagerMetadata"},{"TypeValue":"all_dependencies","TypeType":"{}"},{"TypeValue":"main_deps","TypeType":"poetry_data"},{"TypeValue":"dev_deps","TypeType":"poetry_data"},{"TypeValue":"groups","TypeType":"poetry_data"},{"TypeValue":"group_deps","TypeType":"self"}],"Content":"def _parse_version_constraint(self, constraint: str) -> UnoplatVersion:        \"\"\"Parse version constraint using packaging library\"\"\"        if not constraint or constraint == \"*\":            return UnoplatVersion()        try:            # Simply store the raw version constraint as specifier            return UnoplatVersion(specifier=constraint)        except Exception as e:            activity.logger.warning(f\"Error parsing version constraint '{constraint}': {str(e)}\")            return UnoplatVersion()    "},{"Name":"_create_empty_metadata","Parameters":[{"TypeValue":"metadata","TypeType":"ProgrammingLanguageMetadata"}],"Position":{"StartLine":144,"StartLinePosition":4,"StopLine":148,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":"os"},{"TypeValue":"pyproject_content","TypeType":"f"},{"TypeValue":"pyproject_data","TypeType":"tomlkit"},{"TypeValue":"validator","TypeType":"validator_api"},{"TypeValue":"poetry_data","TypeType":"pyproject_data"},{"TypeValue":"project_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":"RequirementsUtils"},{"TypeValue":"programming_language_version","TypeType":"(project_data.get(\"requires-python\")orpoetry_data.get(\"dependencies\",{}).get(\"python\")ormetadata.language_version)"},{"TypeValue":"package_manager","TypeType":"UnoplatPackageManagerMetadata"},{"TypeValue":"all_dependencies","TypeType":"{}"},{"TypeValue":"main_deps","TypeType":"poetry_data"},{"TypeValue":"dev_deps","TypeType":"poetry_data"},{"TypeValue":"groups","TypeType":"poetry_data"},{"TypeValue":"group_deps","TypeType":"self"}],"Content":"def _create_empty_metadata(self, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Create empty metadata with basic information\"\"\"        return UnoplatPackageManagerMetadata(dependencies={}, programming_language=metadata.language.value, package_manager=metadata.package_manager)    "},{"Name":"_parse_dependencies","Parameters":[{"TypeValue":"deps_dict","TypeType":"Dict"},{"DefaultValue":"None","TypeValue":"group","TypeType":"Optional[str]"}],"Position":{"StartLine":148,"StartLinePosition":4,"StopLine":209,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":"os"},{"TypeValue":"pyproject_content","TypeType":"f"},{"TypeValue":"pyproject_data","TypeType":"tomlkit"},{"TypeValue":"validator","TypeType":"validator_api"},{"TypeValue":"poetry_data","TypeType":"pyproject_data"},{"TypeValue":"project_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":"{}"},{"TypeValue":"programming_language_version","TypeType":"(project_data.get(\"requires-python\")orpoetry_data.get(\"dependencies\",{}).get(\"python\")ormetadata.language_version)"},{"TypeValue":"package_manager","TypeType":"UnoplatPackageManagerMetadata"},{"TypeValue":"all_dependencies","TypeType":"{}"},{"TypeValue":"main_deps","TypeType":"poetry_data"},{"TypeValue":"dev_deps","TypeType":"poetry_data"},{"TypeValue":"groups","TypeType":"poetry_data"},{"TypeValue":"group_deps","TypeType":"self"},{"TypeValue":"version","TypeType":"UnoplatVersion"},{"TypeValue":"extras","TypeType":"constraint"},{"TypeValue":"source","TypeType":"\"url\""},{"TypeValue":"source_url","TypeType":"constraint"},{"TypeValue":"source_reference","TypeType":""},{"TypeValue":"subdirectory","TypeType":"constraint"},{"TypeValue":"tuple_dependency","TypeType":"UnoplatProjectDependency"},{"TypeValue":"dependencies[name]","TypeType":"tuple_dependency"}],"Content":"def _parse_dependencies(self, deps_dict: Dict, group: Optional[str] = None) -> Dict[str, UnoplatProjectDependency]:        dependencies = {}        for name, constraint in deps_dict.items():            if name == \"python\":  # Skip python version constraint                continue            # Initialize dependency fields            version = UnoplatVersion()            extras = None            source = None            source_url = None            source_reference = None            subdirectory = None            try:                # Parse different dependency specification formats                if isinstance(constraint, str):                    # Handle version constraints                    version = self._parse_version_constraint(constraint)                elif isinstance(constraint, dict):                    # Complex dependency specification                    if \"version\" in constraint:                        version = self._parse_version_constraint(constraint[\"version\"])                    extras = constraint.get(\"extras\")                    # Handle git dependencies                    if \"git\" in constraint:                        source = \"git\"                        source_url = constraint[\"git\"]                        source_reference = constraint.get(\"rev\") or constraint.get(\"branch\") or constraint.get(\"tag\")                        subdirectory = constraint.get(\"subdirectory\")                        version = UnoplatVersion()  # Git dependencies don't have version constraints                    # Handle path dependencies                    elif \"path\" in constraint:                        source = \"path\"                        source_url = constraint[\"path\"]                        version = UnoplatVersion()  # Path dependencies don't have version constraints                    # Handle url dependencies                    elif \"url\" in constraint:                        source = \"url\"                        source_url = constraint[\"url\"]                        version = UnoplatVersion()  # URL dependencies don't have version constraints                else:                    activity.logger.warning(f\"Skipping invalid dependency specification for {name}\")                    continue                tuple_dependency = UnoplatProjectDependency(version=version, extras=extras, source=source, source_url=source_url, source_reference=source_reference, subdirectory=subdirectory)                dependencies[name] = tuple_dependency            except Exception as e:                activity.logger.warning(\"Error parsing dependency\", {\"dependency\": name, \"error\": str(e), \"group\": group})                # Add dependency with empty version constraint                tuple_dependency = UnoplatProjectDependency(version=UnoplatVersion())                dependencies[name] = tuple_dependency        return dependencies    "},{"Name":"_get_entry_points","Parameters":[{"TypeValue":"scripts","TypeType":"Dict[str,str]"}],"Position":{"StartLine":209,"StartLinePosition":4,"StopLine":212},"LocalVariables":[{"TypeValue":"pyproject_path","TypeType":"os"},{"TypeValue":"pyproject_content","TypeType":"f"},{"TypeValue":"pyproject_data","TypeType":"tomlkit"},{"TypeValue":"validator","TypeType":"validator_api"},{"TypeValue":"poetry_data","TypeType":"pyproject_data"},{"TypeValue":"project_data","TypeType":"pyproject_data"},{"TypeValue":"dependencies","TypeType":"{}"},{"TypeValue":"programming_language_version","TypeType":"(project_data.get(\"requires-python\")orpoetry_data.get(\"dependencies\",{}).get(\"python\")ormetadata.language_version)"},{"TypeValue":"package_manager","TypeType":"UnoplatPackageManagerMetadata"},{"TypeValue":"all_dependencies","TypeType":"{}"},{"TypeValue":"main_deps","TypeType":"poetry_data"},{"TypeValue":"dev_deps","TypeType":"poetry_data"},{"TypeValue":"groups","TypeType":"poetry_data"},{"TypeValue":"group_deps","TypeType":"self"},{"TypeValue":"version","TypeType":"UnoplatVersion"},{"TypeValue":"extras","TypeType":"constraint"},{"TypeValue":"source","TypeType":"\"url\""},{"TypeValue":"source_url","TypeType":"constraint"},{"TypeValue":"source_reference","TypeType":""},{"TypeValue":"subdirectory","TypeType":"constraint"},{"TypeValue":"tuple_dependency","TypeType":"UnoplatProjectDependency"},{"TypeValue":"dependencies[name]","TypeType":"tuple_dependency"}],"Content":"def _get_entry_points(self, scripts: Dict[str, str]) -> Dict[str, str]:        \"\"\"Get all entry points from Poetry scripts section.\"\"\"        return scripts if scripts else {}"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_project_dependency","UsageName":["UnoplatProjectDependency"]},{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_version","UsageName":["UnoplatVersion"]},{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["PackageManagerType","ProgrammingLanguageMetadata"]},{"Source":"src.code_confluence_flow_bridge.parser.package_manager.package_manager_strategy","UsageName":["PackageManagerStrategy"]},{"Source":"src.code_confluence_flow_bridge.parser.package_manager.utils.requirements_utils","UsageName":["RequirementsUtils"]},{"Source":"src.code_confluence_flow_bridge.parser.package_manager.utils.setup_parser","UsageName":["SetupParser"]},{"Source":"os"},{"Source":"typing","UsageName":["Dict","Optional"]},{"Source":"temporalio","UsageName":["activity"]},{"Source":"tomlkit"},{"Source":"validate_pyproject","AsName":"validator_api","UsageName":["api"]},{"Source":"validate_pyproject.errors","UsageName":["ValidationError"]}],"Position":{"StartLine":36,"StopLine":212},"Content":"class PythonPoetryStrategy(PackageManagerStrategy):    def process_metadata(self, local_workspace_path: str, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Process poetry specific metadata from pyproject.toml\"\"\"        pyproject_path = os.path.join(local_workspace_path, \"pyproject.toml\")        if not os.path.exists(pyproject_path):            activity.logger.warning(\"pyproject.toml not found\", {\"path\": pyproject_path})            return self._create_empty_metadata(metadata)        try:            with open(pyproject_path, \"r\") as f:                pyproject_content = f.read()                pyproject_data = tomlkit.parse(pyproject_content)            # Validate PEP 621 compliance            try:                validator = validator_api.Validator()                validator(pyproject_data)            except ValidationError as e:                activity.logger.warning(\"pyproject.toml validation failed\", {\"error\": e.message, \"path\": pyproject_path})                # Continue processing even if validation fails, but log the warning            poetry_data = pyproject_data.get(\"tool\", {}).get(\"poetry\", {})            project_data = pyproject_data.get(\"project\", {})            if not poetry_data:                return self._handle_fallback(local_workspace_path, metadata)            # Parse dependencies from all groups            dependencies = self._parse_all_dependency_groups(poetry_data)            # Get Python version requirement - check PEP 621 first, then Poetry            programming_language_version = (                project_data.get(\"requires-python\")  # PEP 621 format                or poetry_data.get(\"dependencies\", {}).get(\"python\")  # Poetry format                or metadata.language_version  # Fallback            )            # Create metadata object with all available fields            return UnoplatPackageManagerMetadata(                dependencies=dependencies,                package_name=poetry_data.get(\"name\"),                programming_language=metadata.language.value,                package_manager=metadata.package_manager,                programming_language_version=programming_language_version,                project_version=poetry_data.get(\"version\"),                description=poetry_data.get(\"description\"),                authors=poetry_data.get(\"authors\"),                entry_points=self._get_entry_points(poetry_data.get(\"scripts\", {})),                license=poetry_data.get(\"license\"),                homepage=poetry_data.get(\"homepage\"),                repository=poetry_data.get(\"repository\"),                documentation=poetry_data.get(\"documentation\"),                keywords=poetry_data.get(\"keywords\", []),                maintainers=poetry_data.get(\"maintainers\", []),                readme=poetry_data.get(\"readme\"),            )        except Exception as e:            activity.logger.error(\"Error parsing pyproject.toml\", {\"error\": str(e), \"path\": pyproject_path})            return self._create_empty_metadata(metadata)    def _handle_fallback(self, local_workspace_path: str, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Handle fallback to requirements.txt when no poetry config is found\"\"\"        activity.logger.warning(\"No poetry configuration found, falling back to requirements\", {\"path\": local_workspace_path, \"package_manager\": metadata.package_manager.value})        dependencies = RequirementsUtils.parse_requirements_folder(local_workspace_path)        package_manager = UnoplatPackageManagerMetadata(dependencies=dependencies, programming_language=metadata.language.value, package_manager=PackageManagerType.PIP.value)        try:            return SetupParser.parse_setup_file(local_workspace_path, package_manager)        except FileNotFoundError:            activity.logger.warning(\"setup.py not found, skipping setup.py parsing\")            return package_manager    def _parse_all_dependency_groups(self, poetry_data: Dict) -> Dict[str, UnoplatProjectDependency]:        \"\"\"Parse dependencies from all groups including main, dev, and custom groups\"\"\"        all_dependencies = {}        # Parse main dependencies        main_deps = poetry_data.get(\"dependencies\", {})        all_dependencies.update(self._parse_dependencies(main_deps))        # Parse dev dependencies (legacy format)        dev_deps = poetry_data.get(\"dev-dependencies\", {})        if dev_deps:            all_dependencies.update(self._parse_dependencies(dev_deps, group=\"dev\"))        # Parse group dependencies (Poetry 1.2+ format)        groups = poetry_data.get(\"group\", {})        for group_name, group_data in groups.items():            if isinstance(group_data, dict) and \"dependencies\" in group_data:                group_deps = self._parse_dependencies(group_data[\"dependencies\"], group=group_name)                all_dependencies.update(group_deps)        return all_dependencies    def _parse_version_constraint(self, constraint: str) -> UnoplatVersion:        \"\"\"Parse version constraint using packaging library\"\"\"        if not constraint or constraint == \"*\":            return UnoplatVersion()        try:            # Simply store the raw version constraint as specifier            return UnoplatVersion(specifier=constraint)        except Exception as e:            activity.logger.warning(f\"Error parsing version constraint '{constraint}': {str(e)}\")            return UnoplatVersion()    def _create_empty_metadata(self, metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Create empty metadata with basic information\"\"\"        return UnoplatPackageManagerMetadata(dependencies={}, programming_language=metadata.language.value, package_manager=metadata.package_manager)    def _parse_dependencies(self, deps_dict: Dict, group: Optional[str] = None) -> Dict[str, UnoplatProjectDependency]:        dependencies = {}        for name, constraint in deps_dict.items():            if name == \"python\":  # Skip python version constraint                continue            # Initialize dependency fields            version = UnoplatVersion()            extras = None            source = None            source_url = None            source_reference = None            subdirectory = None            try:                # Parse different dependency specification formats                if isinstance(constraint, str):                    # Handle version constraints                    version = self._parse_version_constraint(constraint)                elif isinstance(constraint, dict):                    # Complex dependency specification                    if \"version\" in constraint:                        version = self._parse_version_constraint(constraint[\"version\"])                    extras = constraint.get(\"extras\")                    # Handle git dependencies                    if \"git\" in constraint:                        source = \"git\"                        source_url = constraint[\"git\"]                        source_reference = constraint.get(\"rev\") or constraint.get(\"branch\") or constraint.get(\"tag\")                        subdirectory = constraint.get(\"subdirectory\")                        version = UnoplatVersion()  # Git dependencies don't have version constraints                    # Handle path dependencies                    elif \"path\" in constraint:                        source = \"path\"                        source_url = constraint[\"path\"]                        version = UnoplatVersion()  # Path dependencies don't have version constraints                    # Handle url dependencies                    elif \"url\" in constraint:                        source = \"url\"                        source_url = constraint[\"url\"]                        version = UnoplatVersion()  # URL dependencies don't have version constraints                else:                    activity.logger.warning(f\"Skipping invalid dependency specification for {name}\")                    continue                tuple_dependency = UnoplatProjectDependency(version=version, extras=extras, source=source, source_url=source_url, source_reference=source_reference, subdirectory=subdirectory)                dependencies[name] = tuple_dependency            except Exception as e:                activity.logger.warning(\"Error parsing dependency\", {\"dependency\": name, \"error\": str(e), \"group\": group})                # Add dependency with empty version constraint                tuple_dependency = UnoplatProjectDependency(version=UnoplatVersion())                dependencies[name] = tuple_dependency        return dependencies    def _get_entry_points(self, scripts: Dict[str, str]) -> Dict[str, str]:        \"\"\"Get all entry points from Poetry scripts section.\"\"\"        return scripts if scripts else {}"},{"NodeName":"PackageManagerStrategyFactory","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/package_manager/package_manager_factory.py","Functions":[{"Name":"get_strategy","Parameters":[{"TypeValue":"cls","TypeType":""},{"TypeValue":"programming_language","TypeType":"ProgrammingLanguage"},{"TypeValue":"package_manager_type","TypeType":"PackageManagerType"}],"Annotations":[{"Name":"classmethod","Position":{"StartLine":31,"StartLinePosition":4,"StopLine":32,"StopLinePosition":4}}],"Position":{"StartLine":32,"StartLinePosition":4,"StopLine":53},"LocalVariables":[{"TypeValue":"_strategies","TypeType":""},{"TypeValue":"key","TypeType":"(programming_language,package_manager_type)"}],"Content":"def get_strategy(cls, programming_language: ProgrammingLanguage, package_manager_type: PackageManagerType) -> PackageManagerStrategy:        \"\"\"        Get appropriate package manager strategy based on programming language and package manager        Args:            metadata: Programming language metadata from config        Returns:            PackageManagerStrategy: Appropriate strategy instance        Raises:            UnsupportedPackageManagerError: If combination is not supported        \"\"\"        key = (programming_language, package_manager_type)        if key not in cls._strategies:            raise UnsupportedPackageManagerError(f\"Unsupported combination - Language: {programming_language}, \" f\"Package Manager: {package_manager_type}\")        return cls._strategies[key]()"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["PackageManagerType","ProgrammingLanguage"]},{"Source":"src.code_confluence_flow_bridge.parser.package_manager.package_manager_strategy","UsageName":["PackageManagerStrategy"]},{"Source":"src.code_confluence_flow_bridge.parser.package_manager.pip.pip_strategy","UsageName":["PipStrategy"]},{"Source":"src.code_confluence_flow_bridge.parser.package_manager.poetry.poetry_strategy","UsageName":["PythonPoetryStrategy"]},{"Source":"src.code_confluence_flow_bridge.parser.package_manager.uv.uv_strategy","UsageName":["UvStrategy"]},{"Source":"typing","UsageName":["Dict","Tuple"]}],"Position":{"StartLine":23,"StopLine":53},"Content":"class PackageManagerStrategyFactory:    # Map (language, package_manager) pairs to their strategy classes    _strategies: Dict[Tuple[ProgrammingLanguage, PackageManagerType], type[PackageManagerStrategy]] = {        (ProgrammingLanguage.PYTHON, PackageManagerType.POETRY): PythonPoetryStrategy,        (ProgrammingLanguage.PYTHON, PackageManagerType.PIP): PipStrategy,        (ProgrammingLanguage.PYTHON, PackageManagerType.UV): UvStrategy,    }    @classmethod    def get_strategy(cls, programming_language: ProgrammingLanguage, package_manager_type: PackageManagerType) -> PackageManagerStrategy:        \"\"\"        Get appropriate package manager strategy based on programming language and package manager        Args:            metadata: Programming language metadata from config        Returns:            PackageManagerStrategy: Appropriate strategy instance        Raises:            UnsupportedPackageManagerError: If combination is not supported        \"\"\"        key = (programming_language, package_manager_type)        if key not in cls._strategies:            raise UnsupportedPackageManagerError(f\"Unsupported combination - Language: {programming_language}, \" f\"Package Manager: {package_manager_type}\")        return cls._strategies[key]()"},{"NodeName":"UnsupportedPackageManagerError","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/package_manager/package_manager_factory.py","MultipleExtend":["Exception"],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["PackageManagerType","ProgrammingLanguage"]},{"Source":"src.code_confluence_flow_bridge.parser.package_manager.package_manager_strategy","UsageName":["PackageManagerStrategy"]},{"Source":"src.code_confluence_flow_bridge.parser.package_manager.pip.pip_strategy","UsageName":["PipStrategy"]},{"Source":"src.code_confluence_flow_bridge.parser.package_manager.poetry.poetry_strategy","UsageName":["PythonPoetryStrategy"]},{"Source":"src.code_confluence_flow_bridge.parser.package_manager.uv.uv_strategy","UsageName":["UvStrategy"]},{"Source":"typing","UsageName":["Dict","Tuple"]}],"Position":{"StartLine":53,"StopLine":55},"Content":"class UnsupportedPackageManagerError(Exception):    pass"},{"NodeName":"PackageManagerParser","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/package_manager/package_manager_parser.py","Functions":[{"Name":"parse_package_metadata","Parameters":[{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"programming_language_metadata","TypeType":"ProgrammingLanguageMetadata"}],"FunctionCalls":[{"NodeName":"PackageManagerStrategyFactory","FunctionName":"get_strategy","Position":{"StartLine":15,"StartLinePosition":56,"StopLine":15,"StopLinePosition":155}}],"Position":{"StartLine":13,"StartLinePosition":4,"StopLine":17},"LocalVariables":[{"TypeValue":"package_strategy","TypeType":"PackageManagerStrategyFactory"}],"Content":"def parse_package_metadata(self, local_workspace_path: str, programming_language_metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Concrete implementation of the parse_codebase method.\"\"\"        package_strategy = PackageManagerStrategyFactory.get_strategy(programming_language_metadata.language, programming_language_metadata.package_manager)        return package_strategy.process_metadata(local_workspace_path, programming_language_metadata)"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package_manager_metadata","UsageName":["UnoplatPackageManagerMetadata"]},{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["ProgrammingLanguageMetadata"]},{"Source":"src.code_confluence_flow_bridge.parser.package_manager.package_manager_factory","UsageName":["PackageManagerStrategyFactory"]}],"Position":{"StartLine":12,"StopLine":17},"Content":"class PackageManagerParser:    def parse_package_metadata(self, local_workspace_path: str, programming_language_metadata: ProgrammingLanguageMetadata) -> UnoplatPackageManagerMetadata:        \"\"\"Concrete implementation of the parse_codebase method.\"\"\"        package_strategy = PackageManagerStrategyFactory.get_strategy(programming_language_metadata.language, programming_language_metadata.package_manager)        return package_strategy.process_metadata(local_workspace_path, programming_language_metadata)"},{"NodeName":"CodebaseParserStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/codebase_parser_strategy.py","MultipleExtend":["ABC"],"Functions":[{"Name":"parse_codebase","Parameters":[{"TypeValue":"codebase_name","TypeType":"str"},{"TypeValue":"json_data","TypeType":"dict"},{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"source_directory","TypeType":"str"},{"TypeValue":"programming_language_metadata","TypeType":"ProgrammingLanguageMetadata"}],"Annotations":[{"Name":"abstractmethod","Position":{"StartLine":16,"StartLinePosition":4,"StopLine":17,"StopLinePosition":4}}],"Position":{"StartLine":17,"StartLinePosition":4,"StopLine":19,"StopLinePosition":12},"Content":"def parse_codebase(self, codebase_name: str, json_data: dict, local_workspace_path: str, source_directory: str, programming_language_metadata: ProgrammingLanguageMetadata) -> List[UnoplatPackage]:        \"\"\"Parse codebase based on language specific implementation\"\"\"        pass"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package","UsageName":["UnoplatPackage"]},{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["ProgrammingLanguageMetadata"]},{"Source":"abc","UsageName":["ABC","abstractmethod"]},{"Source":"typing","UsageName":["List"]}],"Position":{"StartLine":15,"StopLine":19,"StopLinePosition":12},"Content":"class CodebaseParserStrategy(ABC):    @abstractmethod    def parse_codebase(self, codebase_name: str, json_data: dict, local_workspace_path: str, source_directory: str, programming_language_metadata: ProgrammingLanguageMetadata) -> List[UnoplatPackage]:        \"\"\"Parse codebase based on language specific implementation\"\"\"        pass"},{"NodeName":"CodeConfluenceTreeSitter","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/tree_sitter/code_confluence_tree_sitter.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"language","TypeType":"ProgrammingLanguage"}],"Position":{"StartLine":12,"StartLinePosition":4,"StopLine":24,"StopLinePosition":12},"LocalVariables":[{"TypeValue":"language","TypeType":""},{"TypeValue":"ProgrammingLanguage.PYTHON","TypeType":""},{"TypeValue":"PY_LANGUAGE","TypeType":"Language"},{"TypeValue":"self.parser","TypeType":"Parser"},{"TypeValue":"self.parser.language","TypeType":"PY_LANGUAGE"}],"Content":"def __init__(self, language: ProgrammingLanguage):        \"\"\"Initialize parser based on programming language.        Args:            language: Programming language enum value        \"\"\"        match language:            case ProgrammingLanguage.PYTHON:                PY_LANGUAGE = Language(tree_sitter_python.language())  # type: ignore                self.parser = Parser()                self.parser.language = PY_LANGUAGE  # type: ignore            "}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["ProgrammingLanguage"]},{"Source":"tree_sitter","UsageName":["Language","Parser"]},{"Source":"tree_sitter_python"}],"Position":{"StartLine":11,"StopLine":27,"StopLinePosition":4},"Content":"class CodeConfluenceTreeSitter:    def __init__(self, language: ProgrammingLanguage):        \"\"\"Initialize parser based on programming language.        Args:            language: Programming language enum value        \"\"\"        match language:            case ProgrammingLanguage.PYTHON:                PY_LANGUAGE = Language(tree_sitter_python.language())  # type: ignore                self.parser = Parser()                self.parser.language = PY_LANGUAGE  # type: ignore            case _:                raise ValueError(f\"Unsupported programming language: {language}\")    "},{"NodeName":"CodebaseParser","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/codebase_parser.py","Functions":[{"Name":"parse_codebase","Parameters":[{"TypeValue":"codebase_name","TypeType":"str"},{"TypeValue":"json_data","TypeType":"dict"},{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"source_directory","TypeType":"str"},{"TypeValue":"programming_language_metadata","TypeType":"ProgrammingLanguageMetadata"}],"FunctionCalls":[{"NodeName":"CodebaseParserFactory","FunctionName":"get_parser","Position":{"StartLine":18,"StartLinePosition":38,"StopLine":18,"StopLinePosition":94}}],"Position":{"StartLine":16,"StartLinePosition":4,"StopLine":19,"StopLinePosition":225},"LocalVariables":[{"TypeValue":"parser","TypeType":"CodebaseParserFactory"}],"Content":"def parse_codebase(self, codebase_name: str, json_data: dict, local_workspace_path: str, source_directory: str, programming_language_metadata: ProgrammingLanguageMetadata) -> List[UnoplatPackage]:        \"\"\"Concrete implementation of the parse_codebase method.\"\"\"        parser = CodebaseParserFactory.get_parser(programming_language_metadata.language.value)        return parser.parse_codebase(codebase_name=codebase_name, json_data=json_data, local_workspace_path=local_workspace_path, source_directory=source_directory, programming_language_metadata=programming_language_metadata)"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.chapi_forge.unoplat_package","UsageName":["UnoplatPackage"]},{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["ProgrammingLanguageMetadata"]},{"Source":"src.code_confluence_flow_bridge.parser.codebase_parser_factory","UsageName":["CodebaseParserFactory"]},{"Source":"typing","UsageName":["List"]}],"Position":{"StartLine":15,"StopLine":19,"StopLinePosition":225},"Content":"class CodebaseParser:    def parse_codebase(self, codebase_name: str, json_data: dict, local_workspace_path: str, source_directory: str, programming_language_metadata: ProgrammingLanguageMetadata) -> List[UnoplatPackage]:        \"\"\"Concrete implementation of the parse_codebase method.\"\"\"        parser = CodebaseParserFactory.get_parser(programming_language_metadata.language.value)        return parser.parse_codebase(codebase_name=codebase_name, json_data=json_data, local_workspace_path=local_workspace_path, source_directory=source_directory, programming_language_metadata=programming_language_metadata)"},{"NodeName":"LinterStrategyFactory","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/linters/linter_factory.py","Functions":[{"Name":"get_strategy","Parameters":[{"TypeValue":"cls","TypeType":""},{"TypeValue":"programming_language","TypeType":"ProgrammingLanguage"},{"TypeValue":"linter_type","TypeType":"LinterType"}],"Annotations":[{"Name":"classmethod","Position":{"StartLine":23,"StartLinePosition":4,"StopLine":24,"StopLinePosition":4}}],"Position":{"StartLine":24,"StartLinePosition":4,"StopLine":46},"LocalVariables":[{"TypeValue":"_strategies","TypeType":""},{"TypeValue":"key","TypeType":"(programming_language,linter_type)"}],"Content":"def get_strategy(cls, programming_language: ProgrammingLanguage, linter_type: LinterType) -> LinterStrategy:        \"\"\"        Get appropriate linter strategy based on programming language and linter type        Args:            programming_language: Programming language enum            linter_type: Type of linter to use        Returns:            LinterStrategy: Appropriate strategy instance        Raises:            UnsupportedLinterError: If combination is not supported        \"\"\"        key = (programming_language, linter_type)        if key not in cls._strategies:            raise UnsupportedLinterError(f\"Unsupported combination - Language: {programming_language}, \" f\"Linter: {linter_type}\")        return cls._strategies[key]()"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["ProgrammingLanguage"]},{"Source":"src.code_confluence_flow_bridge.models.workflow.code_confluence_linter","UsageName":["LinterType"]},{"Source":"src.code_confluence_flow_bridge.parser.linters.linter_strategy","UsageName":["LinterStrategy"]},{"Source":"src.code_confluence_flow_bridge.parser.linters.python.ruff_strategy","UsageName":["RuffStrategy"]},{"Source":"typing","UsageName":["Dict","Tuple"]}],"Position":{"StartLine":19,"StopLine":46},"Content":"class LinterStrategyFactory:    # Map (language, linter) pairs to their strategy classes    _strategies: Dict[Tuple[ProgrammingLanguage, LinterType], type[LinterStrategy]] = {(ProgrammingLanguage.PYTHON, LinterType.RUFF): RuffStrategy}    @classmethod    def get_strategy(cls, programming_language: ProgrammingLanguage, linter_type: LinterType) -> LinterStrategy:        \"\"\"        Get appropriate linter strategy based on programming language and linter type        Args:            programming_language: Programming language enum            linter_type: Type of linter to use        Returns:            LinterStrategy: Appropriate strategy instance        Raises:            UnsupportedLinterError: If combination is not supported        \"\"\"        key = (programming_language, linter_type)        if key not in cls._strategies:            raise UnsupportedLinterError(f\"Unsupported combination - Language: {programming_language}, \" f\"Linter: {linter_type}\")        return cls._strategies[key]()"},{"NodeName":"UnsupportedLinterError","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/linters/linter_factory.py","MultipleExtend":["Exception"],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["ProgrammingLanguage"]},{"Source":"src.code_confluence_flow_bridge.models.workflow.code_confluence_linter","UsageName":["LinterType"]},{"Source":"src.code_confluence_flow_bridge.parser.linters.linter_strategy","UsageName":["LinterStrategy"]},{"Source":"src.code_confluence_flow_bridge.parser.linters.python.ruff_strategy","UsageName":["RuffStrategy"]},{"Source":"typing","UsageName":["Dict","Tuple"]}],"Position":{"StartLine":46,"StopLine":48},"Content":"class UnsupportedLinterError(Exception):    pass"},{"NodeName":"RuffStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/linters/python/ruff_strategy.py","MultipleExtend":["LinterStrategy"],"Functions":[{"Name":"_get_config_template_path","FunctionCalls":[{"NodeName":"os","FunctionName":"getenv","Position":{"StartLine":21,"StartLinePosition":30,"StopLine":21,"StopLinePosition":60}}],"Position":{"StartLine":18,"StartLinePosition":4,"StopLine":25,"StopLinePosition":8},"LocalVariables":[{"TypeValue":"os.getenv(\"RUFF_CONFIG_TEMPLATE\")","TypeType":""}],"Content":"def _get_config_template_path(self) -> str:        \"\"\"Get path to Ruff config template, checking multiple locations.\"\"\"        # Check environment variable first        if template_path := os.getenv(\"RUFF_CONFIG_TEMPLATE\"):            return template_path                    # Fall back to default locations in order:        "}],"Imports":[{"Source":"src.code_confluence_flow_bridge.parser.linters.linter_strategy","UsageName":["LinterStrategy"]},{"Source":"os"},{"Source":"ast","UsageName":["List"]},{"Source":"pathlib","UsageName":["Path"]},{"Source":"re"},{"Source":"subprocess"},{"Source":"typing","UsageName":["Any","Optional"]},{"Source":"loguru","UsageName":["logger"]},{"Source":"tomlkit"}],"Position":{"StartLine":16,"StopLine":40,"StopLinePosition":4},"Content":"class RuffStrategy(LinterStrategy):        def _get_config_template_path(self) -> str:        \"\"\"Get path to Ruff config template, checking multiple locations.\"\"\"        # Check environment variable first        if template_path := os.getenv(\"RUFF_CONFIG_TEMPLATE\"):            return template_path                    # Fall back to default locations in order:        locations = [            # 1. Config in current working directory            Path.cwd() / \"config\" / \"ruff_config_template.toml\",            # 2. Config in package directory            Path(__file__).parent / \"templates\" / \"ruff_config_template.toml\",            # 3. Config in /etc (for container deployments)            Path(\"/etc/code-confluence/ruff_config_template.toml\")        ]                for path in locations:            if path.exists():                return str(path)                        raise FileNotFoundError(\"Ruff config template not found in any standard location\")        "},{"NodeName":"default","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/linters/python/ruff_strategy.py","Functions":[{"Name":"_convert_pep621_to_ruff_version","Parameters":[{"TypeValue":"version_spec","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":71,"StartLinePosition":18,"StopLine":71,"StopLinePosition":81}}],"Position":{"StartLine":40,"StartLinePosition":4,"StopLine":74,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"os.getenv(\"RUFF_CONFIG_TEMPLATE\")","TypeType":""},{"TypeValue":"locations","TypeType":"[Path.cwd()/\"config\"/\"ruff_config_template.toml\",Path(__file__).parent/\"templates\"/\"ruff_config_template.toml\",Path(\"/etc/code-confluence/ruff_config_template.toml\")]"},{"TypeValue":"match","TypeType":"re"},{"TypeValue":"","TypeType":"match"}],"Content":"def _convert_pep621_to_ruff_version(self, version_spec: str) -> str:        \"\"\"Convert PEP 621 Python version specifier to Ruff target version.                Args:            version_spec: Version specifier like '>=3.11' or '>=3.11,<4.0'                    Returns:            Ruff target version like 'py311'                    Example:            '>=3.11' -> 'py311'            '>=3.11,<4.0' -> 'py311'        \"\"\"        try:            # Extract the minimum version using regex            match = re.search(r'>=?\\s*(\\d+)\\.(\\d+)', version_spec)            if match:                major, minor = match.groups()                return f\"py{major}{minor:0>2}\"  # Pad minor version with leading zero                        # If no >= found, look for exact version            match = re.search(r'==?\\s*(\\d+)\\.(\\d+)', version_spec)            if match:                major, minor = match.groups()                return f\"py{major}{minor:0>2}\"                            # Default to Python 3.9 if no valid version found            logger.warning(f\"Could not parse Python version from '{version_spec}', defaulting to py39\")            return \"py39\"                    except Exception as e:            logger.error(f\"Error converting Python version '{version_spec}': {e}\")            return \"py39\"    "},{"Name":"_create_ruff_config","Parameters":[{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"programming_language_version","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":107,"StartLinePosition":18,"StopLine":107,"StopLinePosition":59}}],"Position":{"StartLine":74,"StartLinePosition":4,"StopLine":110,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"os.getenv(\"RUFF_CONFIG_TEMPLATE\")","TypeType":""},{"TypeValue":"locations","TypeType":"[Path.cwd()/\"config\"/\"ruff_config_template.toml\",Path(__file__).parent/\"templates\"/\"ruff_config_template.toml\",Path(\"/etc/code-confluence/ruff_config_template.toml\")]"},{"TypeValue":"match","TypeType":"re"},{"TypeValue":"","TypeType":"match"},{"TypeValue":"template_path","TypeType":"self"},{"TypeValue":"template","TypeType":"f"},{"TypeValue":"ruff_version","TypeType":"self"},{"TypeValue":"config_content","TypeType":"template"},{"TypeValue":"config","TypeType":"tomlkit"},{"TypeValue":"config_path","TypeType":"os"}],"Content":"def _create_ruff_config(self, local_workspace_path: str, programming_language_version: str) -> bool:        \"\"\"Create Ruff configuration file from template.                Args:            local_workspace_path: Path to workspace            programming_language_version: Python version to target (PEP 621 format)                    Returns:            bool: True if config was created successfully        \"\"\"        try:            # Load template            template_path = self._get_config_template_path()            with open(template_path, 'r', encoding='utf-8') as f:                template = f.read()                        # Convert version format            ruff_version = self._convert_pep621_to_ruff_version(programming_language_version)                        # Format template with values            config_content = template.format(                python_version=ruff_version,                src_path=local_workspace_path            )                        # Parse and write config            config = tomlkit.parse(config_content)            config_path = os.path.join(local_workspace_path, \"ruff.toml\")            with open(config_path, 'w', encoding='utf-8') as f:                tomlkit.dump(config, f)            return True                    except Exception as e:            logger.error(f\"Error creating Ruff config: {e}\")            return False    "},{"Name":"_delete_ruff_sections","Parameters":[{"TypeValue":"pyproject_path","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":156,"StartLinePosition":18,"StopLine":156,"StopLinePosition":62}}],"Position":{"StartLine":110,"StartLinePosition":4,"StopLine":159,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"os.getenv(\"RUFF_CONFIG_TEMPLATE\")","TypeType":""},{"TypeValue":"locations","TypeType":"[Path.cwd()/\"config\"/\"ruff_config_template.toml\",Path(__file__).parent/\"templates\"/\"ruff_config_template.toml\",Path(\"/etc/code-confluence/ruff_config_template.toml\")]"},{"TypeValue":"match","TypeType":"re"},{"TypeValue":"","TypeType":"match"},{"TypeValue":"template_path","TypeType":"self"},{"TypeValue":"template","TypeType":"f"},{"TypeValue":"ruff_version","TypeType":"self"},{"TypeValue":"config_content","TypeType":"template"},{"TypeValue":"config","TypeType":"tomlkit"},{"TypeValue":"config_path","TypeType":"os"},{"TypeValue":"content","TypeType":"f"},{"TypeValue":"doc","TypeType":""},{"TypeValue":"modified","TypeType":"True"},{"TypeValue":"tool_section","TypeType":"doc"},{"TypeValue":"ruff_keys","TypeType":"[keyforkeyintool_section.keys()ifisinstance(key,str)andkey.startswith('ruff')]"}],"Content":"def _delete_ruff_sections(self, pyproject_path: str) -> bool:        \"\"\"        Delete all Ruff-related sections from pyproject.toml                Args:            pyproject_path: Path to pyproject.toml file                    Returns:            bool: True if file was modified, False otherwise        \"\"\"        try:            if not os.path.exists(pyproject_path):                return False                            with open(pyproject_path, 'r', encoding='utf-8') as f:                content = f.read()                doc: dict[str, Any] = tomlkit.parse(content)                        modified = False                        # Check if tool section exists            if isinstance(doc.get('tool', {}), dict):                tool_section = doc['tool']                                # Find and remove all Ruff-related sections                ruff_keys = [                    key for key in tool_section.keys()                    if isinstance(key, str) and key.startswith('ruff')                ]                                for key in ruff_keys:                    del tool_section[key]                    modified = True                                # If tool section is empty after removing Ruff sections, remove it too                if not tool_section and 'tool' in doc:                    del doc['tool']                        if modified:                with open(pyproject_path, 'w', encoding='utf-8') as f:                    f.write(tomlkit.dumps(doc))                logger.info(f\"Deleted Ruff sections from {pyproject_path}\")                            return modified                    except Exception as e:            logger.error(f\"Error handling pyproject.toml: {e}\")            return False        "},{"Name":"lint_codebase","Parameters":[{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"dependencies","TypeType":"Optional[List]"},{"TypeValue":"programming_language_version","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":205,"StartLinePosition":18,"StopLine":205,"StopLinePosition":58}}],"Position":{"StartLine":159,"StartLinePosition":4,"StopLine":207},"LocalVariables":[{"TypeValue":"os.getenv(\"RUFF_CONFIG_TEMPLATE\")","TypeType":""},{"TypeValue":"locations","TypeType":"[Path.cwd()/\"config\"/\"ruff_config_template.toml\",Path(__file__).parent/\"templates\"/\"ruff_config_template.toml\",Path(\"/etc/code-confluence/ruff_config_template.toml\")]"},{"TypeValue":"match","TypeType":"re"},{"TypeValue":"","TypeType":"match"},{"TypeValue":"template_path","TypeType":"self"},{"TypeValue":"template","TypeType":"f"},{"TypeValue":"ruff_version","TypeType":"self"},{"TypeValue":"config_content","TypeType":"template"},{"TypeValue":"config","TypeType":"tomlkit"},{"TypeValue":"config_path","TypeType":"os"},{"TypeValue":"content","TypeType":"f"},{"TypeValue":"doc","TypeType":""},{"TypeValue":"modified","TypeType":"True"},{"TypeValue":"tool_section","TypeType":"doc"},{"TypeValue":"ruff_keys","TypeType":"[keyforkeyintool_section.keys()ifisinstance(key,str)andkey.startswith('ruff')]"},{"TypeValue":"pyproject_path","TypeType":"os"},{"TypeValue":"original_dir","TypeType":"os"},{"TypeValue":"result","TypeType":""}],"Content":"def lint_codebase(self, local_workspace_path: str, dependencies: Optional[List], programming_language_version: str) -> bool:        \"\"\"Run Ruff linter on Python codebase.        Args:            local_workspace_path (str): Path to Python codebase            dependencies (List): List of project dependencies            programming_language_version (str): Python version specifier        Returns:            bool: True if linting violations were found, False otherwise        \"\"\"        try:            # First check and handle existing Ruff config            pyproject_path = os.path.join(local_workspace_path, \"pyproject.toml\")            self._delete_ruff_sections(pyproject_path)                        # Create our Ruff configuration            if not self._create_ruff_config(local_workspace_path, programming_language_version):                return False                        # Store current directory            original_dir = os.getcwd()                        try:                # Change to workspace directory                os.chdir(local_workspace_path)                                # Now run Ruff with our configuration and fix option                result: subprocess.CompletedProcess[str] = subprocess.run(                    [\"ruff\", \"check\",\"--fix\",\".\", \"--unsafe-fixes\"],                    capture_output=True,                    text=True,                    check=False,  # Don't raise CalledProcessError on lint violations                )                                if result.returncode == 0 and result.stdout:                    logger.info(result.stdout)  # Validate JSON but we don't use it currently                    return True                return False                            finally:                # Always restore original directory                os.chdir(original_dir)        except Exception as e:            logger.error(f\"Error running Ruff linter: {e}\")            return False"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.parser.linters.linter_strategy","UsageName":["LinterStrategy"]},{"Source":"os"},{"Source":"ast","UsageName":["List"]},{"Source":"pathlib","UsageName":["Path"]},{"Source":"re"},{"Source":"subprocess"},{"Source":"typing","UsageName":["Any","Optional"]},{"Source":"loguru","UsageName":["logger"]},{"Source":"tomlkit"}],"Content":"from src.code_confluence_flow_bridge.parser.linters.linter_strategy import ("},{"NodeName":"LinterStrategy","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/linters/linter_strategy.py","MultipleExtend":["ABC"],"Functions":[{"Name":"lint_codebase","Parameters":[{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"dependencies","TypeType":"Optional[List]"},{"TypeValue":"programming_language_version","TypeType":"str"}],"Annotations":[{"Name":"abstractmethod","Position":{"StartLine":7,"StartLinePosition":4,"StopLine":8,"StopLinePosition":4}}],"Position":{"StartLine":8,"StartLinePosition":4,"StopLine":19},"Content":"def lint_codebase(self, local_workspace_path: str, dependencies: Optional[List], programming_language_version: str) -> bool:        \"\"\"        Run linting on the codebase and return results        Args:            local_workspace_path: Path to the local workspace        Returns:            bool: True if fixing linting violations else False        \"\"\"        pass"}],"Imports":[{"Source":"abc","UsageName":["ABC","abstractmethod"]},{"Source":"typing","UsageName":["List","Optional"]}],"Position":{"StartLine":5,"StopLine":19},"Content":"class LinterStrategy(ABC):        @abstractmethod    def lint_codebase(self, local_workspace_path: str, dependencies: Optional[List], programming_language_version: str) -> bool:        \"\"\"        Run linting on the codebase and return results        Args:            local_workspace_path: Path to the local workspace        Returns:            bool: True if fixing linting violations else False        \"\"\"        pass"},{"NodeName":"LinterParser","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/parser/linters/linter_parser.py","Functions":[{"Name":"lint_codebase","Parameters":[{"TypeValue":"local_workspace_path","TypeType":"str"},{"TypeValue":"dependencies","TypeType":"Optional[List[str]]"},{"TypeValue":"programming_language_metadata","TypeType":"ProgrammingLanguageMetadata"}],"FunctionCalls":[{"NodeName":"LinterStrategyFactory","FunctionName":"get_strategy","Position":{"StartLine":42,"StartLinePosition":47,"StopLine":45,"StopLinePosition":8}}],"Position":{"StartLine":16,"StartLinePosition":4,"StopLine":50,"StopLinePosition":10},"LocalVariables":[{"TypeValue":"linter_type","TypeType":"LinterType"},{"TypeValue":"linter_strategy","TypeType":"LinterStrategyFactory"}],"Content":"def lint_codebase(        self,        local_workspace_path: str,        dependencies: Optional[List[str]],        programming_language_metadata: ProgrammingLanguageMetadata    ) -> bool:        \"\"\"        Run linting on the codebase using appropriate linter strategy.        Args:            local_workspace_path: Path to the local workspace            programming_language_metadata: Metadata about programming language            programming_language_version: Version of programming language        Returns:            bool: True if linting was successful        \"\"\"                        # Select linter type based on programming language        linter_type: LinterType        if programming_language_metadata.language == \"python\":            linter_type = LinterType.RUFF        else:            linter_type = LinterType.RUFF  # Default to Ruff for now                    linter_strategy = LinterStrategyFactory.get_strategy(            programming_language=programming_language_metadata.language,            linter_type=linter_type        )        return linter_strategy.lint_codebase(            local_workspace_path=local_workspace_path,            dependencies=dependencies,            programming_language_version=programming_language_metadata.language_version #type: ignore        ) "}],"Imports":[{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["ProgrammingLanguageMetadata"]},{"Source":"src.code_confluence_flow_bridge.models.workflow.code_confluence_linter","UsageName":["LinterType"]},{"Source":"src.code_confluence_flow_bridge.parser.linters.linter_factory","UsageName":["LinterStrategyFactory"]},{"Source":"typing","UsageName":["List","Optional"]}],"Position":{"StartLine":15,"StopLine":50,"StopLinePosition":10},"Content":"class LinterParser:    def lint_codebase(        self,        local_workspace_path: str,        dependencies: Optional[List[str]],        programming_language_metadata: ProgrammingLanguageMetadata    ) -> bool:        \"\"\"        Run linting on the codebase using appropriate linter strategy.        Args:            local_workspace_path: Path to the local workspace            programming_language_metadata: Metadata about programming language            programming_language_version: Version of programming language        Returns:            bool: True if linting was successful        \"\"\"                        # Select linter type based on programming language        linter_type: LinterType        if programming_language_metadata.language == \"python\":            linter_type = LinterType.RUFF        else:            linter_type = LinterType.RUFF  # Default to Ruff for now                    linter_strategy = LinterStrategyFactory.get_strategy(            programming_language=programming_language_metadata.language,            linter_type=linter_type        )        return linter_strategy.lint_codebase(            local_workspace_path=local_workspace_path,            dependencies=dependencies,            programming_language_version=programming_language_metadata.language_version #type: ignore        ) "},{"NodeName":"default","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/main.py","Functions":[{"Name":"get_temporal_client","Modifiers":["async"],"Position":{"StartLine":44,"StopLine":53},"LocalVariables":[{"TypeValue":"logger","TypeType":"setup_logging"},{"TypeValue":"temporal_server","TypeType":"os"},{"TypeValue":"temporal_client","TypeType":""}],"Content":"async def get_temporal_client() -> Client:    \"\"\"Create and return a Temporal client instance.\"\"\"    # Connect to local temporal server    # Read from env - TEMPORAL_SERVER_ADDRESS, default to localhost:7233    temporal_server = os.getenv(\"TEMPORAL_SERVER_ADDRESS\", \"localhost:7233\")    temporal_client = await Client.connect(temporal_server)    return temporal_client"},{"Name":"run_worker","Parameters":[{"TypeValue":"activities","TypeType":"List[Callable]"},{"TypeValue":"client","TypeType":"Client"},{"TypeValue":"activity_executor","TypeType":"ThreadPoolExecutor"}],"FunctionCalls":[{"FunctionName":"Worker","Position":{"StartLine":62,"StartLinePosition":19,"StopLine":62,"StopLinePosition":202}}],"Modifiers":["async"],"Position":{"StartLine":53,"StopLine":67},"LocalVariables":[{"TypeValue":"logger","TypeType":"setup_logging"},{"TypeValue":"temporal_server","TypeType":"os"},{"TypeValue":"temporal_client","TypeType":""},{"TypeValue":"worker","TypeType":"Worker"}],"Content":"async def run_worker(activities: List[Callable], client: Client, activity_executor: ThreadPoolExecutor) -> None:    \"\"\"    Run the Temporal worker with given activities    Args:        activities: List of activity functions        client: Temporal client        activity_executor: Thread pool executor for activities    \"\"\"    worker = Worker(client, task_queue=\"unoplat-code-confluence-repository-context-ingestion\", workflows=[RepoWorkflow, CodebaseChildWorkflow], activities=activities, activity_executor=activity_executor)    await worker.run()"},{"Name":"start_workflow","Parameters":[{"TypeValue":"temporal_client","TypeType":"Client"},{"TypeValue":"repository","TypeType":"RepositorySettings"},{"TypeValue":"github_token","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"info","Position":{"StartLine":69,"StartLinePosition":10,"StopLine":69,"StopLinePosition":109}}],"Modifiers":["async"],"Position":{"StartLine":67,"StopLine":74},"LocalVariables":[{"TypeValue":"logger","TypeType":"setup_logging"},{"TypeValue":"temporal_server","TypeType":"os"},{"TypeValue":"temporal_client","TypeType":""},{"TypeValue":"worker","TypeType":"Worker"},{"TypeValue":"workflow_handle","TypeType":""}],"Content":"async def start_workflow(temporal_client: Client, repository: RepositorySettings, github_token: str):    workflow_handle = await temporal_client.start_workflow(RepoWorkflow.run, args=(repository, github_token), id=repository.git_url, task_queue=\"unoplat-code-confluence-repository-context-ingestion\")    logger.info(f\"Started workflow. Workflow ID: {workflow_handle.id}, RunID {workflow_handle.result_run_id}\")    return workflow_handle# Create FastAPI lifespan context manager"},{"Name":"lifespan","Parameters":[{"TypeValue":"app","TypeType":"FastAPI"}],"FunctionCalls":[{"NodeName":"asyncio","FunctionName":"create_task","Position":{"StartLine":100,"StartLinePosition":11,"StopLine":100,"StopLinePosition":138}}],"Annotations":[{"Name":"asynccontextmanager","Position":{"StartLine":74,"StopLine":75}}],"Modifiers":["async"],"Position":{"StartLine":75,"StopLine":105},"LocalVariables":[{"TypeValue":"logger","TypeType":"setup_logging"},{"TypeValue":"temporal_server","TypeType":"os"},{"TypeValue":"temporal_client","TypeType":""},{"TypeValue":"worker","TypeType":"Worker"},{"TypeValue":"workflow_handle","TypeType":""},{"TypeValue":"app.state.code_confluence_env","TypeType":"EnvironmentSettings"},{"TypeValue":"app.state.temporal_client","TypeType":""},{"TypeValue":"app.state.code_confluence_graph_ingestion","TypeType":"CodeConfluenceGraphIngestion"},{"TypeValue":"app.state.activity_executor","TypeType":"ThreadPoolExecutor"},{"TypeValue":"activities","TypeType":""},{"TypeValue":"git_activity","TypeType":"GitActivity"},{"TypeValue":"package_metadata_activity","TypeType":"PackageMetadataActivity"},{"TypeValue":"confluence_git_graph","TypeType":"ConfluenceGitGraph"},{"TypeValue":"codebase_package_ingestion","TypeType":"PackageManagerMetadataIngestion"},{"TypeValue":"codebase_processing_activity","TypeType":"CodebaseProcessingActivity"}],"Content":"async def lifespan(app: FastAPI):    app.state.code_confluence_env = EnvironmentSettings()    app.state.temporal_client = await get_temporal_client()    app.state.code_confluence_graph_ingestion = CodeConfluenceGraphIngestion(code_confluence_env=app.state.code_confluence_env)    await app.state.code_confluence_graph_ingestion.initialize()    app.state.activity_executor = ThreadPoolExecutor()    # Define activities    activities: List[Callable] = []    git_activity = GitActivity()    activities.append(git_activity.process_git_activity)    package_metadata_activity = PackageMetadataActivity()    activities.append(package_metadata_activity.get_package_metadata)    confluence_git_graph = ConfluenceGitGraph(code_confluence_graph_ingestion=app.state.code_confluence_graph_ingestion)    activities.append(confluence_git_graph.insert_git_repo_into_graph_db)    codebase_package_ingestion = PackageManagerMetadataIngestion(code_confluence_graph_ingestion=app.state.code_confluence_graph_ingestion)    activities.append(codebase_package_ingestion.insert_package_manager_metadata)        codebase_processing_activity = CodebaseProcessingActivity()    activities.append(codebase_processing_activity.process_codebase)    asyncio.create_task(run_worker(activities=activities, client=app.state.temporal_client, activity_executor=app.state.activity_executor))    yield    await app.state.code_confluence_graph_ingestion.close()"},{"Name":"monitor_workflow","Parameters":[{"TypeValue":"workflow_handle","TypeType":"WorkflowHandle"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":114,"StartLinePosition":14,"StopLine":114,"StopLinePosition":44}}],"Modifiers":["async"],"Position":{"StartLine":109,"StopLine":117},"LocalVariables":[{"TypeValue":"logger","TypeType":"setup_logging"},{"TypeValue":"temporal_server","TypeType":"os"},{"TypeValue":"temporal_client","TypeType":""},{"TypeValue":"worker","TypeType":"Worker"},{"TypeValue":"workflow_handle","TypeType":""},{"TypeValue":"app.state.code_confluence_env","TypeType":"EnvironmentSettings"},{"TypeValue":"app.state.temporal_client","TypeType":""},{"TypeValue":"app.state.code_confluence_graph_ingestion","TypeType":"CodeConfluenceGraphIngestion"},{"TypeValue":"app.state.activity_executor","TypeType":"ThreadPoolExecutor"},{"TypeValue":"activities","TypeType":""},{"TypeValue":"git_activity","TypeType":"GitActivity"},{"TypeValue":"package_metadata_activity","TypeType":"PackageMetadataActivity"},{"TypeValue":"confluence_git_graph","TypeType":"ConfluenceGitGraph"},{"TypeValue":"codebase_package_ingestion","TypeType":"PackageManagerMetadataIngestion"},{"TypeValue":"codebase_processing_activity","TypeType":"CodebaseProcessingActivity"},{"TypeValue":"app","TypeType":"FastAPI"},{"TypeValue":"result","TypeType":""}],"Content":"async def monitor_workflow(workflow_handle: WorkflowHandle):    try:        result = await workflow_handle.result()        logger.info(f\"Workflow completed with result: {result}\")    except Exception as e:        logger.error(f\"Workflow failed: {e}\")"},{"Name":"ingestion","Parameters":[{"TypeValue":"repository","TypeType":"RepositorySettings"},{"DefaultValue":"Header(None)","TypeValue":"authorization","TypeType":"Optional[str]"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"info","Position":{"StartLine":135,"StartLinePosition":10,"StopLine":135,"StopLinePosition":109}}],"Annotations":[{"Name":"app.post","KeyValues":[{"Key":"\"/start-ingestion\""},{"Key":"status_code","Value":"201"}],"Position":{"StartLine":117,"StopLine":118}}],"Modifiers":["async"],"Position":{"StartLine":118,"StopLine":141},"LocalVariables":[{"TypeValue":"logger","TypeType":"setup_logging"},{"TypeValue":"temporal_server","TypeType":"os"},{"TypeValue":"temporal_client","TypeType":""},{"TypeValue":"worker","TypeType":"Worker"},{"TypeValue":"workflow_handle","TypeType":""},{"TypeValue":"app.state.code_confluence_env","TypeType":"EnvironmentSettings"},{"TypeValue":"app.state.temporal_client","TypeType":""},{"TypeValue":"app.state.code_confluence_graph_ingestion","TypeType":"CodeConfluenceGraphIngestion"},{"TypeValue":"app.state.activity_executor","TypeType":"ThreadPoolExecutor"},{"TypeValue":"activities","TypeType":""},{"TypeValue":"git_activity","TypeType":"GitActivity"},{"TypeValue":"package_metadata_activity","TypeType":"PackageMetadataActivity"},{"TypeValue":"confluence_git_graph","TypeType":"ConfluenceGitGraph"},{"TypeValue":"codebase_package_ingestion","TypeType":"PackageManagerMetadataIngestion"},{"TypeValue":"codebase_processing_activity","TypeType":"CodebaseProcessingActivity"},{"TypeValue":"app","TypeType":"FastAPI"},{"TypeValue":"result","TypeType":""},{"TypeValue":"github_token","TypeType":"authorization"}],"Content":"async def ingestion(repository: RepositorySettings, authorization: Optional[str] = Header(None)):    \"\"\"    Start the ingestion workflow.    \"\"\"    # Extract GitHub token from Authorization header    if not authorization or not authorization.startswith(\"Bearer \"):        raise HTTPException(status_code=401, detail=\"Missing or invalid Authorization header\")    github_token = authorization.replace(\"Bearer \", \"\")    # Start the workflow    workflow_handle = await start_workflow(app.state.temporal_client, repository, github_token)    # Wait for the workflow to complete if needed    asyncio.create_task(monitor_workflow(workflow_handle))    logger.info(f\"Started workflow. Workflow ID: {workflow_handle.id}, RunID {workflow_handle.result_run_id}\")    return {\"workflow_id\": workflow_handle.id, \"run_id\": workflow_handle.result_run_id}# Create FastAPI application"}],"Imports":[{"Source":"src.code_confluence_flow_bridge.logging.log_config","UsageName":["setup_logging"]},{"Source":"src.code_confluence_flow_bridge.models.configuration.settings","UsageName":["EnvironmentSettings","RepositorySettings"]},{"Source":"src.code_confluence_flow_bridge.processor.codebase_child_workflow","UsageName":["CodebaseChildWorkflow"]},{"Source":"src.code_confluence_flow_bridge.processor.codebase_processing.codebase_processing_activity","UsageName":["CodebaseProcessingActivity"]},{"Source":"src.code_confluence_flow_bridge.processor.db.graph_db.code_confluence_graph_ingestion","UsageName":["CodeConfluenceGraphIngestion"]},{"Source":"src.code_confluence_flow_bridge.processor.git_activity.confluence_git_activity","UsageName":["GitActivity"]},{"Source":"src.code_confluence_flow_bridge.processor.git_activity.confluence_git_graph","UsageName":["ConfluenceGitGraph"]},{"Source":"src.code_confluence_flow_bridge.processor.package_metadata_activity.package_manager_metadata_activity","UsageName":["PackageMetadataActivity"]},{"Source":"src.code_confluence_flow_bridge.processor.package_metadata_activity.package_manager_metadata_ingestion","UsageName":["PackageManagerMetadataIngestion"]},{"Source":"src.code_confluence_flow_bridge.processor.repo_workflow","UsageName":["RepoWorkflow"]},{"Source":"os"},{"Source":"asyncio"},{"Source":"concurrent.futures","UsageName":["ThreadPoolExecutor"]},{"Source":"typing","UsageName":["Callable","List","Optional"]},{"Source":"fastapi","UsageName":["FastAPI","Header","HTTPException"]},{"Source":"fastapi.concurrency","UsageName":["asynccontextmanager"]},{"Source":"loguru","UsageName":["logger"]},{"Source":"temporalio.client","UsageName":["Client","WorkflowHandle"]},{"Source":"temporalio.worker","UsageName":["Worker"]}],"Content":"from src.code_confluence_flow_bridge.logging.log_config import setup_logging"},{"NodeName":"PydanticJSONPayloadConverter","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/pydantic_converter/converter.py","MultipleExtend":["JSONPlainPayloadConverter"],"Functions":[{"Name":"to_payload","Parameters":[{"TypeValue":"value","TypeType":"Any"}],"Position":{"StartLine":21,"StartLinePosition":4,"StopLine":35},"Content":"def to_payload(self, value: Any) -> Optional[Payload]:        \"\"\"Convert all values with Pydantic encoder or fail.        Like the base class, we fail if we cannot convert. This payload        converter is expected to be the last in the chain, so it can fail if        unable to convert.        \"\"\"        # We let JSON conversion errors be thrown to caller        return Payload(            metadata={\"encoding\": self.encoding.encode()},            data=json.dumps(value, separators=(\",\", \":\"), sort_keys=True, default=pydantic_encoder).encode(),        )"}],"Imports":[{"Source":"json"},{"Source":"typing","UsageName":["Any","Optional"]},{"Source":"pydantic.json","UsageName":["pydantic_encoder"]},{"Source":"temporalio.api.common.v1","UsageName":["Payload"]},{"Source":"temporalio.converter","UsageName":["CompositePayloadConverter","DataConverter","DefaultPayloadConverter","JSONPlainPayloadConverter"]}],"Position":{"StartLine":14,"StopLine":35},"Content":"class PydanticJSONPayloadConverter(JSONPlainPayloadConverter):    \"\"\"Pydantic JSON payload converter.    This extends the :py:class:`JSONPlainPayloadConverter` to override    :py:meth:`to_payload` using the Pydantic encoder.    \"\"\"    def to_payload(self, value: Any) -> Optional[Payload]:        \"\"\"Convert all values with Pydantic encoder or fail.        Like the base class, we fail if we cannot convert. This payload        converter is expected to be the last in the chain, so it can fail if        unable to convert.        \"\"\"        # We let JSON conversion errors be thrown to caller        return Payload(            metadata={\"encoding\": self.encoding.encode()},            data=json.dumps(value, separators=(\",\", \":\"), sort_keys=True, default=pydantic_encoder).encode(),        )"},{"NodeName":"PydanticPayloadConverter","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/pydantic_converter/converter.py","MultipleExtend":["CompositePayloadConverter"],"Functions":[{"Name":"__init__","FunctionCalls":[{"FunctionName":"super","Position":{"StartLine":41,"StartLinePosition":13,"StopLine":41,"StopLinePosition":14}},{"NodeName":"super","FunctionName":"__init__","Position":{"StartLine":41,"StartLinePosition":15,"StopLine":41,"StopLinePosition":182}}],"Position":{"StartLine":40,"StartLinePosition":4,"StopLine":44},"Content":"def __init__(self) -> None:        super().__init__(*(c if not isinstance(c, JSONPlainPayloadConverter) else PydanticJSONPayloadConverter() for c in DefaultPayloadConverter.default_encoding_payload_converters))"}],"Imports":[{"Source":"json"},{"Source":"typing","UsageName":["Any","Optional"]},{"Source":"pydantic.json","UsageName":["pydantic_encoder"]},{"Source":"temporalio.api.common.v1","UsageName":["Payload"]},{"Source":"temporalio.converter","UsageName":["CompositePayloadConverter","DataConverter","DefaultPayloadConverter","JSONPlainPayloadConverter"]}],"Position":{"StartLine":35,"StopLine":44},"Content":"class PydanticPayloadConverter(CompositePayloadConverter):    \"\"\"Payload converter that replaces Temporal JSON conversion with Pydantic    JSON conversion.    \"\"\"    def __init__(self) -> None:        super().__init__(*(c if not isinstance(c, JSONPlainPayloadConverter) else PydanticJSONPayloadConverter() for c in DefaultPayloadConverter.default_encoding_payload_converters))"},{"NodeName":"LogConfig","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/logging/log_config.py","Functions":[{"Name":"make_logger_config","Parameters":[{"DefaultValue":"\"logs\"","TypeValue":"log_path","TypeType":"str"}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":11,"StartLinePosition":4,"StopLine":12,"StopLinePosition":4}}],"Position":{"StartLine":12,"StartLinePosition":4,"StopLine":23},"Content":"def make_logger_config(log_path: str = \"logs\") -> Dict[str, Any]:        \"\"\"Return logger configuration\"\"\"        return {            \"handlers\": [                {\"sink\": sys.stdout, \"format\": \"<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>\", \"colorize\": True, \"level\": \"INFO\"},                {\"sink\": f\"{log_path}/code_confluence_api.log\", \"format\": \"{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}\", \"rotation\": \"500 MB\", \"retention\": \"10 days\", \"compression\": \"zip\", \"level\": \"DEBUG\", \"enqueue\": True},            ],            \"extra\": {\"app_name\": \"code-confluence-flow-bridge\"},        }"}],"Imports":[{"Source":"sys"},{"Source":"pathlib","UsageName":["Path"]},{"Source":"typing","UsageName":["Any","Dict"]},{"Source":"loguru","UsageName":["logger"]}],"Position":{"StartLine":8,"StopLine":23},"Content":"class LogConfig:    \"\"\"Logging configuration to be set for the server\"\"\"    @staticmethod    def make_logger_config(log_path: str = \"logs\") -> Dict[str, Any]:        \"\"\"Return logger configuration\"\"\"        return {            \"handlers\": [                {\"sink\": sys.stdout, \"format\": \"<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>\", \"colorize\": True, \"level\": \"INFO\"},                {\"sink\": f\"{log_path}/code_confluence_api.log\", \"format\": \"{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} - {message}\", \"rotation\": \"500 MB\", \"retention\": \"10 days\", \"compression\": \"zip\", \"level\": \"DEBUG\", \"enqueue\": True},            ],            \"extra\": {\"app_name\": \"code-confluence-flow-bridge\"},        }"},{"NodeName":"default","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/logging/log_config.py","Functions":[{"Name":"setup_logging","FunctionCalls":[{"NodeName":"logger","FunctionName":"configure","Position":{"StartLine":40,"StartLinePosition":10,"StopLine":40,"StopLinePosition":42}}],"Position":{"StartLine":23,"StopLine":43},"LocalVariables":[{"TypeValue":"log_path","TypeType":"Path"},{"TypeValue":"config","TypeType":"LogConfig"}],"Content":"def setup_logging():    \"\"\"Configure logging for the application\"\"\"    # Create logs directory    log_path = Path(\"logs\")    log_path.mkdir(exist_ok=True)    # Remove default logger    logger.remove()    # Configure logger with our settings    config = LogConfig.make_logger_config(str(log_path))    # Add handlers    for handler in config[\"handlers\"]:        logger.add(**handler)    # Set extra attributes    logger.configure(extra=config[\"extra\"])    return logger"}],"Imports":[{"Source":"sys"},{"Source":"pathlib","UsageName":["Path"]},{"Source":"typing","UsageName":["Any","Dict"]},{"Source":"loguru","UsageName":["logger"]}],"Content":"import sys"},{"NodeName":"default","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/debug/debug_tree_sitter.py","Functions":[{"Name":"serialize_node","Parameters":[{"TypeValue":"node","TypeType":"Node"},{"TypeValue":"code_bytes","TypeType":"bytes"}],"Position":{"StartLine":12,"StopLine":32},"LocalVariables":[{"TypeValue":"start_byte","TypeType":""},{"TypeValue":"end_byte","TypeType":""}],"Content":"def serialize_node(node: Node, code_bytes: bytes) -> Dict[str, Any]:    \"\"\"    Serialize a tree-sitter Node into a JSON-serializable dictionary.        Args:        node: The tree-sitter Node.        code_bytes: The original source code bytes.        Returns:        A dictionary with node type, start/end points, and the extracted text.    \"\"\"    start_byte: int = node.start_byte    end_byte: int = node.end_byte    return {        \"type\": node.type,        \"start_point\": node.start_point,        \"end_point\": node.end_point,        \"text\": code_bytes[start_byte:end_byte].decode(\"utf8\", errors=\"replace\").strip()    }"},{"Name":"process_from_import_statement","Parameters":[{"TypeValue":"node","TypeType":"Node"},{"TypeValue":"code_bytes","TypeType":"bytes"}],"FunctionCalls":[{"NodeName":"current","FunctionName":"next_named_sibling","Position":{"StartLine":77,"StartLinePosition":25,"StopLine":77,"StopLinePosition":26}}],"Position":{"StartLine":32,"StopLine":88},"LocalVariables":[{"TypeValue":"start_byte","TypeType":""},{"TypeValue":"end_byte","TypeType":""},{"TypeValue":"source","TypeType":""},{"TypeValue":"usage_names","TypeType":""},{"TypeValue":"current","TypeType":"current"},{"TypeValue":"name_node","TypeType":"current"},{"TypeValue":"alias_node","TypeType":"current"},{"TypeValue":"name_text","TypeType":""},{"TypeValue":"alias_text","TypeType":"alias_node"}],"Content":"def process_from_import_statement(node: Node, code_bytes: bytes) -> Optional[Dict[str, Any]]:    \"\"\"    Process a from-import statement AST node using next_named_sibling to walk through imports.        Args:        node: The from-import statement node (dotted_name node captured as @module).        code_bytes: The original source code bytes.        Returns:        A dict matching UnoplatImport structure or None if parsing fails.    \"\"\"    # Get the source from the dotted_name node (which is our @module capture)    source: str = code_bytes[node.start_byte:node.end_byte].decode(\"utf8\").strip()        usage_names: List[Dict[str, Optional[str]]] = []        # Start with the first aliased_import after the 'import' keyword    current = node.next_named_sibling          # Walk through all aliased_imports using next_named_sibling    while current:        if current.type == \"aliased_import\":            name_node = current.child(0)            alias_node = current.child(2)                        if name_node:                name_text: str = name_node.text.decode(\"utf8\").strip() #type: ignore                alias_text: Optional[str] = None                                if alias_node:                    alias_text = alias_node.text.decode(\"utf8\").strip() #type: ignore                                usage_names.append({                    \"original_name\": name_text,                    \"alias\": alias_text                })        elif current.type == \"dotted_name\":            name_node = current.child(0)            if name_node:                name_text: str = name_node.text.decode(\"utf8\").strip() #type: ignore                usage_names.append({\"original_name\": name_text})                                                 # Move to next sibling        current = current.next_named_sibling        if not usage_names:        return None            return {        \"Source\": source,        \"UsageName\": usage_names,        \"ImportType\": \"internal\" if source.startswith(INTERNAL_PREFIX) else None    }"},{"Name":"test_import_query","FunctionCalls":[{"NodeName":"unoplat_imports","FunctionName":"append","Position":{"StartLine":131,"StartLinePosition":31,"StopLine":131,"StopLinePosition":45}}],"Position":{"StartLine":88,"StopLine":134,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"start_byte","TypeType":""},{"TypeValue":"end_byte","TypeType":""},{"TypeValue":"source","TypeType":""},{"TypeValue":"usage_names","TypeType":""},{"TypeValue":"current","TypeType":"current"},{"TypeValue":"name_node","TypeType":"current"},{"TypeValue":"alias_node","TypeType":"current"},{"TypeValue":"name_text","TypeType":""},{"TypeValue":"alias_text","TypeType":"alias_node"},{"TypeValue":"code","TypeType":""},{"TypeValue":"code_bytes","TypeType":""},{"TypeValue":"parser","TypeType":""},{"TypeValue":"PY_LANGUAGE","TypeType":""},{"TypeValue":"parser.language","TypeType":"PY_LANGUAGE"},{"TypeValue":"tree","TypeType":"parser"},{"TypeValue":"query","TypeType":"PY_LANGUAGE"},{"TypeValue":"matches","TypeType":""},{"TypeValue":"unoplat_imports","TypeType":""},{"TypeValue":"module_nodes","TypeType":"captures"},{"TypeValue":"process_from_import_statement(module_node,code_bytes)","TypeType":""}],"Content":"def test_import_query() -> None:    \"\"\"Test to analyze Tree-sitter query results for Python imports.\"\"\"    code: str = \"\"\"# First Party (internal imports) - single linefrom src.code_confluence_flow_bridge.models.chapi.chapi_node import ChapiNode as Node, Program as pmfrom src.code_confluence_flow_bridge.models.code import Function, Class as csfrom src.code_confluence_flow_bridge.models.utils import get_all_utils# First Party (internal imports) - multi-line using parenthesesfrom src.code_confluence_flow_bridge.models.chapi_forge.unoplat_import import (    ImportedName,    UnoplatImport as Import)\"\"\"    code_bytes: bytes = code.encode(\"utf8\")        # 1) Create and configure parser    parser: Parser = Parser()    PY_LANGUAGE: Language = Language(tree_sitter_python.language())  # type: ignore    parser.language = PY_LANGUAGE  # type: ignore        # 2) Parse the code    tree = parser.parse(code_bytes)        # 3) Define query to capture internal from-imports    query = PY_LANGUAGE.query(        r\"\"\"        (          import_from_statement             module_name: (dotted_name) @module             (#match? @module \"^src\\\\.code_confluence_flow_bridge\")        )        \"\"\"    )        # 4) Get matches    matches: List[Tuple[int, Dict[str, List[Node]]]] = query.matches(tree.root_node)        # 5) Process each match using next_named_sibling traversal    unoplat_imports: List[Dict[str, Any]] = []    for _, captures in matches:        module_nodes = captures.get(\"module\", [])        for module_node in module_nodes:            if result := process_from_import_statement(module_node, code_bytes):                unoplat_imports.append(result)        # 6) Write output    "}],"Imports":[{"Source":"os"},{"Source":"datetime","UsageName":["datetime"]},{"Source":"json"},{"Source":"typing","UsageName":["Any","Dict","List","Optional","Tuple"]},{"Source":"tree_sitter","UsageName":["Language","Node","Parser"]},{"Source":"tree_sitter_python"}],"Content":"import os"},{"NodeName":"TotalFileCount","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/utility/total_file_count.py","Functions":[{"Name":"__init__","Parameters":[{"TypeValue":"directory","TypeType":""},{"TypeValue":"extension","TypeType":""}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"info","Position":{"StartLine":13,"StartLinePosition":14,"StopLine":13,"StopLinePosition":101}}],"Position":{"StartLine":10,"StartLinePosition":4,"StopLine":15,"StopLinePosition":4},"LocalVariables":[{"TypeValue":"self.directory","TypeType":"directory"},{"TypeValue":"self.extension","TypeType":"extension"}],"Content":"def __init__(self, directory, extension):        self.directory = directory        self.extension = extension        logger.info(f\"FileCounter initialized with directory: {directory} and extension: {extension}\")    "},{"Name":"count_files","FunctionCalls":[{"NodeName":"logger","FunctionName":"info","Position":{"StartLine":27,"StartLinePosition":14,"StopLine":27,"StopLinePosition":54}}],"Position":{"StartLine":15,"StartLinePosition":4,"StopLine":31},"LocalVariables":[{"TypeValue":"self.directory","TypeType":"directory"},{"TypeValue":"self.extension","TypeType":""},{"TypeValue":"pattern","TypeType":"os"},{"TypeValue":"files","TypeType":"glob"}],"Content":"def count_files(self):        logger.info(\"Counting files...\")        # Ensure the extension starts with a dot        if not self.extension.startswith(\".\"):            self.extension = \".\" + self.extension        # Create a pattern for glob to match all files with the extension        pattern = os.path.join(self.directory, \"**\", \"*\" + self.extension)        # Use glob.glob with recursive=True to find all files matching the pattern        files = glob.glob(pattern, recursive=True)        logger.info(f\"Total files found: {len(files)}\")        # Return the count of files        return len(files)"}],"Imports":[{"Source":"os"},{"Source":"glob"},{"Source":"loguru","UsageName":["logger"]}],"Position":{"StartLine":9,"StopLine":31},"Content":"class TotalFileCount:    def __init__(self, directory, extension):        self.directory = directory        self.extension = extension        logger.info(f\"FileCounter initialized with directory: {directory} and extension: {extension}\")    def count_files(self):        logger.info(\"Counting files...\")        # Ensure the extension starts with a dot        if not self.extension.startswith(\".\"):            self.extension = \".\" + self.extension        # Create a pattern for glob to match all files with the extension        pattern = os.path.join(self.directory, \"**\", \"*\" + self.extension)        # Use glob.glob with recursive=True to find all files matching the pattern        files = glob.glob(pattern, recursive=True)        logger.info(f\"Total files found: {len(files)}\")        # Return the count of files        return len(files)"},{"NodeName":"IsClassName","Type":"CLASS","FilePath":"/Users/jayghiya/Documents/unoplat/unoplat-codebase-understanding/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/tests/test_data/unoplat-code-confluence-ingestion/code-confluence-flow-bridge/src/code_confluence_flow_bridge/utility/is_class_name.py","Functions":[{"Name":"is_python_class_name","Parameters":[{"TypeValue":"name","TypeType":"str"}],"FunctionCalls":[{"NodeName":"logger","FunctionName":"error","Position":{"StartLine":49,"StartLinePosition":18,"StopLine":49,"StopLinePosition":130}}],"Annotations":[{"Name":"staticmethod","Position":{"StartLine":12,"StartLinePosition":4,"StopLine":13,"StopLinePosition":4}}],"Position":{"StartLine":13,"StartLinePosition":4,"StopLine":51},"LocalVariables":[{"TypeValue":"_valid_class_pattern","TypeType":"re"}],"Content":"def is_python_class_name(name: str) -> bool:        \"\"\"Check if a name follows Python class naming convention (CamelCase).        Args:            name: Name to check        Returns:            bool: True if name follows Python class naming convention.                 Returns False for invalid inputs after logging error.        \"\"\"        try:            # Handle None or non-string types            if not isinstance(name, str):                logger.error(\"Invalid input type for class name check: {}\\n\" \"Expected str, got {}\", name, type(name).__name__)                return False            # Handle empty string            if not name:                logger.debug(\"Empty string provided for class name check\")                return False            # Class names should:            # 1. Start with uppercase letter            # 2. Not contain underscores (CamelCase not snake_case)            # 3. Not be all uppercase (to exclude constants)            # 4. Allow single uppercase letter (e.g., A, B, T for generics)            # 5. Not contain special characters            # Handle single character case            if len(name) == 1:                return name.isupper()            # Check if name matches valid pattern and is not all uppercase            return bool(IsClassName._valid_class_pattern.match(name) and not name.isupper())        except Exception as e:            logger.error(\"Unexpected error checking class name: {}\\n\" \"Input: {}\\n\" \"Error: {}\", name, type(name).__name__, str(e))            return False"}],"Imports":[{"Source":"re"},{"Source":"loguru","UsageName":["logger"]}],"Position":{"StartLine":8,"StopLine":51},"Content":"class IsClassName:    # Class-level regex pattern for valid Python class names    _valid_class_pattern = re.compile(r\"^[A-Z][a-zA-Z0-9]*$\")    @staticmethod    def is_python_class_name(name: str) -> bool:        \"\"\"Check if a name follows Python class naming convention (CamelCase).        Args:            name: Name to check        Returns:            bool: True if name follows Python class naming convention.                 Returns False for invalid inputs after logging error.        \"\"\"        try:            # Handle None or non-string types            if not isinstance(name, str):                logger.error(\"Invalid input type for class name check: {}\\n\" \"Expected str, got {}\", name, type(name).__name__)                return False            # Handle empty string            if not name:                logger.debug(\"Empty string provided for class name check\")                return False            # Class names should:            # 1. Start with uppercase letter            # 2. Not contain underscores (CamelCase not snake_case)            # 3. Not be all uppercase (to exclude constants)            # 4. Allow single uppercase letter (e.g., A, B, T for generics)            # 5. Not contain special characters            # Handle single character case            if len(name) == 1:                return name.isupper()            # Check if name matches valid pattern and is not all uppercase            return bool(IsClassName._valid_class_pattern.match(name) and not name.isupper())        except Exception as e:            logger.error(\"Unexpected error checking class name: {}\\n\" \"Input: {}\\n\" \"Error: {}\", name, type(name).__name__, str(e))            return False"}]