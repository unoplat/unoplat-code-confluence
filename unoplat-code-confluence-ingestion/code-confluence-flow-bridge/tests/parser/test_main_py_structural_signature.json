{
  "module_docstring": null,
  "global_variables": [
    {
      "start_line": 152,
      "end_line": 154,
      "signature": "logger = setup_logging(\n    service_name=\"code-confluence-flow-bridge\", app_name=\"unoplat-code-confluence\"\n)"
    },
    {
      "start_line": 701,
      "end_line": 701,
      "signature": "app = FastAPI(lifespan=lifespan)"
    },
    {
      "start_line": 704,
      "end_line": 704,
      "signature": "origins: List[str] = os.getenv(\"ALLOWED_ORIGINS\", \"http://localhost:5173\").split(\",\")"
    }
  ],
  "functions": [
    {
      "start_line": 160,
      "end_line": 168,
      "signature": "async def get_temporal_client() -> Client:",
      "docstring": "Create and return a Temporal client instance.",
      "function_calls": [
        "os.getenv(\"TEMPORAL_SERVER_ADDRESS\", \"localhost:7233\")",
        "Client.connect(\n        temporal_server, data_converter=pydantic_data_converter\n    )"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 171,
      "end_line": 175,
      "signature": "async def _serve_worker(stop: asyncio.Event, worker: Worker) -> None:",
      "docstring": "Keep the worker running until `stop` is set, then shut it down.",
      "function_calls": [
        "stop.wait()"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 178,
      "end_line": 317,
      "signature": "def create_worker(\n    activities: List[Callable],\n    client: Client,\n    activity_executor: ThreadPoolExecutor,\n    env_settings: EnvironmentSettings,\n) -> Worker:",
      "docstring": "\n    Create a Temporal worker with given activities\n\n    Args:\n        activities: List of activity functions\n        client: Temporal client\n        activity_executor: Thread pool executor for activities\n        env_settings: Environment settings containing Temporal worker configuration\n\n    Returns:\n        Worker: Configured Temporal worker instance\n    ",
      "function_calls": [
        "ParentWorkflowStatusInterceptor()",
        "ActivityStatusInterceptor()",
        "PollerBehaviorAutoscaling(\n                minimum=env_settings.temporal_workflow_poller_min,\n                initial=env_settings.temporal_workflow_poller_initial,\n                maximum=env_settings.temporal_workflow_poller_max,\n            )",
        "PollerBehaviorAutoscaling(\n                minimum=env_settings.temporal_activity_poller_min,\n                initial=env_settings.temporal_activity_poller_initial,\n                maximum=env_settings.temporal_activity_poller_max,\n            )",
        "logger.info(\n                \"Starting Temporal worker with autoscaling pollers enabled. \"\n                \"Workflow poller: min={}, initial={}, max={}. \"\n                \"Activity poller: min={}, initial={}, max={}\",\n                env_settings.temporal_workflow_poller_min,\n                env_settings.temporal_workflow_poller_initial,\n                env_settings.temporal_workflow_poller_max,\n                env_settings.temporal_activity_poller_min,\n                env_settings.temporal_activity_poller_initial,\n                env_settings.temporal_activity_poller_max,\n            )",
        "logger.info(\n                \"Starting Temporal worker with max_concurrent_activities={}, \"\n                \"max_concurrent_activity_task_polls={}\",\n                env_settings.temporal_max_concurrent_activities,\n                env_settings.temporal_max_concurrent_activity_task_polls,\n            )",
        "Worker(\n                client,  # Client must be passed as a positional argument\n                task_queue=\"unoplat-code-confluence-repository-context-ingestion\",\n                workflows=[RepoWorkflow, CodebaseChildWorkflow],\n                activities=activities,\n                activity_executor=activity_executor,\n                interceptors=[\n                    ParentWorkflowStatusInterceptor(),\n                    ActivityStatusInterceptor(),\n                ],\n                max_concurrent_activities=env_settings.temporal_max_concurrent_activities,\n                # Configure poller behaviors with autoscaling\n                workflow_task_poller_behavior=PollerBehaviorAutoscaling(\n                    minimum=env_settings.temporal_workflow_poller_min,\n                    initial=env_settings.temporal_workflow_poller_initial,\n                    maximum=env_settings.temporal_workflow_poller_max,\n                ),\n                activity_task_poller_behavior=PollerBehaviorAutoscaling(\n                    minimum=env_settings.temporal_activity_poller_min,\n                    initial=env_settings.temporal_activity_poller_initial,\n                    maximum=env_settings.temporal_activity_poller_max,\n                ),\n            )",
        "ParentWorkflowStatusInterceptor()",
        "ActivityStatusInterceptor()",
        "PollerBehaviorAutoscaling(\n                    minimum=env_settings.temporal_workflow_poller_min,\n                    initial=env_settings.temporal_workflow_poller_initial,\n                    maximum=env_settings.temporal_workflow_poller_max,\n                )",
        "PollerBehaviorAutoscaling(\n                    minimum=env_settings.temporal_activity_poller_min,\n                    initial=env_settings.temporal_activity_poller_initial,\n                    maximum=env_settings.temporal_activity_poller_max,\n                )",
        "Worker(\n                client,  # Client must be passed as a positional argument\n                task_queue=\"unoplat-code-confluence-repository-context-ingestion\",\n                workflows=[RepoWorkflow, CodebaseChildWorkflow],\n                activities=activities,\n                activity_executor=activity_executor,\n                interceptors=[\n                    ParentWorkflowStatusInterceptor(),\n                    ActivityStatusInterceptor(),\n                ],\n                max_concurrent_activities=env_settings.temporal_max_concurrent_activities,\n                max_concurrent_activity_task_polls=env_settings.temporal_max_concurrent_activity_task_polls,\n            )",
        "ParentWorkflowStatusInterceptor()",
        "ActivityStatusInterceptor()",
        "str(e)",
        "traceback.format_exc()",
        "logger.error(\n            \"Failed to start Temporal worker: {}\",\n            str(e),\n            extra={\"error_context\": error_context},\n        )",
        "str(e)",
        "\"Failed to start Temporal worker: {}\".format(str(e))",
        "str(e)",
        "ApplicationError(error_message, type=\"WORKER_INITIALIZATION_ERROR\")"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 320,
      "end_line": 352,
      "signature": "async def fetch_github_token_from_db(session: AsyncSession) -> str:",
      "docstring": "\n    Fetch and decrypt GitHub token from database.\n\n    Args:\n        session: Database session\n\n    Returns:\n        Decrypted GitHub token\n\n    Raises:\n        HTTPException: If no credentials found or decryption fails\n    ",
      "function_calls": [
        "session.execute(select(Credentials))",
        "select(Credentials)",
        "result.scalars().first()",
        "result.scalars()",
        "logger.error(\"Database error while fetching credentials: {}\", db_error)",
        "HTTPException(\n            status_code=500, detail=\"Database error while fetching credentials\"\n        )",
        "HTTPException(status_code=404, detail=\"No credentials found\")",
        "decrypt_token(credential.token_hash)",
        "logger.error(\"Failed to decrypt token: {}\", decrypt_error)",
        "HTTPException(\n            status_code=500,\n            detail=\"Internal error during authentication token decryption\",\n        )"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 355,
      "end_line": 379,
      "signature": "async def start_workflow(\n    temporal_client: Client,\n    repo_request: GitHubRepoRequestConfiguration,\n    github_token: str,\n    workflow_id: str,\n    trace_id: str,\n) -> WorkflowHandle:",
      "docstring": "\n    Start a Temporal workflow for the given repository request and workflow id.\n    ",
      "function_calls": [
        "RepoWorkflowRunEnvelope(\n        repo_request=repo_request, github_token=github_token, trace_id=trace_id\n    )",
        "temporal_client.start_workflow(\n        RepoWorkflow.run,\n        arg=envelope,\n        id=workflow_id,\n        task_queue=\"unoplat-code-confluence-repository-context-ingestion\",\n    )",
        "logger.info(\n        \"Started workflow. Workflow ID: {}, RunID {}\",\n        workflow_handle.id,\n        workflow_handle.result_run_id,\n    )"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 382,
      "end_line": 491,
      "signature": "async def generate_sse_events(\n    git_url: str, github_token: str, detector: PythonRipgrepDetector\n) -> AsyncGenerator[str, None]:",
      "docstring": "\n    Generate SSE events for codebase detection progress (v2).\n\n    Args:\n        git_url: GitHub repository URL or local path\n        github_token: GitHub authentication token\n        detector: PythonRipgrepDetector instance\n\n    Yields:\n        SSE-formatted event strings\n    ",
      "function_calls": [
        "SSEMessage.connected()",
        "SSEMessage.format_sse(\n        data={\n            \"state\": DetectionState.INITIALIZING.value,\n            \"message\": \"Starting codebase detection...\",\n            \"repository_url\": git_url,\n        },\n        event=\"progress\",\n    )",
        "time.time()",
        "os.path.exists(git_url)",
        "SSEMessage.format_sse(\n                data={\n                    \"state\": DetectionState.ANALYZING.value,\n                    \"message\": \"Analyzing local repository...\",\n                    \"repository_url\": git_url,\n                },\n                event=\"progress\",\n            )",
        "SSEMessage.format_sse(\n                data={\n                    \"state\": DetectionState.CLONING.value,\n                    \"message\": \"Cloning repository...\",\n                    \"repository_url\": git_url,\n                },\n                event=\"progress\",\n            )",
        "detector.detect_codebases(git_url, github_token)",
        "time.time()",
        "SSEMessage.format_sse(\n            data={\n                \"state\": DetectionState.COMPLETE.value,\n                \"message\": f\"Detection completed. Found {len(codebases)} codebases.\",\n                \"repository_url\": git_url,\n            },\n            event=\"progress\",\n        )",
        "len(codebases)",
        "DetectionResult(\n            repository_url=git_url,\n            duration_seconds=duration,\n            codebases=codebases,\n            error=None,\n        )",
        "SSEMessage.result(result.model_dump())",
        "result.model_dump()",
        "SSEMessage.done()",
        "time.time()",
        "locals()",
        "SSEMessage.format_sse(\n            data={\n                \"state\": DetectionState.COMPLETE.value,\n                \"message\": f\"Detection failed: {str(e)}\",\n                \"repository_url\": git_url,\n            },\n            event=\"progress\",\n        )",
        "str(e)",
        "DetectionResult(\n            repository_url=git_url,\n            duration_seconds=duration,\n            codebases=[],\n            error=str(e),\n        )",
        "str(e)",
        "SSEMessage.result(result.model_dump())",
        "result.model_dump()",
        "logger.error(\"Detection error: {}\", str(e))",
        "str(e)",
        "SSEMessage.error(str(e), error_type=\"DETECTION_ERROR\")",
        "str(e)",
        "SSEMessage.done()"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 494,
      "end_line": 545,
      "signature": "async def detect_codebases_sse(\n    git_url: str = Query(\n        ..., description=\"GitHub repository URL or folder name for local repository\"\n    ),\n    is_local: bool = Query(False, description=\"Whether this is a local repository\"),\n    session: AsyncSession = Depends(get_session),\n) -> StreamingResponse:",
      "docstring": "\n    Server-Sent Events endpoint for real-time codebase detection progress (v2).\n\n    Streams progress updates during Python codebase auto-detection from GitHub repositories\n    or local Git repositories. Uses FastAPI's StreamingResponse with proper SSE formatting.\n\n    Args:\n        git_url: GitHub repository URL or folder name for local repository\n        is_local: Whether this is a local repository\n        session: Database session for token retrieval\n\n    Returns:\n        StreamingResponse with text/event-stream content type\n    ",
      "function_calls": [
        "Query(\n        ..., description=\"GitHub repository URL or folder name for local repository\"\n    )",
        "Query(False, description=\"Whether this is a local repository\")",
        "Depends(get_session)",
        "construct_local_repository_path(git_url)",
        "logger.info(\n            \"Local repository detection - folder: {}, resolved path: {}, environment: {}\",\n            git_url,\n            actual_path,\n            get_runtime_environment(),\n        )",
        "get_runtime_environment()",
        "fetch_github_token_from_db(session)",
        "logger.info(\"Remote repository detection - URL: {}\", git_url)",
        "StreamingResponse(\n        generate_sse_events(actual_path, github_token, detector),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"X-Accel-Buffering\": \"no\",  # Disable nginx buffering\n        },\n    )",
        "generate_sse_events(actual_path, github_token, detector)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 549,
      "end_line": 698,
      "signature": "@asynccontextmanager\nasync def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:",
      "docstring": null,
      "function_calls": [
        "EnvironmentSettings()",
        "get_temporal_client()",
        "CodeConfluenceGraph(\n        code_confluence_env=app.state.code_confluence_env\n    )",
        "app.state.code_confluence_graph.connect()",
        "app.state.code_confluence_graph.create_schema()",
        "logger.info(\"Neo4j connection and schema initialized successfully\")",
        "PythonRipgrepDetector()",
        "app.state.python_codebase_detector.initialize_rules()",
        "logger.info(\"PythonRipgrepDetector initialized successfully\")",
        "CodeConfluenceGraphDeletion(\n        app.state.code_confluence_graph\n    )",
        "ThreadPoolExecutor(max_workers=pool_size)",
        "logger.info(\n        \"Initialized activity executor with {} threads (max_concurrent_activities={} + 4 buffer threads)\",\n        pool_size,\n        app.state.code_confluence_env.temporal_max_concurrent_activities,\n    )",
        "asyncio.get_running_loop()",
        "loop.set_default_executor(app.state.activity_executor)",
        "logger.info(\"Set default executor for asyncio loop\")",
        "GitActivity()",
        "activities.append(git_activity.process_git_activity)",
        "ParentWorkflowDbActivity()",
        "activities.append(parent_workflow_db_activity.update_repository_workflow_status)",
        "ChildWorkflowDbActivity()",
        "activities.append(child_workflow_db_activity.update_codebase_workflow_status)",
        "PackageMetadataActivity()",
        "activities.append(package_metadata_activity.get_package_metadata)",
        "ConfluenceGitGraph(\n        code_confluence_graph=app.state.code_confluence_graph\n    )",
        "activities.append(confluence_git_graph.insert_git_repo_into_graph_db)",
        "PackageManagerMetadataIngestion(\n        code_confluence_graph=app.state.code_confluence_graph\n    )",
        "activities.append(codebase_package_ingestion.insert_package_manager_metadata)",
        "GenericCodebaseProcessingActivity(\n        code_confluence_graph=app.state.code_confluence_graph\n    )",
        "activities.append(generic_activity.process_codebase_generic)",
        "create_db_and_tables()",
        "os.getenv(\"LOAD_FRAMEWORK_DEFINITIONS\", \"true\").lower()",
        "os.getenv(\"LOAD_FRAMEWORK_DEFINITIONS\", \"true\")",
        "FrameworkDefinitionLoader(app.state.code_confluence_env)",
        "get_session_cm()",
        "framework_loader.load_framework_definitions_at_startup(\n                    session\n                )",
        "metrics.get(\"skipped\")",
        "logger.info(\n                        \"Framework definitions loaded in {:.3f}s\", metrics[\"total_time\"]\n                    )",
        "logger.error(\"Failed to load framework definitions: {}\", e)",
        "os.getenv(\"FRAMEWORK_DEFINITIONS_REQUIRED\", \"false\").lower()",
        "os.getenv(\"FRAMEWORK_DEFINITIONS_REQUIRED\", \"false\")",
        "create_worker(\n        activities=activities,\n        client=app.state.temporal_client,\n        activity_executor=app.state.activity_executor,\n        env_settings=app.state.code_confluence_env,\n    )",
        "asyncio.Event()",
        "asyncio.create_task(_serve_worker(stop_event, worker))",
        "_serve_worker(stop_event, worker)",
        "logger.info(\"Shutting down application...\")",
        "stop_event.set()",
        "logger.info(\"Temporal worker shut down successfully\")",
        "logger.error(\"Error during worker shutdown: {}\", e)",
        "app.state.code_confluence_graph.close()",
        "logger.info(\"Neo4j global connection closed\")",
        "logger.error(\"Error closing Neo4j connections: {}\", e)",
        "dispose_current_engine()",
        "logger.info(\"SQLAlchemy engine disposed\")",
        "logger.warning(\"Failed to dispose async engine during shutdown: {}\", exc)",
        "app.state.activity_executor.shutdown(wait=True)",
        "logger.info(\"Thread pool executor shut down\")",
        "logger.error(\"Error shutting down thread pool executor: {}\", e)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 716,
      "end_line": 721,
      "signature": "async def monitor_workflow(workflow_handle: WorkflowHandle) -> None:",
      "docstring": null,
      "function_calls": [
        "workflow_handle.result()",
        "logger.info(\"Workflow completed with result: {}\", result)",
        "logger.error(\"Workflow failed: {}\", e)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 724,
      "end_line": 772,
      "signature": "@app.post(\"/ingest-token\", status_code=201)\nasync def ingest_token(\n    authorization: str = Header(...), session: AsyncSession = Depends(get_session)\n) -> Dict[str, str]:",
      "docstring": null,
      "function_calls": [
        "Header(...)",
        "Depends(get_session)",
        "authorization.startswith(\"Bearer \")",
        "HTTPException(status_code=401, detail=\"Invalid Authorization header\")",
        "authorization[7:].strip()",
        "encrypt_token(token)",
        "datetime.now(timezone.utc)",
        "session.execute(select(Credentials))",
        "select(Credentials)",
        "result.scalars().first()",
        "result.scalars()",
        "HTTPException(\n                status_code=409,\n                detail=\"Token already ingested. Use update-token to update it.\",\n            )",
        "Credentials(token_hash=encrypted_token, created_at=current_time)",
        "session.add(credential)",
        "session.execute(\n            select(Flag).where(Flag.name == \"isTokenSubmitted\")\n        )",
        "select(Flag)",
        "select(Flag).where(Flag.name == \"isTokenSubmitted\")",
        "flag_result.scalar_one_or_none()",
        "Flag(name=\"isTokenSubmitted\", status=True)",
        "session.add(token_flag)",
        "logger.error(\"Failed to process token: {}\", str(e))",
        "str(e)",
        "HTTPException(\n            status_code=500, detail=\"Failed to process authentication token\"\n        )"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 775,
      "end_line": 800,
      "signature": "@app.put(\"/update-token\", status_code=200)\nasync def update_token(\n    authorization: str = Header(...), session: AsyncSession = Depends(get_session)\n) -> Dict[str, str]:",
      "docstring": null,
      "function_calls": [
        "Header(...)",
        "Depends(get_session)",
        "authorization.startswith(\"Bearer \")",
        "HTTPException(status_code=401, detail=\"Invalid Authorization header\")",
        "authorization[7:].strip()",
        "encrypt_token(token)",
        "datetime.now(timezone.utc)",
        "session.execute(select(Credentials))",
        "select(Credentials)",
        "result.scalars().first()",
        "result.scalars()",
        "HTTPException(status_code=404, detail=\"No token found to update\")",
        "session.add(credential)",
        "logger.error(\"Failed to update token: {}\", str(e))",
        "str(e)",
        "HTTPException(\n            status_code=500, detail=\"Failed to update authentication token\"\n        )"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 803,
      "end_line": 837,
      "signature": "@app.delete(\"/delete-token\", status_code=200)\nasync def delete_token(session: AsyncSession = Depends(get_session)) -> Dict[str, str]:",
      "docstring": null,
      "function_calls": [
        "Depends(get_session)",
        "session.execute(select(Credentials))",
        "select(Credentials)",
        "result.scalars().first()",
        "result.scalars()",
        "HTTPException(status_code=404, detail=\"No token found to delete\")",
        "session.delete(credential)",
        "session.execute(\n            select(Flag).where(Flag.name == \"isTokenSubmitted\")\n        )",
        "select(Flag).where(Flag.name == \"isTokenSubmitted\")",
        "select(Flag)",
        "flag_result.scalar_one_or_none()",
        "Flag(name=\"isTokenSubmitted\", status=False)",
        "session.add(token_flag)",
        "logger.error(\"Failed to delete token: {}\", str(e))",
        "str(e)",
        "HTTPException(\n            status_code=500, detail=\"Failed to delete authentication token\"\n        )"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 840,
      "end_line": 986,
      "signature": "@app.get(\"/repos\", response_model=PaginatedResponse)\nasync def get_repos(\n    per_page: int = Query(30, ge=1, le=100, description=\"Items per page\"),\n    cursor: Optional[str] = Query(None, description=\"Pagination cursor\"),\n    filterValues: Optional[str] = Query(\n        None, description=\"Optional JSON filter values to filter repositories\"\n    ),\n    session: AsyncSession = Depends(get_session),\n) -> PaginatedResponse:",
      "docstring": null,
      "function_calls": [
        "Query(30, ge=1, le=100, description=\"Items per page\")",
        "Query(None, description=\"Pagination cursor\")",
        "Query(\n        None, description=\"Optional JSON filter values to filter repositories\"\n    )",
        "Depends(get_session)",
        "fetch_github_token_from_db(session)",
        "json.loads(filterValues)",
        "logger.error(\"Invalid JSON in filterValues: {}\", e)",
        "HTTPException(\n                status_code=400, detail=\"Invalid JSON in filterValues query parameter\"\n            )",
        "AIOHTTPTransport(\n            url=\"https://api.github.com/graphql\",\n            headers={\n                \"Authorization\": f\"Bearer {token}\",\n                \"User-Agent\": \"Unoplat Code Confluence\",\n            },\n        )",
        "GQLClient(\n            transport=transport,\n            fetch_schema_from_transport=False,\n        )",
        "gql(\n                    \"\"\"\n                    query SearchRepositories($query: String!, $first: Int!, $after: String) {\n                        search(query: $query, type: REPOSITORY, first: $first, after: $after) {\n                            pageInfo {\n                                endCursor\n                                hasNextPage\n                            }\n                            nodes {\n                                ... on Repository {\n                                    name\n                                    isPrivate\n                                    url\n                                    owner {\n                                        login\n                                        url\n                                    }\n                                }\n                            }\n                        }\n                    }\n                    \"\"\"\n                )",
        "client.execute(\n                    query,\n                    variable_values={\n                        \"query\": search_query,\n                        \"first\": per_page,\n                        \"after\": cursor,\n                    },\n                )",
        "gql(\n                    \"\"\"\n                    query GetRepositories($first: Int!, $after: String) {\n                        viewer {\n                            repositories(\n                                first: $first,\n                                affiliations: [OWNER, COLLABORATOR, ORGANIZATION_MEMBER],\n                                after: $after\n                            ) {\n                                pageInfo {\n                                    endCursor\n                                    hasNextPage\n                                }\n                                nodes {\n                                    name\n                                    isPrivate\n                                    url\n                                    owner {\n                                        login\n                                        url\n                                    }\n                                }\n                            }\n                        }\n                    }\n                    \"\"\"\n                )",
        "client.execute(\n                    query, variable_values={\"first\": per_page, \"after\": cursor}\n                )",
        "GitHubRepoSummary(\n                    name=item[\"name\"],\n                    owner_url=item[\"owner\"][\"url\"],\n                    private=item[\"isPrivate\"],\n                    git_url=item[\"url\"],\n                    owner_name=item[\"owner\"][\"login\"],\n                )",
        "repos_list.append(repo_summary)",
        "PaginatedResponse(\n                items=repos_list,\n                per_page=per_page,\n                has_next=has_next,\n                next_cursor=next_cursor,\n            )",
        "logger.error(\"GraphQL Error: {}\", str(e))",
        "str(e)",
        "HTTPException(\n            status_code=500, detail=f\"Failed to fetch repositories: {str(e)}\"\n        )",
        "str(e)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 989,
      "end_line": 1067,
      "signature": "@app.post(\"/start-ingestion\", status_code=201)\nasync def ingestion(\n    repo_request: GitHubRepoRequestConfiguration,\n    session: AsyncSession = Depends(get_session),\n    request_logger: \"Logger\" = Depends(trace_dependency),  # type: ignore\n) -> dict[str, str]:",
      "docstring": "\n    Start the ingestion workflow for the entire repository using the GitHub token from the database.\n    Submits the whole repo_request at once to the Temporal workflow.\n    Returns the workflow_id and run_id.\n    Also ingests the repository configuration into the database.\n    ",
      "function_calls": [
        "Depends(get_session)",
        "Depends(trace_dependency)",
        "construct_local_repository_path(folder_name)",
        "request_logger.info(\n            \"Local repository ingestion - folder: {}, resolved path: {}, environment: {}\",\n            folder_name,\n            actual_path,\n            get_runtime_environment(),\n        )",
        "get_runtime_environment()",
        "extract_github_organization_from_local_repo(\n            actual_path, original_owner\n        )",
        "request_logger.info(\n            \"Local repository GitHub organization - original: {}, extracted: {}\",\n            original_owner,\n            github_organization,\n        )",
        "build_trace_id(\n            repo_request.repository_name, github_organization\n        )",
        "trace_id_var.set(updated_trace_id)",
        "request_logger.info(\n            \"Updated trace_id for workflow consistency - new: {}\", updated_trace_id\n        )",
        "fetch_github_token_from_db(session)",
        "trace_id_var.get()",
        "HTTPException(500, \"trace_id not set by dependency\")",
        "start_workflow(\n        temporal_client=app.state.temporal_client,\n        repo_request=repo_request,\n        github_token=github_token,\n        workflow_id=f\"ingest-{trace_id}\",\n        trace_id=trace_id,\n    )",
        "asyncio.create_task(monitor_workflow(workflow_handle))",
        "monitor_workflow(workflow_handle)",
        "request_logger.info(\n        \"Started workflow. Workflow ID: {}, RunID {}\",\n        workflow_handle.id,\n        workflow_handle.result_run_id,\n    )"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1070,
      "end_line": 1097,
      "signature": "@app.get(\"/flags/{flag_name}\", status_code=200)\nasync def get_flag_status(\n    flag_name: str, session: AsyncSession = Depends(get_session)\n) -> Dict[str, Any]:",
      "docstring": "\n    Get the status of a specific flag by name.\n\n    Args:\n        flag_name (str): The name of the flag to check\n        session (Session): Database session\n\n    Returns:\n        Dict[str, Any]: Flag information including status\n    ",
      "function_calls": [
        "Depends(get_session)",
        "session.execute(select(Flag).where(Flag.name == flag_name))",
        "select(Flag)",
        "select(Flag).where(Flag.name == flag_name)",
        "result.scalar_one_or_none()",
        "HTTPException(status_code=404, detail=f\"Flag '{flag_name}' not found\")",
        "logger.error(\"Failed to get flag status: {}\", str(e))",
        "str(e)",
        "HTTPException(\n            status_code=500, detail=\"Failed to get flag status for {}\".format(flag_name)\n        )",
        "\"Failed to get flag status for {}\".format(flag_name)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1100,
      "end_line": 1119,
      "signature": "@app.get(\"/flags\", status_code=200)\nasync def get_all_flags(\n    session: AsyncSession = Depends(get_session),\n) -> List[Dict[str, Any]]:",
      "docstring": "\n    Get the status of all available flags.\n\n    Args:\n        session (Session): Database session\n\n    Returns:\n        List[Dict[str, Any]]: List of flag information\n    ",
      "function_calls": [
        "Depends(get_session)",
        "session.execute(select(Flag))",
        "select(Flag)",
        "result.scalars().all()",
        "result.scalars()",
        "logger.error(\"Failed to get flags: {}\", str(e))",
        "str(e)",
        "HTTPException(status_code=500, detail=\"Failed to get flags\")"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1122,
      "end_line": 1155,
      "signature": "@app.put(\"/flags/{flag_name}\", status_code=200)\nasync def set_flag_status(\n    flag_name: str, status: bool, session: AsyncSession = Depends(get_session)\n) -> Dict[str, Any]:",
      "docstring": "\n    Set the status of a specific flag by name.\n\n    Args:\n        flag_name (str): The name of the flag to set\n        status (bool): The status to set for the flag\n        session (Session): Database session\n\n    Returns:\n        Dict[str, Any]: Updated flag information\n    ",
      "function_calls": [
        "Depends(get_session)",
        "session.execute(select(Flag).where(Flag.name == flag_name))",
        "select(Flag).where(Flag.name == flag_name)",
        "select(Flag)",
        "result.scalar_one_or_none()",
        "Flag(name=flag_name, status=status)",
        "session.add(flag)",
        "logger.error(\"Failed to set flag status: {}\", str(e))",
        "str(e)",
        "HTTPException(\n            status_code=500, detail=\"Failed to set flag status for {}\".format(flag_name)\n        )",
        "\"Failed to set flag status for {}\".format(flag_name)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1175,
      "end_line": 1294,
      "signature": "@app.get(\n    \"/repository-status\",\n    response_model=GithubRepoStatus,\n)\nasync def get_repository_status(\n    repository_name: str = Query(..., description=\"The name of the repository\"),\n    repository_owner_name: str = Query(\n        ..., description=\"The name of the repository owner\"\n    ),\n    workflow_run_id: str = Query(\n        ..., description=\"The workflow run ID to fetch status for\"\n    ),\n    session: AsyncSession = Depends(get_session),\n) -> GithubRepoStatus:",
      "docstring": "\n    Get the current status of a repository workflow run and its associated codebase runs.\n    ",
      "function_calls": [
        "Query(..., description=\"The name of the repository\")",
        "Query(\n        ..., description=\"The name of the repository owner\"\n    )",
        "Query(\n        ..., description=\"The workflow run ID to fetch status for\"\n    )",
        "Depends(get_session)",
        "select(RepositoryWorkflowRun)",
        "select(RepositoryWorkflowRun).where(\n            RepositoryWorkflowRun.repository_name == repository_name,\n            RepositoryWorkflowRun.repository_owner_name == repository_owner_name,\n            RepositoryWorkflowRun.repository_workflow_run_id == workflow_run_id,\n        )",
        "(await session.execute(stmt)).scalar_one_or_none()",
        "session.execute(stmt)",
        "\"Workflow run {} not found for {}/{}\".format(\n                workflow_run_id, repository_name, repository_owner_name\n            )",
        "HTTPException(status_code=404, detail=error_msg)",
        "select(CodebaseWorkflowRun)",
        "select(CodebaseWorkflowRun).where(\n            CodebaseWorkflowRun.repository_name == repository_name,\n            CodebaseWorkflowRun.repository_owner_name == repository_owner_name,\n            CodebaseWorkflowRun.repository_workflow_run_id\n            == parent_run.repository_workflow_run_id,\n        )",
        "(await session.execute(cb_stmt)).scalars().all()",
        "(await session.execute(cb_stmt)).scalars()",
        "session.execute(cb_stmt)",
        "ErrorReport(**run.error_report)",
        "WorkflowRun(\n                codebase_workflow_run_id=run.codebase_workflow_run_id,\n                status=JobStatus(run.status),\n                started_at=run.started_at,\n                completed_at=run.completed_at,\n                error_report=error_report,\n                issue_tracking=IssueTracking(**run.issue_tracking)\n                if run.issue_tracking\n                else None,\n            )",
        "JobStatus(run.status)",
        "IssueTracking(**run.issue_tracking)",
        "codebase_data[codebase_folder].append(\n                (run.codebase_workflow_id, workflow_run)\n            )",
        "codebase_data.items()",
        "workflow_map[workflow_id].append(wf_run)",
        "workflow_map.items()",
        "workflows.append(\n                    WorkflowStatus(\n                        codebase_workflow_id=workflow_id,\n                        codebase_workflow_runs=workflow_runs,\n                    )\n                )",
        "WorkflowStatus(\n                        codebase_workflow_id=workflow_id,\n                        codebase_workflow_runs=workflow_runs,\n                    )",
        "codebases.append(\n                CodebaseStatus(codebase_folder=codebase_folder, workflows=workflows)\n            )",
        "CodebaseStatus(codebase_folder=codebase_folder, workflows=workflows)",
        "CodebaseStatusList(codebases=codebases)",
        "GithubRepoStatus(\n            repository_name=parent_run.repository_name,\n            repository_owner_name=parent_run.repository_owner_name,\n            repository_workflow_run_id=parent_run.repository_workflow_run_id,\n            repository_workflow_id=parent_run.repository_workflow_id,\n            issue_tracking=IssueTracking(**parent_run.issue_tracking)\n            if parent_run.issue_tracking\n            else None,\n            status=JobStatus(parent_run.status),\n            started_at=parent_run.started_at,\n            completed_at=parent_run.completed_at,\n            error_report=ErrorReport(**parent_run.error_report)\n            if parent_run.error_report\n            else None,\n            codebase_status_list=codebase_status_list,\n        )",
        "IssueTracking(**parent_run.issue_tracking)",
        "JobStatus(parent_run.status)",
        "ErrorReport(**parent_run.error_report)",
        "logger.error(\"Error retrieving repository status: {}\", str(e))",
        "str(e)",
        "HTTPException(\n            status_code=500,\n            detail=\"Error retrieving repository status: {}\".format(str(e)),\n        )",
        "\"Error retrieving repository status: {}\".format(str(e))",
        "str(e)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1297,
      "end_line": 1353,
      "signature": "@app.get(\n    \"/repository-data\",\n    response_model=GitHubRepoResponseConfiguration,\n)\nasync def get_repository_data(\n    repository_name: str = Query(..., description=\"The name of the repository\"),\n    repository_owner_name: str = Query(\n        ..., description=\"The name of the repository owner\"\n    ),\n    session: AsyncSession = Depends(get_session),\n) -> GitHubRepoResponseConfiguration:",
      "docstring": null,
      "function_calls": [
        "Query(..., description=\"The name of the repository\")",
        "Query(\n        ..., description=\"The name of the repository owner\"\n    )",
        "Depends(get_session)",
        "session.get(\n        Repository,\n        (repository_name, repository_owner_name),\n        options=[selectinload(cast(QueryableAttribute[Any], Repository.configs))],\n    )",
        "selectinload(cast(QueryableAttribute[Any], Repository.configs))",
        "cast(QueryableAttribute[Any], Repository.configs)",
        "HTTPException(\n            status_code=404,\n            detail=\"Repository data not found for {}/{}\".format(\n                repository_name, repository_owner_name\n            ),\n        )",
        "\"Repository data not found for {}/{}\".format(\n                repository_name, repository_owner_name\n            )",
        "CodebaseConfig(\n                codebase_folder=config.codebase_folder,\n                root_packages=config.root_packages,\n                programming_language_metadata=ProgrammingLanguageMetadata(\n                    language=config.programming_language_metadata[\"language\"],\n                    package_manager=config.programming_language_metadata[\n                        \"package_manager\"\n                    ],\n                    language_version=config.programming_language_metadata.get(\n                        \"language_version\"\n                    ),\n                ),\n            )",
        "ProgrammingLanguageMetadata(\n                    language=config.programming_language_metadata[\"language\"],\n                    package_manager=config.programming_language_metadata[\n                        \"package_manager\"\n                    ],\n                    language_version=config.programming_language_metadata.get(\n                        \"language_version\"\n                    ),\n                )",
        "config.programming_language_metadata.get(\n                        \"language_version\"\n                    )",
        "GitHubRepoResponseConfiguration(\n            repository_name=db_obj.repository_name,\n            repository_owner_name=db_obj.repository_owner_name,\n            repository_metadata=codebases,\n        )",
        "logger.error(\"Error mapping repository data: {}\", str(e))",
        "str(e)",
        "HTTPException(\n            status_code=500,\n            detail=\"Error processing repository data for {}/{}\".format(\n                repository_name, repository_owner_name\n            ),\n        )",
        "\"Error processing repository data for {}/{}\".format(\n                repository_name, repository_owner_name\n            )"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1356,
      "end_line": 1394,
      "signature": "@app.get(\n    \"/parent-workflow-jobs\",\n    response_model=ParentWorkflowJobListResponse,\n    description=\"Get all parent workflow jobs data without pagination\",\n)\nasync def get_parent_workflow_jobs(\n    session: AsyncSession = Depends(get_session),\n) -> ParentWorkflowJobListResponse:",
      "docstring": "Get all parent workflow jobs data without pagination.\n\n    Returns job information for all parent workflows (RepositoryWorkflowRun).\n    Includes repository_name, repository_owner_name, repository_workflow_run_id, status, started_at, completed_at.\n    ",
      "function_calls": [
        "Depends(get_session)",
        "select(RepositoryWorkflowRun)",
        "session.execute(query)",
        "result.scalars().all()",
        "result.scalars()",
        "ParentWorkflowJobResponse(\n                repository_name=run.repository_name,\n                repository_owner_name=run.repository_owner_name,\n                repository_workflow_run_id=run.repository_workflow_run_id,\n                status=JobStatus(run.status),\n                started_at=run.started_at,\n                completed_at=run.completed_at,\n            )",
        "JobStatus(run.status)",
        "ParentWorkflowJobListResponse(jobs=jobs)",
        "logger.error(\"Error retrieving parent workflow jobs: {}\", str(e))",
        "str(e)",
        "HTTPException(\n            status_code=500,\n            detail=\"Error retrieving parent workflow jobs: {}\".format(str(e)),\n        )",
        "\"Error retrieving parent workflow jobs: {}\".format(str(e))",
        "str(e)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1397,
      "end_line": 1433,
      "signature": "@app.get(\n    \"/get/ingestedRepositories\",\n    response_model=IngestedRepositoriesListResponse,\n    description=\"Get all ingested repositories without pagination\",\n)\nasync def get_ingested_repositories(\n    session: AsyncSession = Depends(get_session),\n) -> IngestedRepositoriesListResponse:",
      "docstring": "Get all ingested repositories without pagination.\n\n    Returns basic information for all repositories in the database.\n    Includes repository_name and repository_owner_name only.\n    ",
      "function_calls": [
        "Depends(get_session)",
        "select(Repository)",
        "session.execute(query)",
        "result.scalars().all()",
        "result.scalars()",
        "IngestedRepositoryResponse(\n                repository_name=repo.repository_name,\n                repository_owner_name=repo.repository_owner_name,\n                is_local=repo.is_local,\n                local_path=repo.local_path,\n            )",
        "IngestedRepositoriesListResponse(repositories=repo_list)",
        "logger.error(\"Error retrieving ingested repositories: {}\", str(e))",
        "str(e)",
        "HTTPException(\n            status_code=500,\n            detail=\"Error retrieving ingested repositories: {}\".format(str(e)),\n        )",
        "\"Error retrieving ingested repositories: {}\".format(str(e))",
        "str(e)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1436,
      "end_line": 1529,
      "signature": "@app.delete(\"/delete-repository\", status_code=200)\nasync def delete_repository(\n    repo_info: IngestedRepositoryResponse, session: AsyncSession = Depends(get_session)\n) -> Dict[str, Any]:",
      "docstring": "Delete a repository from both PostgreSQL and Neo4j databases.\n\n    This endpoint removes a repository and all its associated data including:\n    - Repository record and cascaded relations in PostgreSQL\n    - Repository node and all connected nodes/relationships in Neo4j\n\n    Args:\n        repo_info: IngestedRepositoryResponse containing repository_name and repository_owner_name\n        session: Database session\n\n    Returns:\n        Success message with deletion statistics\n\n    Raises:\n        HTTPException: 404 if repository not found, 500 on error\n    ",
      "function_calls": [
        "Depends(get_session)",
        "session.get(\n            Repository, (repository_name, repository_owner_name)\n        )",
        "HTTPException(\n                status_code=404,\n                detail=\"Repository not found: {}/{}\".format(\n                    repository_owner_name, repository_name\n                ),\n            )",
        "\"Repository not found: {}/{}\".format(\n                    repository_owner_name, repository_name\n                )",
        "session.delete(db_obj)",
        "session.commit()",
        "logger.info(\n            \"Deleted repository from PostgreSQL: {}/{}\",\n            repository_owner_name,\n            repository_name,\n        )",
        "\"{}_{}\".format(repository_owner_name, repository_name)",
        "app.state.code_confluence_graph.get_session()",
        "app.state.code_confluence_graph_deletion.delete_repository_by_qualified_name_managed(\n                    session=session, qualified_name=qualified_name\n                )",
        "logger.info(\"Deleted repository from Neo4j: {}\", qualified_name)",
        "logger.warning(\n                    \"Repository not found in Neo4j (may have been already deleted): {}\",\n                    qualified_name,\n                )",
        "\"Successfully deleted repository {}/{}\".format(\n                repository_owner_name, repository_name\n            )",
        "logger.error(\"Neo4j deletion error: {}\", str(e))",
        "str(e)",
        "HTTPException(\n            status_code=500,\n            detail=\"Failed to delete repository from graph database: {}\".format(str(e)),\n        )",
        "\"Failed to delete repository from graph database: {}\".format(str(e))",
        "str(e)",
        "logger.error(\"Error deleting repository: {}\", str(e))",
        "str(e)",
        "HTTPException(\n            status_code=500, detail=\"Error deleting repository: {}\".format(str(e))\n        )",
        "\"Error deleting repository: {}\".format(str(e))",
        "str(e)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1532,
      "end_line": 1710,
      "signature": "@app.post(\n    \"/refresh-repository\", response_model=RefreshRepositoryResponse, status_code=201\n)\nasync def refresh_repository(\n    repo_info: IngestedRepositoryResponse,\n    session: AsyncSession = Depends(get_session),\n    request_logger: \"Logger\" = Depends(trace_dependency),  # type: ignore\n) -> RefreshRepositoryResponse:",
      "docstring": "\n    Refresh a repository by purging Neo4j data and re-ingesting.\n\n    This endpoint:\n    1. Deletes all repository data from Neo4j (keeps PostgreSQL intact)\n    2. Re-detects codebases using PythonCodebaseDetector\n    3. Starts a new Temporal workflow for ingestion\n\n    Args:\n        repo_info: Repository name and owner\n        session: Database session\n        request_logger: Logger with trace ID\n\n    Returns:\n        RefreshRepositoryResponse with workflow IDs\n    ",
      "function_calls": [
        "Depends(get_session)",
        "Depends(trace_dependency)",
        "session.get(\n            Repository, (repository_name, repository_owner_name)\n        )",
        "HTTPException(\n                status_code=404,\n                detail=\"Repository not found in database: {}/{}\".format(\n                    repository_name, repository_owner_name\n                ),\n            )",
        "\"Repository not found in database: {}/{}\".format(\n                    repository_name, repository_owner_name\n                )",
        "\"{}_{}\".format(repository_owner_name, repository_name)",
        "app.state.code_confluence_graph.get_session()",
        "app.state.code_confluence_graph_deletion.delete_repository_by_qualified_name_managed(\n                    session=session, qualified_name=qualified_name\n                )",
        "request_logger.info(\"Deleted repository from Neo4j: {}\", qualified_name)",
        "request_logger.warning(\n                    \"Repository not found in Neo4j: {}\", qualified_name\n                )",
        "HTTPException(\n                    status_code=500, detail=f\"Neo4j deletion failed: {str(neo4j_error)}\"\n                )",
        "str(neo4j_error)",
        "fetch_github_token_from_db(session)",
        "HTTPException(\n                    status_code=400, detail=\"local_path required for local repositories\"\n                )",
        "os.path.isabs(local_path)",
        "os.path.basename(local_path)",
        "construct_local_repository_path(local_path)",
        "validate_local_repository_path(\n                folder_name_for_validation\n            )",
        "HTTPException(\n                    status_code=404,\n                    detail=f\"Local repository not found: {validated_path}\",\n                )",
        "request_logger.info(\"Refreshing local repository: {}\", actual_local_path)",
        "request_logger.info(\"Refreshing GitHub repository: {}\", repository_url)",
        "detector.detect_codebases(\n                    actual_local_path, github_token=github_token\n                )",
        "detector.detect_codebases(\n                    repository_url, github_token=github_token\n                )",
        "request_logger.info(\n                \"Detected {} codebases for {}/{}\",\n                len(detected_codebases),\n                repository_owner_name,\n                repository_name,\n            )",
        "len(detected_codebases)",
        "request_logger.error(\"Codebase detection failed: {}\", str(e))",
        "str(e)",
        "HTTPException(\n                status_code=500, detail=f\"Failed to detect codebases: {str(e)}\"\n            )",
        "str(e)",
        "GitHubRepoRequestConfiguration(\n            repository_name=repository_name,\n            repository_owner_name=repository_owner_name,\n            repository_git_url=repository_url,\n            repository_metadata=detected_codebases,\n            is_local=is_local,\n            local_path=actual_local_path if is_local else None,\n        )",
        "trace_id_var.get()",
        "HTTPException(500, \"trace_id not set by dependency\")",
        "start_workflow(\n            temporal_client=app.state.temporal_client,\n            repo_request=repo_request,\n            github_token=github_token,\n            workflow_id=f\"refresh-{repository_owner_name}-{repository_name}-{trace_id}\",\n            trace_id=trace_id,\n        )",
        "asyncio.create_task(monitor_workflow(workflow_handle))",
        "monitor_workflow(workflow_handle)",
        "request_logger.info(\n            f\"Started refresh workflow for {repository_owner_name}/{repository_name}. \"\n            f\"Workflow ID: {workflow_handle.id}, RunID: {workflow_handle.result_run_id}\"\n        )",
        "RefreshRepositoryResponse(\n            repository_name=repository_name,\n            repository_owner_name=repository_owner_name,\n            workflow_id=workflow_handle.id or \"\",\n            run_id=workflow_handle.result_run_id or \"\",\n        )",
        "request_logger.error(\"Error refreshing repository: {}\", str(e))",
        "str(e)",
        "HTTPException(\n            status_code=500, detail=f\"Error refreshing repository: {str(e)}\"\n        )",
        "str(e)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1713,
      "end_line": 1761,
      "signature": "@app.get(\"/user-details\", status_code=200)\nasync def get_user_details(\n    session: AsyncSession = Depends(get_session),\n) -> Dict[str, Optional[str]]:",
      "docstring": "\n    Fetch authenticated GitHub user's name, avatar URL, and email.\n    ",
      "function_calls": [
        "Depends(get_session)",
        "fetch_github_token_from_db(session)",
        "httpx.AsyncClient()",
        "client.get(\"https://api.github.com/user\", headers=headers)",
        "HTTPException(\n                status_code=user_resp.status_code, detail=\"Failed to fetch user info\"\n            )",
        "user_resp.json()",
        "user_data.get(\"email\")",
        "client.get(\n                \"https://api.github.com/user/emails\", headers=headers\n            )",
        "emails_resp.json()",
        "next(\n                    (\n                        e[\"email\"]\n                        for e in emails\n                        if e.get(\"primary\") and e.get(\"verified\")\n                    ),\n                    None,\n                )",
        "e.get(\"primary\")",
        "e.get(\"verified\")",
        "user_data.get(\"name\")",
        "user_data.get(\"avatar_url\")"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1764,
      "end_line": 1787,
      "signature": "@app.get(\"/detect-codebases-sse\")\nasync def detect_codebases_sse_endpoint(\n    git_url: str = Query(\n        ..., description=\"GitHub repository URL or folder name for local repository\"\n    ),\n    is_local: bool = Query(False, description=\"Whether this is a local repository\"),\n    session: AsyncSession = Depends(get_session),\n) -> StreamingResponse:",
      "docstring": "\n    Server-Sent Events endpoint for real-time codebase detection progress (v2).\n\n    This version directly uses PythonCodebaseDetector without the AsyncDetectorWrapper.\n    Streams progress updates during Python codebase auto-detection from GitHub repositories\n    or local Git repositories.\n\n    Args:\n        git_url: GitHub repository URL or folder name for local repository\n        is_local: Whether this is a local repository\n        session: Database session for token retrieval\n\n    Returns:\n        StreamingResponse with text/event-stream content type\n    ",
      "function_calls": [
        "Query(\n        ..., description=\"GitHub repository URL or folder name for local repository\"\n    )",
        "Query(False, description=\"Whether this is a local repository\")",
        "Depends(get_session)",
        "detect_codebases_sse(git_url, is_local, session)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1790,
      "end_line": 1959,
      "signature": "@app.post(\"/code-confluence/issues\", response_model=IssueTracking)\nasync def create_github_issue(\n    request: GithubIssueSubmissionRequest, session: AsyncSession = Depends(get_session)\n) -> IssueTracking:",
      "docstring": "\n    Create a GitHub issue based on error information and track it in the database.\n\n    This endpoint creates a GitHub issue using the provided error information and then updates\n    either the codebase workflow run or repository workflow run record with the issue details.\n    ",
      "function_calls": [
        "Depends(get_session)",
        "fetch_github_token_from_db(session)",
        "len(request.error_message_body)",
        "Github(token)",
        "g.get_repo(\"unoplat/unoplat-code-confluence\")",
        "repo.create_issue(\n                title=title,\n                body=body,\n                labels=labels,  # type: ignore\n            )",
        "logger.error(\"GitHub API error: {}\", str(e))",
        "str(e)",
        "HTTPException(\n                status_code=500, detail=f\"Failed to create GitHub issue: {str(e)}\"\n            )",
        "str(e)",
        "IssueTracking(\n            issue_id=str(github_issue.id),\n            issue_url=github_issue.html_url,\n            issue_status=IssueStatus.OPEN,\n        )",
        "str(github_issue.id)",
        "IssueTracking(\n            issue_id=issue_tracking.issue_id,\n            issue_number=github_issue.number,\n            issue_url=issue_tracking.issue_url,\n            issue_status=issue_tracking.issue_status,\n            created_at=datetime.now(timezone.utc).isoformat(),\n        )",
        "datetime.now(timezone.utc).isoformat()",
        "datetime.now(timezone.utc)",
        "issue_tracking_full.model_dump()",
        "select(CodebaseWorkflowRun)",
        "select(CodebaseWorkflowRun).where(condition)",
        "session.execute(codebase_run_query)",
        "codebase_run_result.scalar_one_or_none()",
        "HTTPException(\n                    status_code=404,\n                    detail=f\"Codebase workflow run {request.codebase_workflow_run_id} not found\",\n                )",
        "select(RepositoryWorkflowRun).where(condition)",
        "select(RepositoryWorkflowRun)",
        "session.execute(repo_run_query)",
        "repo_run_result.scalar_one_or_none()",
        "HTTPException(\n                    status_code=404,\n                    detail=f\"Repository workflow run {request.parent_workflow_run_id} not found\",\n                )",
        "session.commit()",
        "logger.error(\"Error creating GitHub issue: {}\", str(e))",
        "str(e)",
        "str(e)",
        "traceback.format_exc()",
        "HTTPException(\n            status_code=500,\n            detail={\n                \"message\": \"Faced an error while creating GitHub issue. Please try after some time.\",\n                \"error_context\": error_context,\n            },\n        )"
      ],
      "nested_functions": [],
      "instance_variables": []
    }
  ],
  "classes": []
}