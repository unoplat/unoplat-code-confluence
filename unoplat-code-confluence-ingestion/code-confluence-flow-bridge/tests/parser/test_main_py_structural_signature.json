{
  "module_docstring": null,
  "global_variables": [
    {
      "start_line": 154,
      "end_line": 154,
      "signature": "logger = setup_logging(service_name=\"code-confluence-flow-bridge\", app_name=\"unoplat-code-confluence\")"
    },
    {
      "start_line": 578,
      "end_line": 578,
      "signature": "origins: List[str] = os.getenv(\"ALLOWED_ORIGINS\", \"http://localhost:5173\").split(\",\")"
    },
    {
      "start_line": 575,
      "end_line": 575,
      "signature": "app = FastAPI(lifespan=lifespan)"
    }
  ],
  "functions": [
    {
      "start_line": 161,
      "end_line": 170,
      "signature": "async def get_temporal_client() -> Client:",
      "docstring": "Create and return a Temporal client instance.",
      "function_calls": [
        "os.getenv(\"TEMPORAL_SERVER_ADDRESS\", \"localhost:7233\")",
        "Client.connect(\n        temporal_server,\n        data_converter=pydantic_data_converter\n    )"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 173,
      "end_line": 180,
      "signature": "async def _serve_worker(\n    stop: asyncio.Event,\n    worker: Worker\n) -> None:",
      "docstring": "Keep the worker running until `stop` is set, then shut it down.",
      "function_calls": [
        "stop.wait()"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 183,
      "end_line": 304,
      "signature": "def create_worker(activities: List[Callable], client: Client, activity_executor: ThreadPoolExecutor, env_settings: EnvironmentSettings) -> Worker:",
      "docstring": "\n    Create a Temporal worker with given activities\n\n    Args:\n        activities: List of activity functions\n        client: Temporal client\n        activity_executor: Thread pool executor for activities\n        env_settings: Environment settings containing Temporal worker configuration\n        \n    Returns:\n        Worker: Configured Temporal worker instance\n    ",
      "function_calls": [
        "ParentWorkflowStatusInterceptor()",
        "ActivityStatusInterceptor()",
        "PollerBehaviorAutoscaling(\n                minimum=env_settings.temporal_workflow_poller_min,\n                initial=env_settings.temporal_workflow_poller_initial,\n                maximum=env_settings.temporal_workflow_poller_max\n            )",
        "PollerBehaviorAutoscaling(\n                minimum=env_settings.temporal_activity_poller_min,\n                initial=env_settings.temporal_activity_poller_initial,\n                maximum=env_settings.temporal_activity_poller_max\n            )",
        "logger.info(\n                f\"Starting Temporal worker with autoscaling pollers enabled. \"\n                f\"Workflow poller: min={env_settings.temporal_workflow_poller_min}, \"\n                f\"initial={env_settings.temporal_workflow_poller_initial}, \"\n                f\"max={env_settings.temporal_workflow_poller_max}. \"\n                f\"Activity poller: min={env_settings.temporal_activity_poller_min}, \"\n                f\"initial={env_settings.temporal_activity_poller_initial}, \"\n                f\"max={env_settings.temporal_activity_poller_max}\"\n            )",
        "logger.info(\n                f\"Starting Temporal worker with max_concurrent_activities={env_settings.temporal_max_concurrent_activities}, \"\n                f\"max_concurrent_activity_task_polls={env_settings.temporal_max_concurrent_activity_task_polls}\"\n            )",
        "Worker(\n                client,  # Client must be passed as a positional argument\n                task_queue=\"unoplat-code-confluence-repository-context-ingestion\",\n                workflows=[RepoWorkflow, CodebaseChildWorkflow],\n                activities=activities,\n                activity_executor=activity_executor,\n                interceptors=[ParentWorkflowStatusInterceptor(), ActivityStatusInterceptor()],\n                max_concurrent_activities=env_settings.temporal_max_concurrent_activities,\n                # Configure poller behaviors with autoscaling\n                workflow_task_poller_behavior=PollerBehaviorAutoscaling(\n                    minimum=env_settings.temporal_workflow_poller_min,\n                    initial=env_settings.temporal_workflow_poller_initial,\n                    maximum=env_settings.temporal_workflow_poller_max\n                ),\n                activity_task_poller_behavior=PollerBehaviorAutoscaling(\n                    minimum=env_settings.temporal_activity_poller_min,\n                    initial=env_settings.temporal_activity_poller_initial,\n                    maximum=env_settings.temporal_activity_poller_max\n                )\n            )",
        "ParentWorkflowStatusInterceptor()",
        "ActivityStatusInterceptor()",
        "PollerBehaviorAutoscaling(\n                    minimum=env_settings.temporal_workflow_poller_min,\n                    initial=env_settings.temporal_workflow_poller_initial,\n                    maximum=env_settings.temporal_workflow_poller_max\n                )",
        "PollerBehaviorAutoscaling(\n                    minimum=env_settings.temporal_activity_poller_min,\n                    initial=env_settings.temporal_activity_poller_initial,\n                    maximum=env_settings.temporal_activity_poller_max\n                )",
        "Worker(\n                client,  # Client must be passed as a positional argument\n                task_queue=\"unoplat-code-confluence-repository-context-ingestion\",\n                workflows=[RepoWorkflow, CodebaseChildWorkflow],\n                activities=activities,\n                activity_executor=activity_executor,\n                interceptors=[ParentWorkflowStatusInterceptor(), ActivityStatusInterceptor()],\n                max_concurrent_activities=env_settings.temporal_max_concurrent_activities,\n                max_concurrent_activity_task_polls=env_settings.temporal_max_concurrent_activity_task_polls\n            )",
        "ParentWorkflowStatusInterceptor()",
        "ActivityStatusInterceptor()",
        "str(e)",
        "traceback.format_exc()",
        "logger.error(\n            f\"Failed to start Temporal worker: {str(e)}\", \n            extra={\"error_context\": error_context}\n        )",
        "str(e)",
        "str(e)",
        "ApplicationError(\n            error_message,\n            type=\"WORKER_INITIALIZATION_ERROR\"\n        )"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 307,
      "end_line": 334,
      "signature": "async def fetch_github_token_from_db(session: AsyncSession) -> str:",
      "docstring": "\n    Fetch and decrypt GitHub token from database.\n    \n    Args:\n        session: Database session\n        \n    Returns:\n        Decrypted GitHub token\n        \n    Raises:\n        HTTPException: If no credentials found or decryption fails\n    ",
      "function_calls": [
        "session.execute(select(Credentials))",
        "select(Credentials)",
        "result.scalars().first()",
        "result.scalars()",
        "logger.error(f\"Database error while fetching credentials: {db_error}\")",
        "HTTPException(status_code=500, detail=\"Database error while fetching credentials\")",
        "HTTPException(status_code=404, detail=\"No credentials found\")",
        "decrypt_token(credential.token_hash)",
        "logger.error(f\"Failed to decrypt token: {decrypt_error}\")",
        "HTTPException(status_code=500, detail=\"Internal error during authentication token decryption\")"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 337,
      "end_line": 359,
      "signature": "async def start_workflow(\n    temporal_client: Client,\n    repo_request: GitHubRepoRequestConfiguration,\n    github_token: str,\n    workflow_id: str,\n    trace_id: str,\n) -> WorkflowHandle:",
      "docstring": "\n    Start a Temporal workflow for the given repository request and workflow id.\n    ",
      "function_calls": [
        "RepoWorkflowRunEnvelope(\n        repo_request=repo_request,\n        github_token=github_token,\n        trace_id=trace_id\n    )",
        "temporal_client.start_workflow(\n        RepoWorkflow.run,\n        arg=envelope,\n        id=workflow_id,\n        task_queue=\"unoplat-code-confluence-repository-context-ingestion\"\n    )",
        "logger.info(f\"Started workflow. Workflow ID: {workflow_handle.id}, RunID {workflow_handle.result_run_id}\")"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 362,
      "end_line": 449,
      "signature": "async def generate_sse_events(\n    git_url: str,\n    github_token: str,\n    detector_wrapper: AsyncDetectorWrapper\n) -> AsyncGenerator[str, None]:",
      "docstring": "\n    Generate SSE events for codebase detection progress.\n    \n    Args:\n        git_url: GitHub repository URL\n        github_token: GitHub authentication token\n        detector_wrapper: AsyncDetectorWrapper instance\n        \n    Yields:\n        SSE-formatted event strings\n    ",
      "function_calls": [
        "asyncio.Queue(maxsize=100)",
        "SSEMessage.connected()",
        "asyncio.create_task(\n        detector_wrapper.detect_codebases_async(\n            git_url=git_url,\n            github_token=github_token,\n            progress_queue=progress_queue\n        )\n    )",
        "detector_wrapper.detect_codebases_async(\n            git_url=git_url,\n            github_token=github_token,\n            progress_queue=progress_queue\n        )",
        "asyncio.wait_for(\n                    progress_queue.get(), \n                    timeout=1.0\n                )",
        "progress_queue.get()",
        "SSEMessage.format_sse(\n                    data={\n                        \"state\": progress.state.value,\n                        \"message\": progress.message,\n                        \"repository_url\": progress.repository_url\n                    },\n                    event=\"progress\"\n                )",
        "detection_task.done()",
        "SSEMessage.comment(\"heartbeat\")",
        "SSEMessage.result(result.model_dump())",
        "result.model_dump()",
        "SSEMessage.done()",
        "logger.info(\"Client disconnected, cancelling detection\")",
        "detection_task.done()",
        "detection_task.cancel()",
        "logger.error(f\"Detection error: {str(e)}\")",
        "str(e)",
        "SSEMessage.error(str(e), error_type=\"DETECTION_ERROR\")",
        "str(e)",
        "SSEMessage.done()"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 453,
      "end_line": 571,
      "signature": "@asynccontextmanager\nasync def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:",
      "docstring": null,
      "function_calls": [
        "EnvironmentSettings()",
        "get_temporal_client()",
        "CodeConfluenceGraph(code_confluence_env=app.state.code_confluence_env)",
        "app.state.code_confluence_graph.connect()",
        "app.state.code_confluence_graph.create_schema()",
        "logger.info(\"Neo4j connection and schema initialized successfully\")",
        "CodeConfluenceGraphDeletion(code_confluence_env=app.state.code_confluence_env)",
        "ThreadPoolExecutor(max_workers=pool_size)",
        "logger.info(f\"Initialized activity executor with {pool_size} threads (max_concurrent_activities={app.state.code_confluence_env.temporal_max_concurrent_activities} + 4 buffer threads)\")",
        "GitActivity()",
        "activities.append(git_activity.process_git_activity)",
        "ParentWorkflowDbActivity()",
        "activities.append(parent_workflow_db_activity.update_repository_workflow_status)",
        "ChildWorkflowDbActivity()",
        "activities.append(child_workflow_db_activity.update_codebase_workflow_status)",
        "PackageMetadataActivity()",
        "activities.append(package_metadata_activity.get_package_metadata)",
        "ConfluenceGitGraph()",
        "activities.append(confluence_git_graph.insert_git_repo_into_graph_db)",
        "PackageManagerMetadataIngestion()",
        "activities.append(codebase_package_ingestion.insert_package_manager_metadata)",
        "activities.append(process_codebase_generic)",
        "create_db_and_tables()",
        "os.getenv(\"LOAD_FRAMEWORK_DEFINITIONS\", \"true\").lower()",
        "os.getenv(\"LOAD_FRAMEWORK_DEFINITIONS\", \"true\")",
        "FrameworkDefinitionLoader(app.state.code_confluence_env)",
        "get_session_cm()",
        "framework_loader.load_framework_definitions_at_startup(session)",
        "metrics.get(\"skipped\")",
        "logger.info(f\"Framework definitions loaded in {metrics['total_time']:.3f}s\")",
        "logger.error(f\"Failed to load framework definitions: {e}\")",
        "os.getenv(\"FRAMEWORK_DEFINITIONS_REQUIRED\", \"false\").lower()",
        "os.getenv(\"FRAMEWORK_DEFINITIONS_REQUIRED\", \"false\")",
        "create_worker(\n        activities=activities,\n        client=app.state.temporal_client,\n        activity_executor=app.state.activity_executor,\n        env_settings=app.state.code_confluence_env\n    )",
        "asyncio.Event()",
        "asyncio.create_task(_serve_worker(stop_event, worker))",
        "_serve_worker(stop_event, worker)",
        "logger.info(\"Shutting down application...\")",
        "stop_event.set()",
        "logger.info(\"Temporal worker shut down successfully\")",
        "logger.error(f\"Error during worker shutdown: {e}\")",
        "app.state.code_confluence_graph.close()",
        "logger.info(\"Neo4j global connection closed\")",
        "logger.error(f\"Error closing Neo4j connections: {e}\")",
        "async_engine.dispose()",
        "logger.info(\"SQLAlchemy engine disposed\")",
        "logger.warning(f\"Failed to dispose async engine during shutdown: {exc}\")",
        "app.state.activity_executor.shutdown(wait=True)",
        "logger.info(\"Thread pool executor shut down\")",
        "logger.error(f\"Error shutting down thread pool executor: {e}\")"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 590,
      "end_line": 595,
      "signature": "async def monitor_workflow(workflow_handle: WorkflowHandle) -> None:",
      "docstring": null,
      "function_calls": [
        "workflow_handle.result()",
        "logger.info(f\"Workflow completed with result: {result}\")",
        "logger.error(f\"Workflow failed: {e}\")"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 598,
      "end_line": 640,
      "signature": "@app.post(\"/ingest-token\", status_code=201)\nasync def ingest_token(authorization: str = Header(...), session: AsyncSession = Depends(get_session)) -> Dict[str, str]:",
      "docstring": null,
      "function_calls": [
        "Header(...)",
        "Depends(get_session)",
        "authorization.startswith(\"Bearer \")",
        "HTTPException(status_code=401, detail=\"Invalid Authorization header\")",
        "authorization[7:].strip()",
        "encrypt_token(token)",
        "datetime.now(timezone.utc)",
        "session.execute(select(Credentials))",
        "select(Credentials)",
        "result.scalars().first()",
        "result.scalars()",
        "HTTPException(status_code=409, detail=\"Token already ingested. Use update-token to update it.\")",
        "Credentials(token_hash=encrypted_token, created_at=current_time)",
        "session.add(credential)",
        "session.execute(select(Flag).where(Flag.name == \"isTokenSubmitted\"))",
        "select(Flag)",
        "select(Flag).where(Flag.name == \"isTokenSubmitted\")",
        "flag_result.scalar_one_or_none()",
        "Flag(name=\"isTokenSubmitted\", status=True)",
        "session.add(token_flag)",
        "session.commit()",
        "session.refresh(credential)",
        "logger.error(f\"Failed to process token: {str(e)}\")",
        "str(e)",
        "HTTPException(status_code=500, detail=\"Failed to process authentication token\")"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 642,
      "end_line": 665,
      "signature": "@app.put(\"/update-token\", status_code=200)\nasync def update_token(authorization: str = Header(...), session: AsyncSession = Depends(get_session)) -> Dict[str, str]:",
      "docstring": null,
      "function_calls": [
        "Header(...)",
        "Depends(get_session)",
        "authorization.startswith(\"Bearer \")",
        "HTTPException(status_code=401, detail=\"Invalid Authorization header\")",
        "authorization[7:].strip()",
        "encrypt_token(token)",
        "datetime.now(timezone.utc)",
        "session.execute(select(Credentials))",
        "select(Credentials)",
        "result.scalars().first()",
        "result.scalars()",
        "HTTPException(status_code=404, detail=\"No token found to update\")",
        "session.add(credential)",
        "session.commit()",
        "session.refresh(credential)",
        "logger.error(f\"Failed to update token: {str(e)}\")",
        "str(e)",
        "HTTPException(status_code=500, detail=\"Failed to update authentication token\")"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 667,
      "end_line": 699,
      "signature": "@app.delete(\"/delete-token\", status_code=200)\nasync def delete_token(session: AsyncSession = Depends(get_session)) -> Dict[str, str]:",
      "docstring": null,
      "function_calls": [
        "Depends(get_session)",
        "session.execute(select(Credentials))",
        "select(Credentials)",
        "result.scalars().first()",
        "result.scalars()",
        "HTTPException(status_code=404, detail=\"No token found to delete\")",
        "session.delete(credential)",
        "session.execute(select(Flag).where(Flag.name == \"isTokenSubmitted\"))",
        "select(Flag).where(Flag.name == \"isTokenSubmitted\")",
        "select(Flag)",
        "flag_result.scalar_one_or_none()",
        "Flag(name=\"isTokenSubmitted\", status=False)",
        "session.add(token_flag)",
        "session.commit()",
        "logger.error(f\"Failed to delete token: {str(e)}\")",
        "str(e)",
        "HTTPException(status_code=500, detail=\"Failed to delete authentication token\")"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 703,
      "end_line": 842,
      "signature": "@app.get(\"/repos\", response_model=PaginatedResponse)\nasync def get_repos(\n    per_page: int = Query(30, ge=1, le=100, description=\"Items per page\"),\n    cursor: Optional[str] = Query(None, description=\"Pagination cursor\"),\n    filterValues: Optional[str] = Query(None, description=\"Optional JSON filter values to filter repositories\"),\n    session: AsyncSession = Depends(get_session)\n) -> PaginatedResponse:",
      "docstring": null,
      "function_calls": [
        "Query(30, ge=1, le=100, description=\"Items per page\")",
        "Query(None, description=\"Pagination cursor\")",
        "Query(None, description=\"Optional JSON filter values to filter repositories\")",
        "Depends(get_session)",
        "fetch_github_token_from_db(session)",
        "json.loads(filterValues)",
        "logger.error(f\"Invalid JSON in filterValues: {e}\")",
        "HTTPException(status_code=400, detail=\"Invalid JSON in filterValues query parameter\")",
        "AIOHTTPTransport(\n            url=\"https://api.github.com/graphql\",\n            headers={\n                \"Authorization\": f\"Bearer {token}\",\n                \"User-Agent\": \"Unoplat Code Confluence\"\n            }\n        )",
        "GQLClient(\n            transport=transport,\n            fetch_schema_from_transport=False,\n        )",
        "gql(\n                    \"\"\"\n                    query SearchRepositories($query: String!, $first: Int!, $after: String) {\n                        search(query: $query, type: REPOSITORY, first: $first, after: $after) {\n                            pageInfo {\n                                endCursor\n                                hasNextPage\n                            }\n                            nodes {\n                                ... on Repository {\n                                    name\n                                    isPrivate\n                                    url\n                                    owner {\n                                        login\n                                        url\n                                    }\n                                }\n                            }\n                        }\n                    }\n                    \"\"\"\n                )",
        "client.execute(query, variable_values={\n                    \"query\": search_query,\n                    \"first\": per_page,\n                    \"after\": cursor\n                })",
        "gql(\n                    \"\"\"\n                    query GetRepositories($first: Int!, $after: String) {\n                        viewer {\n                            repositories(\n                                first: $first,\n                                affiliations: [OWNER, COLLABORATOR, ORGANIZATION_MEMBER],\n                                after: $after\n                            ) {\n                                pageInfo {\n                                    endCursor\n                                    hasNextPage\n                                }\n                                nodes {\n                                    name\n                                    isPrivate\n                                    url\n                                    owner {\n                                        login\n                                        url\n                                    }\n                                }\n                            }\n                        }\n                    }\n                    \"\"\"\n                )",
        "client.execute(query, variable_values={\n                    \"first\": per_page,\n                    \"after\": cursor\n                })",
        "GitHubRepoSummary(\n                    name=item[\"name\"],\n                    owner_url=item[\"owner\"][\"url\"],\n                    private=item[\"isPrivate\"],\n                    git_url=item[\"url\"],\n                    owner_name=item[\"owner\"][\"login\"]\n                )",
        "repos_list.append(repo_summary)",
        "PaginatedResponse(\n                items=repos_list,\n                per_page=per_page,\n                has_next=has_next,\n                next_cursor=next_cursor\n            )",
        "logger.error(f\"GraphQL Error: {str(e)}\")",
        "str(e)",
        "HTTPException(status_code=500, detail=f\"Failed to fetch repositories: {str(e)}\")",
        "str(e)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 844,
      "end_line": 907,
      "signature": "@app.post(\"/start-ingestion\", status_code=201)\nasync def ingestion(\n    repo_request: GitHubRepoRequestConfiguration,\n    session: AsyncSession = Depends(get_session),\n    request_logger: \"Logger\" = Depends(trace_dependency) #type: ignore\n) -> dict[str, str]:",
      "docstring": "\n    Start the ingestion workflow for the entire repository using the GitHub token from the database.\n    Submits the whole repo_request at once to the Temporal workflow.\n    Returns the workflow_id and run_id.\n    Also ingests the repository configuration into the database.\n    ",
      "function_calls": [
        "Depends(get_session)",
        "Depends(trace_dependency)",
        "construct_local_repository_path(folder_name)",
        "request_logger.info(f\"Local repository ingestion - folder: {folder_name}, resolved path: {actual_path}, environment: {get_runtime_environment()}\")",
        "get_runtime_environment()",
        "extract_github_organization_from_local_repo(actual_path, original_owner)",
        "request_logger.info(f\"Local repository GitHub organization - original: {original_owner}, extracted: {github_organization}\")",
        "build_trace_id(repo_request.repository_name, github_organization)",
        "trace_id_var.set(updated_trace_id)",
        "request_logger.info(f\"Updated trace_id for workflow consistency - new: {updated_trace_id}\")",
        "fetch_github_token_from_db(session)",
        "trace_id_var.get()",
        "HTTPException(500, \"trace_id not set by dependency\")",
        "start_workflow(\n        temporal_client=app.state.temporal_client,\n        repo_request=repo_request,\n        github_token=github_token,\n        workflow_id=f\"ingest-{trace_id}\",\n        trace_id=trace_id,\n    )",
        "asyncio.create_task(monitor_workflow(workflow_handle))",
        "monitor_workflow(workflow_handle)",
        "request_logger.info(f\"Started workflow. Workflow ID: {workflow_handle.id}, RunID {workflow_handle.result_run_id}\")"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 909,
      "end_line": 936,
      "signature": "@app.get(\"/flags/{flag_name}\", status_code=200)\nasync def get_flag_status(flag_name: str, session: AsyncSession = Depends(get_session)) -> Dict[str, Any]:",
      "docstring": "\n    Get the status of a specific flag by name.\n    \n    Args:\n        flag_name (str): The name of the flag to check\n        session (Session): Database session\n        \n    Returns:\n        Dict[str, Any]: Flag information including status\n    ",
      "function_calls": [
        "Depends(get_session)",
        "session.execute(select(Flag).where(Flag.name == flag_name))",
        "select(Flag)",
        "select(Flag).where(Flag.name == flag_name)",
        "result.scalar_one_or_none()",
        "HTTPException(status_code=404, detail=f\"Flag '{flag_name}' not found\")",
        "logger.error(f\"Failed to get flag status: {str(e)}\")",
        "str(e)",
        "HTTPException(status_code=500, detail=f\"Failed to get flag status for {flag_name}\")"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 938,
      "end_line": 961,
      "signature": "@app.get(\"/flags\", status_code=200)\nasync def get_all_flags(session: AsyncSession = Depends(get_session)) -> List[Dict[str, Any]]:",
      "docstring": "\n    Get the status of all available flags.\n    \n    Args:\n        session (Session): Database session\n        \n    Returns:\n        List[Dict[str, Any]]: List of flag information\n    ",
      "function_calls": [
        "Depends(get_session)",
        "session.execute(select(Flag))",
        "select(Flag)",
        "result.scalars()",
        "result.scalars().all()",
        "logger.error(f\"Failed to get flags: {str(e)}\")",
        "str(e)",
        "HTTPException(status_code=500, detail=\"Failed to get flags\")"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 963,
      "end_line": 997,
      "signature": "@app.put(\"/flags/{flag_name}\", status_code=200)\nasync def set_flag_status(flag_name: str, status: bool, session: AsyncSession = Depends(get_session)) -> Dict[str, Any]:",
      "docstring": "\n    Set the status of a specific flag by name.\n    \n    Args:\n        flag_name (str): The name of the flag to set\n        status (bool): The status to set for the flag\n        session (Session): Database session\n        \n    Returns:\n        Dict[str, Any]: Updated flag information\n    ",
      "function_calls": [
        "Depends(get_session)",
        "session.execute(select(Flag).where(Flag.name == flag_name))",
        "select(Flag).where(Flag.name == flag_name)",
        "select(Flag)",
        "result.scalar_one_or_none()",
        "Flag(name=flag_name, status=status)",
        "session.add(flag)",
        "session.commit()",
        "session.refresh(flag)",
        "logger.error(f\"Failed to set flag status: {str(e)}\")",
        "str(e)",
        "HTTPException(status_code=500, detail=f\"Failed to set flag status for {flag_name}\")"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1015,
      "end_line": 1116,
      "signature": "@app.get(\n    \"/repository-status\",\n    response_model=GithubRepoStatus,\n)\nasync def get_repository_status(\n    repository_name: str = Query(..., description=\"The name of the repository\"),\n    repository_owner_name: str = Query(..., description=\"The name of the repository owner\"),\n    workflow_run_id: str = Query(..., description=\"The workflow run ID to fetch status for\"),\n    session: AsyncSession = Depends(get_session),\n) -> GithubRepoStatus:",
      "docstring": "\n    Get the current status of a repository workflow run and its associated codebase runs.\n    ",
      "function_calls": [
        "Query(..., description=\"The name of the repository\")",
        "Query(..., description=\"The name of the repository owner\")",
        "Query(..., description=\"The workflow run ID to fetch status for\")",
        "Depends(get_session)",
        "select(RepositoryWorkflowRun).where(\n            RepositoryWorkflowRun.repository_name == repository_name,\n            RepositoryWorkflowRun.repository_owner_name == repository_owner_name,\n            RepositoryWorkflowRun.repository_workflow_run_id == workflow_run_id\n        )",
        "select(RepositoryWorkflowRun)",
        "(await session.execute(stmt)).scalar_one_or_none()",
        "session.execute(stmt)",
        "HTTPException(status_code=404, detail=error_msg)",
        "select(CodebaseWorkflowRun)",
        "select(CodebaseWorkflowRun).where(\n            CodebaseWorkflowRun.repository_name == repository_name,\n            CodebaseWorkflowRun.repository_owner_name == repository_owner_name,\n            CodebaseWorkflowRun.repository_workflow_run_id == parent_run.repository_workflow_run_id,\n        )",
        "(await session.execute(cb_stmt)).scalars().all()",
        "(await session.execute(cb_stmt)).scalars()",
        "session.execute(cb_stmt)",
        "ErrorReport(**run.error_report)",
        "WorkflowRun(\n                codebase_workflow_run_id=run.codebase_workflow_run_id,\n                status=JobStatus(run.status),\n                started_at=run.started_at,\n                completed_at=run.completed_at,\n                error_report=error_report,\n                issue_tracking=IssueTracking(**run.issue_tracking) if run.issue_tracking else None\n            )",
        "JobStatus(run.status)",
        "IssueTracking(**run.issue_tracking)",
        "codebase_data[codebase_folder].append((run.codebase_workflow_id, workflow_run))",
        "codebase_data.items()",
        "workflow_map[workflow_id].append(wf_run)",
        "workflow_map.items()",
        "workflows.append(WorkflowStatus(\n                    codebase_workflow_id=workflow_id,\n                    codebase_workflow_runs=workflow_runs\n                ))",
        "WorkflowStatus(\n                    codebase_workflow_id=workflow_id,\n                    codebase_workflow_runs=workflow_runs\n                )",
        "codebases.append(CodebaseStatus(\n                codebase_folder=codebase_folder,\n                workflows=workflows\n            ))",
        "CodebaseStatus(\n                codebase_folder=codebase_folder,\n                workflows=workflows\n            )",
        "CodebaseStatusList(codebases=codebases)",
        "GithubRepoStatus(\n            repository_name=parent_run.repository_name,\n            repository_owner_name=parent_run.repository_owner_name,\n            repository_workflow_run_id=parent_run.repository_workflow_run_id,\n            repository_workflow_id=parent_run.repository_workflow_id,\n            issue_tracking=IssueTracking(**parent_run.issue_tracking) if parent_run.issue_tracking else None,\n            status=JobStatus(parent_run.status),\n            started_at=parent_run.started_at,\n            completed_at=parent_run.completed_at,\n            error_report=ErrorReport(**parent_run.error_report) if parent_run.error_report else None,\n            codebase_status_list=codebase_status_list\n        )",
        "IssueTracking(**parent_run.issue_tracking)",
        "JobStatus(parent_run.status)",
        "ErrorReport(**parent_run.error_report)",
        "logger.error(f\"Error retrieving repository status: {str(e)}\")",
        "str(e)",
        "HTTPException(\n            status_code=500,\n            detail=f\"Error retrieving repository status: {str(e)}\"\n        )",
        "str(e)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1118,
      "end_line": 1169,
      "signature": "@app.get(\n    \"/repository-data\",\n    response_model=GitHubRepoResponseConfiguration,\n)\nasync def get_repository_data(\n    repository_name: str = Query(..., description=\"The name of the repository\"),\n    repository_owner_name: str = Query(..., description=\"The name of the repository owner\"),\n    session: AsyncSession = Depends(get_session),\n) -> GitHubRepoResponseConfiguration:",
      "docstring": null,
      "function_calls": [
        "Query(..., description=\"The name of the repository\")",
        "Query(..., description=\"The name of the repository owner\")",
        "Depends(get_session)",
        "session.get(\n        Repository,\n        (repository_name, repository_owner_name),\n        options=[\n            selectinload(\n                cast(QueryableAttribute[Any], Repository.configs)\n            )\n        ]\n    )",
        "selectinload(\n                cast(QueryableAttribute[Any], Repository.configs)\n            )",
        "cast(QueryableAttribute[Any], Repository.configs)",
        "HTTPException(\n            status_code=404,\n            detail=f\"Repository data not found for {repository_name}/{repository_owner_name}\"\n        )",
        "CodebaseConfig(\n                codebase_folder=config.codebase_folder,\n                root_packages=config.root_packages,\n                programming_language_metadata=ProgrammingLanguageMetadata(\n                    language=config.programming_language_metadata[\"language\"],\n                    package_manager=config.programming_language_metadata[\"package_manager\"],\n                    language_version=config.programming_language_metadata.get(\"language_version\"),\n                    role=config.programming_language_metadata.get(\"role\", \"NA\"),\n                ),\n            )",
        "ProgrammingLanguageMetadata(\n                    language=config.programming_language_metadata[\"language\"],\n                    package_manager=config.programming_language_metadata[\"package_manager\"],\n                    language_version=config.programming_language_metadata.get(\"language_version\"),\n                    role=config.programming_language_metadata.get(\"role\", \"NA\"),\n                )",
        "config.programming_language_metadata.get(\"language_version\")",
        "config.programming_language_metadata.get(\"role\", \"NA\")",
        "GitHubRepoResponseConfiguration(\n            repository_name=db_obj.repository_name,\n            repository_owner_name=db_obj.repository_owner_name,\n            repository_metadata=codebases,\n        )",
        "logger.error(f\"Error mapping repository data: {str(e)}\")",
        "str(e)",
        "HTTPException(\n            status_code=500,\n            detail=f\"Error processing repository data for {repository_name}/{repository_owner_name}\"\n        )"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1171,
      "end_line": 1206,
      "signature": "@app.get(\n    \"/parent-workflow-jobs\",\n    response_model=ParentWorkflowJobListResponse,\n    description=\"Get all parent workflow jobs data without pagination\"\n)\nasync def get_parent_workflow_jobs(session: AsyncSession = Depends(get_session)) -> ParentWorkflowJobListResponse:",
      "docstring": "Get all parent workflow jobs data without pagination.\n    \n    Returns job information for all parent workflows (RepositoryWorkflowRun).\n    Includes repository_name, repository_owner_name, repository_workflow_run_id, status, started_at, completed_at.\n    ",
      "function_calls": [
        "Depends(get_session)",
        "select(RepositoryWorkflowRun)",
        "session.execute(query)",
        "result.scalars()",
        "result.scalars().all()",
        "ParentWorkflowJobResponse(\n                repository_name=run.repository_name,\n                repository_owner_name=run.repository_owner_name,\n                repository_workflow_run_id=run.repository_workflow_run_id,\n                status=JobStatus(run.status),\n                started_at=run.started_at,\n                completed_at=run.completed_at\n            )",
        "JobStatus(run.status)",
        "ParentWorkflowJobListResponse(jobs=jobs)",
        "logger.error(f\"Error retrieving parent workflow jobs: {str(e)}\")",
        "str(e)",
        "HTTPException(\n            status_code=500,\n            detail=f\"Error retrieving parent workflow jobs: {str(e)}\"\n        )",
        "str(e)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1208,
      "end_line": 1241,
      "signature": "@app.get(\n    \"/get/ingestedRepositories\",\n    response_model=IngestedRepositoriesListResponse,\n    description=\"Get all ingested repositories without pagination\"\n)\nasync def get_ingested_repositories(session: AsyncSession = Depends(get_session)) -> IngestedRepositoriesListResponse:",
      "docstring": "Get all ingested repositories without pagination.\n    \n    Returns basic information for all repositories in the database.\n    Includes repository_name and repository_owner_name only.\n    ",
      "function_calls": [
        "Depends(get_session)",
        "select(Repository)",
        "session.execute(query)",
        "result.scalars().all()",
        "result.scalars()",
        "IngestedRepositoryResponse(\n                repository_name=repo.repository_name,\n                repository_owner_name=repo.repository_owner_name,\n                is_local=repo.is_local,\n                local_path=repo.local_path\n            )",
        "IngestedRepositoriesListResponse(repositories=repo_list)",
        "logger.error(f\"Error retrieving ingested repositories: {str(e)}\")",
        "str(e)",
        "HTTPException(\n            status_code=500,\n            detail=f\"Error retrieving ingested repositories: {str(e)}\"\n        )",
        "str(e)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1243,
      "end_line": 1324,
      "signature": "@app.delete(\"/delete-repository\", status_code=200)\nasync def delete_repository(\n    repo_info: IngestedRepositoryResponse,\n    session: AsyncSession = Depends(get_session)\n) -> Dict[str, Any]:",
      "docstring": "Delete a repository from both PostgreSQL and Neo4j databases.\n    \n    This endpoint removes a repository and all its associated data including:\n    - Repository record and cascaded relations in PostgreSQL\n    - Repository node and all connected nodes/relationships in Neo4j\n    \n    Args:\n        repo_info: IngestedRepositoryResponse containing repository_name and repository_owner_name\n        session: Database session\n        \n    Returns:\n        Success message with deletion statistics\n        \n    Raises:\n        HTTPException: 404 if repository not found, 500 on error\n    ",
      "function_calls": [
        "Depends(get_session)",
        "session.get(\n            Repository,\n            (repository_name, repository_owner_name)\n        )",
        "HTTPException(\n                status_code=404,\n                detail=f\"Repository not found: {repository_owner_name}/{repository_name}\"\n            )",
        "session.delete(db_obj)",
        "session.commit()",
        "logger.info(f\"Deleted repository from PostgreSQL: {repository_owner_name}/{repository_name}\")",
        "app.state.code_confluence_graph_deletion.delete_repository_by_qualified_name(\n                qualified_name=qualified_name\n            )",
        "logger.info(f\"Deleted repository from Neo4j: {qualified_name}\")",
        "logger.warning(f\"Repository not found in Neo4j (may have been already deleted): {qualified_name}\")",
        "logger.error(f\"Neo4j deletion error: {str(e)}\")",
        "str(e)",
        "HTTPException(\n            status_code=500,\n            detail=f\"Failed to delete repository from graph database: {str(e)}\"\n        )",
        "str(e)",
        "logger.error(f\"Error deleting repository: {str(e)}\")",
        "str(e)",
        "HTTPException(\n            status_code=500,\n            detail=f\"Error deleting repository: {str(e)}\"\n        )",
        "str(e)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1326,
      "end_line": 1477,
      "signature": "@app.post(\"/refresh-repository\", response_model=RefreshRepositoryResponse, status_code=201)\nasync def refresh_repository(\n    repo_info: IngestedRepositoryResponse,\n    session: AsyncSession = Depends(get_session),\n    request_logger: \"Logger\" = Depends(trace_dependency) #type: ignore\n) -> RefreshRepositoryResponse:",
      "docstring": "\n    Refresh a repository by purging Neo4j data and re-ingesting.\n    \n    This endpoint:\n    1. Deletes all repository data from Neo4j (keeps PostgreSQL intact)\n    2. Re-detects codebases using PythonCodebaseDetector\n    3. Starts a new Temporal workflow for ingestion\n    \n    Args:\n        repo_info: Repository name and owner\n        session: Database session\n        request_logger: Logger with trace ID\n        \n    Returns:\n        RefreshRepositoryResponse with workflow IDs\n    ",
      "function_calls": [
        "Depends(get_session)",
        "Depends(trace_dependency)",
        "session.get(\n            Repository,\n            (repository_name, repository_owner_name)\n        )",
        "HTTPException(\n                status_code=404,\n                detail=f\"Repository not found in database: {repository_name}/{repository_owner_name}\"\n            )",
        "app.state.code_confluence_graph_deletion.delete_repository_by_qualified_name(\n                qualified_name=qualified_name\n            )",
        "request_logger.info(f\"Deleted repository from Neo4j: {qualified_name}\")",
        "request_logger.warning(f\"Repository not found in Neo4j: {qualified_name}\")",
        "HTTPException(status_code=500, detail=f\"Neo4j deletion failed: {str(neo4j_error)}\")",
        "str(neo4j_error)",
        "fetch_github_token_from_db(session)",
        "HTTPException(status_code=400, detail=\"local_path required for local repositories\")",
        "os.path.isabs(local_path)",
        "os.path.basename(local_path)",
        "construct_local_repository_path(local_path)",
        "validate_local_repository_path(folder_name_for_validation)",
        "HTTPException(status_code=404, detail=f\"Local repository not found: {validated_path}\")",
        "request_logger.info(f\"Refreshing local repository: {actual_local_path}\")",
        "request_logger.info(f\"Refreshing GitHub repository: {repository_url}\")",
        "PythonCodebaseDetector(github_token=github_token)",
        "detector.detect_codebases(actual_local_path)",
        "detector.detect_codebases(repository_url)",
        "request_logger.info(f\"Detected {len(detected_codebases)} codebases for {repository_owner_name}/{repository_name}\")",
        "len(detected_codebases)",
        "request_logger.error(f\"Codebase detection failed: {str(e)}\")",
        "str(e)",
        "HTTPException(status_code=500, detail=f\"Failed to detect codebases: {str(e)}\")",
        "str(e)",
        "GitHubRepoRequestConfiguration(\n            repository_name=repository_name,\n            repository_owner_name=repository_owner_name,\n            repository_git_url=repository_url,\n            repository_metadata=detected_codebases,\n            is_local=is_local,\n            local_path=actual_local_path if is_local else None\n        )",
        "trace_id_var.get()",
        "HTTPException(500, \"trace_id not set by dependency\")",
        "start_workflow(\n            temporal_client=app.state.temporal_client,\n            repo_request=repo_request,\n            github_token=github_token,\n            workflow_id=f\"refresh-{repository_owner_name}-{repository_name}-{trace_id}\",\n            trace_id=trace_id\n        )",
        "asyncio.create_task(monitor_workflow(workflow_handle))",
        "monitor_workflow(workflow_handle)",
        "request_logger.info(\n            f\"Started refresh workflow for {repository_owner_name}/{repository_name}. \"\n            f\"Workflow ID: {workflow_handle.id}, RunID: {workflow_handle.result_run_id}\"\n        )",
        "RefreshRepositoryResponse(\n            repository_name=repository_name,\n            repository_owner_name=repository_owner_name,\n            workflow_id=workflow_handle.id or \"\",\n            run_id=workflow_handle.result_run_id or \"\"\n        )",
        "request_logger.error(f\"Error refreshing repository: {str(e)}\")",
        "str(e)",
        "HTTPException(\n            status_code=500,\n            detail=f\"Error refreshing repository: {str(e)}\"\n        )",
        "str(e)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1479,
      "end_line": 1514,
      "signature": "@app.get(\"/user-details\", status_code=200)\nasync def get_user_details(session: AsyncSession = Depends(get_session)) -> Dict[str, Optional[str]]:",
      "docstring": "\n    Fetch authenticated GitHub user's name, avatar URL, and email.\n    ",
      "function_calls": [
        "Depends(get_session)",
        "fetch_github_token_from_db(session)",
        "httpx.AsyncClient()",
        "client.get(\"https://api.github.com/user\", headers=headers)",
        "HTTPException(status_code=user_resp.status_code, detail=\"Failed to fetch user info\")",
        "user_resp.json()",
        "user_data.get(\"email\")",
        "client.get(\"https://api.github.com/user/emails\", headers=headers)",
        "emails_resp.json()",
        "next((e[\"email\"] for e in emails if e.get(\"primary\") and e.get(\"verified\")), None)",
        "e.get(\"primary\")",
        "e.get(\"verified\")",
        "user_data.get(\"name\")",
        "user_data.get(\"avatar_url\")"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1517,
      "end_line": 1564,
      "signature": "@app.get(\"/detect-codebases-sse\")\nasync def detect_codebases_sse(\n    git_url: str = Query(..., description=\"GitHub repository URL or folder name for local repository\"),\n    is_local: bool = Query(False, description=\"Whether this is a local repository\"),\n    session: AsyncSession = Depends(get_session)\n):",
      "docstring": "\n    Server-Sent Events endpoint for real-time codebase detection progress.\n    \n    Streams progress updates during Python codebase auto-detection from GitHub repositories\n    or local Git repositories.\n    Uses FastAPI's StreamingResponse with proper SSE formatting.\n    \n    Args:\n        git_url: GitHub repository URL or folder name for local repository\n        is_local: Whether this is a local repository\n        session: Database session for token retrieval\n        \n    Returns:\n        StreamingResponse with text/event-stream content type\n    ",
      "function_calls": [
        "Query(..., description=\"GitHub repository URL or folder name for local repository\")",
        "Query(False, description=\"Whether this is a local repository\")",
        "Depends(get_session)",
        "construct_local_repository_path(git_url)",
        "logger.info(f\"Local repository detection - folder: {git_url}, resolved path: {actual_path}, environment: {get_runtime_environment()}\")",
        "get_runtime_environment()",
        "fetch_github_token_from_db(session)",
        "logger.info(f\"Remote repository detection - URL: {git_url}\")",
        "AsyncDetectorWrapper(app.state.activity_executor, is_local=is_local)",
        "StreamingResponse(\n        generate_sse_events(actual_path, github_token, detector_wrapper),\n        media_type=\"text/event-stream\",\n        headers={\n            \"Cache-Control\": \"no-cache\",\n            \"Connection\": \"keep-alive\",\n            \"X-Accel-Buffering\": \"no\"  # Disable nginx buffering\n        }\n    )",
        "generate_sse_events(actual_path, github_token, detector_wrapper)"
      ],
      "nested_functions": [],
      "instance_variables": []
    },
    {
      "start_line": 1567,
      "end_line": 1705,
      "signature": "@app.post(\"/code-confluence/issues\", response_model=IssueTracking)\nasync def create_github_issue(request: GithubIssueSubmissionRequest, session: AsyncSession = Depends(get_session)):",
      "docstring": "\n    Create a GitHub issue based on error information and track it in the database.\n    \n    This endpoint creates a GitHub issue using the provided error information and then updates \n    either the codebase workflow run or repository workflow run record with the issue details.\n    ",
      "function_calls": [
        "Depends(get_session)",
        "fetch_github_token_from_db(session)",
        "len(request.error_message_body)",
        "Github(token)",
        "g.get_repo(\"unoplat/unoplat-code-confluence\")",
        "repo.create_issue(\n                title=title,\n                body=body,\n                labels=labels #type: ignore\n            )",
        "logger.error(f\"GitHub API error: {str(e)}\")",
        "str(e)",
        "HTTPException(\n                status_code=500,\n                detail=f\"Failed to create GitHub issue: {str(e)}\"\n            )",
        "str(e)",
        "IssueTracking(\n            issue_id=str(github_issue.id), \n            issue_url=github_issue.html_url,\n            issue_status=IssueStatus.OPEN\n        )",
        "str(github_issue.id)",
        "IssueTracking(\n            issue_id=issue_tracking.issue_id,\n            issue_number=github_issue.number,\n            issue_url=issue_tracking.issue_url,\n            issue_status=issue_tracking.issue_status,\n            created_at=datetime.now(timezone.utc).isoformat()\n        )",
        "datetime.now(timezone.utc)",
        "datetime.now(timezone.utc).isoformat()",
        "issue_tracking_full.model_dump()",
        "select(CodebaseWorkflowRun).where(condition)",
        "select(CodebaseWorkflowRun)",
        "session.execute(codebase_run_query)",
        "codebase_run_result.scalar_one_or_none()",
        "HTTPException(status_code=404, \n                                   detail=f\"Codebase workflow run {request.codebase_workflow_run_id} not found\")",
        "select(RepositoryWorkflowRun).where(condition)",
        "select(RepositoryWorkflowRun)",
        "session.execute(repo_run_query)",
        "repo_run_result.scalar_one_or_none()",
        "HTTPException(status_code=404, \n                               detail=f\"Repository workflow run {request.parent_workflow_run_id} not found\")",
        "session.commit()",
        "logger.error(f\"Error creating GitHub issue: {str(e)}\")",
        "str(e)",
        "str(e)",
        "traceback.format_exc()",
        "HTTPException(\n            status_code=500,\n            detail={\n                \"message\": \"Faced an error while creating GitHub issue. Please try after some time.\",\n                \"error_context\": error_context\n            }\n        )"
      ],
      "nested_functions": [],
      "instance_variables": []
    }
  ],
  "classes": []
}