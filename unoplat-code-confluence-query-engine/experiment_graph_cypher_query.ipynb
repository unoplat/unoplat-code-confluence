{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "\n",
    "class UnoplatEmbeddingGenerator:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('jinaai/jina-embeddings-v3', trust_remote_code=True)\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        task = 'retrieval.query'\n",
    "        return self.model.encode(texts, task=task).tolist()\n",
    "   \n",
    "    def generate_embeddings_for_single_text(self, text: str) -> List[float]:\n",
    "        task = 'retrieval.query'\n",
    "        return self.model.encode(text, task=task).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "uri = 'bolt://localhost:7687'\n",
    "user = 'neo4j'\n",
    "password = 'Ke7Rk7jB:Jn2Uz:'\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query, parameters=None):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query, parameters)\n",
    "        return result.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"which function processes string-based inputs to return an output string?\"\n",
    "embedding_generator = UnoplatEmbeddingGenerator()\n",
    "embedding = embedding_generator.generate_embeddings_for_single_text(user_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MethodName': 'autograd.StringBasedFunction.forward', 'Objective': 'The `forward` function processes string-based inputs to return an output string with an optional role description, while logging the input-output relationship and setting the gradient function for backpropagation, thereby supporting machine learning optimization and feedback mechanisms.', 'score': 0.7843685150146484}\n",
      "{'MethodName': 'autograd.StringBasedFunction.__init__', 'Objective': 'The `__init__` function of the `StringBasedFunction` class sets up an autograd function for string operations, initializing internal state and logging capabilities while ensuring type safety and integration within complex systems.', 'score': 0.7730774879455566}\n",
      "{'MethodName': 'autograd.StringBasedFunction._backward_through_string_fn_base', 'Objective': 'The `_backward_through_string_fn_base` function computes gradients for a list of variables based on a response variable by constructing an input string and generating a backward prompt. It logs prompts and gradients for traceability, updates variables with computed gradients, and incorporates metadata reduction for efficiency. Recent enhancements allow for additional variables, improving gradient computation accuracy and performance feedback.', 'score': 0.76025390625}\n"
     ]
    }
   ],
   "source": [
    "# Define the Cypher query\n",
    "cypher_query = \"\"\"\n",
    "CALL db.index.vector.queryNodes($index_name, $k, $embedding) \n",
    "YIELD node, score\n",
    "RETURN node.qualified_name AS MethodName, node.objective AS Objective, score\n",
    "ORDER BY score DESC\n",
    "LIMIT 3;\n",
    "\"\"\"\n",
    "\n",
    "# Parameters for the query\n",
    "parameters = {\n",
    "    \"index_name\": \"Method_implementation_embedding_vector_index\",  # Replace with your index name\n",
    "    \"k\": 10,  # Number of nearest neighbors\n",
    "    \"embedding\": embedding  # Your query vector\n",
    "}\n",
    "\n",
    "# Execute the query\n",
    "results = run_query(cypher_query, parameters)\n",
    "\n",
    "# Display the results\n",
    "for record in results:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    reasoning='The user is asking for an overview of the codebase and specifically inquiring about a function that processes string-based inputs. This indicates a need for both a summary of the codebase and details about a specific function, which aligns with the intents of CODE_SUMMARIZATION and FUNCTIONAL_IMPLEMENTATION. We will include both intents in the response.',\n",
      "    user_intent=['CODE_SUMMARIZATION', 'FUNCTIONAL_IMPLEMENTATION']\n",
      ")\n",
      "Prediction(\n",
      "    answer=Prediction(\n",
      "    reasoning='The user is asking for an overview of the codebase and specifically inquiring about a function that processes string-based inputs. This indicates a need for both a summary of the codebase and details about a specific function, which aligns with the intents of CODE_SUMMARIZATION and FUNCTIONAL_IMPLEMENTATION. We will include both intents in the response.',\n",
      "    user_intent=['CODE_SUMMARIZATION', 'FUNCTIONAL_IMPLEMENTATION']\n",
      ")\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "import json\n",
    "import dspy \n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "import litellm\n",
    "litellm.set_verbose=False\n",
    "#ollama_provider = dspy.OllamaLocal(model=\"qwen2.5:72b-instruct-fp16\",base_url=\"http://206.1.58.174:11434\",temperature=0.0,format=\"json\")\n",
    "\n",
    "\n",
    "ollama_provider = dspy.LM(model=\"ollama/qwen2.5:72b-instruct-fp16\",api_base=\"http://206.1.58.174:11434\",temperature=0.0)\n",
    "openai_provider = dspy.LM(model=\"openai/gpt-4o-mini\",api_key=\"insert your api key here\",max_tokens=512,temperature=0.0)\n",
    "dspy.configure(lm=openai_provider)\n",
    "\n",
    "# Prepare intent descriptions\n",
    "intent_descriptions = {\n",
    "    \"CODE_SUMMARIZATION\": \"User wants an overview or summary of the codebase.\",\n",
    "    \"CODE_FEATURE\": \"User is looking for specific features that can be answered by going through the package summaries.\",\n",
    "    \"FUNCTIONAL_IMPLEMENTATION\": \"User wants detailed understanding at the function level.\"\n",
    "}\n",
    "\n",
    "# Create a context string\n",
    "def get_intent_context():\n",
    "    context = \"The possible user intents are:\\n\"\n",
    "    for intent_name, description in intent_descriptions.items():\n",
    "        context += f\"- **{intent_name}**: {description}\\n\"\n",
    "    return context\n",
    "\n",
    "# class UserIntent(str, Enum):\n",
    "#     CODE_SUMMARIZATION = \"CODE_SUMMARIZATION\"\n",
    "#     CODE_FEATURE = \"CODE_FEATURE\"\n",
    "#     FUNCTIONAL_IMPLEMENTATION = \"FUNCTIONAL_IMPLEMENTATION\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CodeConfluenceUserQuerySignature(dspy.Signature):\n",
    "    \"\"\"Based on user query and context of intents, return the user intent as list of intents in valid json format. Verify the json format strictly before returning.\"\"\"\n",
    "    user_query: str = dspy.InputField(desc=\"This will contain user query\")\n",
    "    intent_descriptions: str = dspy.InputField(desc=\"this will contain intents and their respective descriptions\")\n",
    "    user_intent: List[str] = dspy.OutputField(default_factory=list,desc=\"This will strictly return json format of list of items from intents\")\n",
    "\n",
    "    \n",
    "   \n",
    "class CodeConfluenceIntentDetectionModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.intent_detection = dspy.TypedChainOfThought(CodeConfluenceUserQuerySignature)\n",
    "\n",
    "    def forward(self, user_query: str):\n",
    "        intent_detection = self.intent_detection(user_query=user_query,intent_descriptions=json.dumps(get_intent_context()))\n",
    "        print(intent_detection)\n",
    "        return dspy.Prediction(answer=intent_detection)\n",
    "        \n",
    " \n",
    "intent_module = CodeConfluenceIntentDetectionModule()\n",
    "print(intent_module(user_query=\"tell me about codebase overview and which function processes string-based inputs to return an output string?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t1.\tWhat is the primary purpose of this codebase?\n",
    "\t•\tUnderstanding the main goal or functionality of the application or module.\n",
    "\t2.\tHow is the project structured?\n",
    "\t•\tExploring the directory layout, modules, and how they interact.\n",
    "\t3.\tWhat are the core features and functionalities?\n",
    "\t•\tIdentifying the key components and services provided.\n",
    "\t4.\tWhere is the code related to specific features located?\n",
    "\t•\tLocating classes, methods, or scripts that implement certain functionalities.\n",
    "\t5.\tHow do different components communicate?\n",
    "\t•\tUnderstanding APIs, interfaces, and data flow between modules.\n",
    "\t6.\tWhat are the entry points of the application?\n",
    "\t•\tFinding the main functions, scripts, or controllers that start processes.\n",
    "\t7.\tHow is data handled and stored?\n",
    "\t•\tLooking into databases, data models, and storage mechanisms used.\n",
    "\t8.\tWhat external dependencies or services are used?\n",
    "\t•\tIdentifying third-party libraries, APIs, or microservices integrated into the project.\n",
    "\t9.\tHow is the application configured?\n",
    "\t•\tUnderstanding configuration files, environment variables, and settings.\n",
    "\t10.\tHow is error handling and logging implemented?\n",
    "\t•\tReviewing how the application handles exceptions and records events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_list = [\"looking for function designed for handling datasets specifically from the BIG-Bench Hard challenge.\",\n",
    "                   \"which function processes string-based inputs to return an output string?\",\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_functions(tx, query_embedding, top_k=5):\n",
    "    query = \"\"\"\n",
    "    CALL db.index.vector.queryNodes('Method_objective_embedding_vector_index', $top_k, $embedding)\n",
    "    YIELD node, score\n",
    "    RETURN node.qualified_name AS function_name, node.objective AS function_objective, node.implementation_summary AS function_summary, score\n",
    "    ORDER BY score DESC\n",
    "    \"\"\"\n",
    "    return list(tx.run(query, embedding=query_embedding, top_k=top_k))\n",
    "\n",
    "def search_similar_packages(tx, query_embedding, top_k=5):\n",
    "    query = \"\"\"\n",
    "    CALL db.index.vector.queryNodes('Package_objective_embedding_vector_index', $top_k, $embedding)\n",
    "    YIELD node, score\n",
    "    RETURN node.qualified_name AS package_name, node.objective AS package_objective, score\n",
    "    ORDER BY score DESC\n",
    "    \"\"\"\n",
    "    return list(tx.run(query, embedding=query_embedding, top_k=top_k))\n",
    "\n",
    "def search_similar_classes(tx, query_embedding, top_k=5):\n",
    "    query = \"\"\"\n",
    "    CALL db.index.vector.queryNodes('Class_objective_embedding_vector_index', $top_k, $embedding)\n",
    "    YIELD node, score\n",
    "    RETURN node.qualified_name AS class_name, node.objective AS class_objective, score\n",
    "    ORDER BY score DESC\n",
    "    \"\"\"\n",
    "    return list(tx.run(query, embedding=query_embedding, top_k=top_k))\n",
    "\n",
    "def search_similar_codebases(tx, query_embedding, top_k=5):\n",
    "    query = \"\"\"\n",
    "    CALL db.index.vector.queryNodes('Codebase_objective_embedding_vector_index', $top_k, $embedding)\n",
    "    YIELD node, score\n",
    "    RETURN node.qualified_name AS codebase_name, node.objective AS codebase_objective, score\n",
    "    ORDER BY score DESC\n",
    "    \"\"\"\n",
    "    return list(tx.run(query, embedding=query_embedding, top_k=top_k))\n",
    "\n",
    "def get_function_hierarchy_and_details(tx, function_name):\n",
    "    query = \"\"\"\n",
    "    MATCH (f:Method {qualified_name: $function_name})\n",
    "    OPTIONAL MATCH (f)<-[:CONTAINS]-(c:Class)<-[:CONTAINS]-(p:Package)<-[:CONTAINS]-(cb:Codebase)\n",
    "    RETURN \n",
    "        cb.qualified_name AS codebase_name,\n",
    "        cb.objective AS codebase_objective,\n",
    "        cb.implementation_summary AS codebase_summary,\n",
    "        p.qualified_name AS package_name,\n",
    "        p.objective AS package_objective,\n",
    "        p.implementation_summary AS package_summary,\n",
    "        c.qualified_name AS class_name,\n",
    "        c.objective AS class_objective,\n",
    "        c.implementation_summary AS class_summary,\n",
    "        f.qualified_name AS function_name,\n",
    "        f.objective AS function_objective,\n",
    "        f.implementation_summary AS function_summary\n",
    "    \"\"\"\n",
    "    return tx.run(query, function_name=function_name).single()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "from loguru import logger as log\n",
    "\n",
    "\n",
    "class UnoplatRerankEmbedding:\n",
    "    def __init__(self, sentence_transformer_model: str):\n",
    "        self.sentence_rerank_model = SentenceTransformer(sentence_transformer_model, trust_remote_code=False)\n",
    "        self.query_prompt_name = \"s2p_query\"  # Change to \"s2s_query\" for sentence-to-sentence tasks\n",
    "\n",
    "    def generate_rerank_embedding(self, query: List[str], documents: List[str]):\n",
    "        # Encode the queries with the s2p query prompt\n",
    "        query_embeddings = self.sentence_rerank_model.encode(query, prompt_name=self.query_prompt_name)\n",
    "        # Encode the documents (no prompt needed)\n",
    "        doc_embeddings = self.sentence_rerank_model.encode(documents)\n",
    "        similarities = self.sentence_rerank_model.similarity(query_embeddings, doc_embeddings)\n",
    "        log.info(f\"Similarity Matrix: {similarities}\")\n",
    "        return similarities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy \n",
    "from typing import Dict, List\n",
    "\n",
    "import litellm\n",
    "litellm.set_verbose=False\n",
    "#ollama_provider = dspy.OllamaLocal(model=\"qwen2.5:72b-instruct-fp16\",base_url=\"http://206.1.58.174:11434\",temperature=0.0,format=\"json\")\n",
    "\n",
    "\n",
    "class CodeConfluenceUserQueryReRankSignature(dspy.Signature):\n",
    "    \"\"\"Based on user query and possible answers, return the most relevant function names from the list based on the user query\"\"\"\n",
    "    user_query: str = dspy.InputField(desc=\"This will contain user query\")\n",
    "    possible_answers: Dict[str,str] = dspy.InputField(desc=\"this will contain list of possibly relevant answers with function name and their description \")\n",
    "    relevant_answers: Dict[str,int] = dspy.OutputField(default_factory=list,desc=\"return  the most relevant function names from the list based on the functions descriptions matching with user query with score from 1 to 10 with 10 being the highest match \")\n",
    "\n",
    "    \n",
    "   \n",
    "class CodeConfluenceUserQueryReRankModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rerank_module = dspy.TypedChainOfThought(CodeConfluenceUserQueryReRankSignature)\n",
    "\n",
    "    def forward(self, user_query: str, possible_answers: List[str]):\n",
    "        rerank_answers = self.rerank_module(user_query=user_query,possible_answers=possible_answers)\n",
    "        print(rerank_answers)\n",
    "        return dspy.Prediction(answer=rerank_answers)\n",
    "        \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydantic data model to hold <Record codebase_name='textgrad' codebase_objective='Provide a modular framework for managing and validating language model engines, enhancing logging, error handling, and performance optimization, while supporting machine learning tasks and educational analysis through customizable prompts.' codebase_summary='The `engine` package provides a modular framework for managing and validating language model engines, essential for state management and evaluation in machine learning tasks, including those involving GPQA, MMLU, and GSM8K datasets. It implements a singleton pattern for consistent engine management, enhances logging capabilities, and includes comprehensive error handling and data caching. Utility functions optimize performance, such as image URL validation and efficient downloading, which are vital for structured task configuration and dataset management. The package also supports automatic differentiation and efficient gradient computation, integrating optimization strategies for parameter gradients, thereby improving usability in educational navigation and analysis through customizable prompts.' package_name='textgrad.tasks' package_objective='The `tasks` package aims to provide a comprehensive suite of tools for managing and evaluating GPQA, MMLU, and GSM8K datasets, facilitating structured task configuration, question extraction, answer evaluation, and efficient dataset management for enhanced educational navigation and analysis.' package_summary='The `tasks` package offers a comprehensive suite of tools for managing GPQA, MMLU, and GSM8K datasets, with a strong emphasis on the `GSM8K` class for extracting questions, answers, and reasoning for mathematical problem-solving. It efficiently supports the BIG-Bench Hard challenge by enabling structured task configuration, parameter validation, and effective data loading for reasoning questions. The `GPQA` class retrieves questions, shuffles multiple-choice answers, and evaluates responses using regular expressions for scoring accuracy. The `MMLU` class evaluates multiple-choice responses, integrating libraries for dataset management and evaluation metrics. The `GPQAInstanceDataset` class enhances usability with methods for answer extraction, scoring, and question retrieval, while the `MMLUInstanceDataset` class manages MMLU datasets, offering methods for question retrieval and answer formatting. The `big_bench_hard` class introduces advanced data processing methods for numeric extraction and string comparison, utilizing libraries for complex manipulations in machine learning evaluations. The `LeetCodeHardEval` class manages a dataset of challenging LeetCode problems, providing methods for retrieving problem descriptions and specific problem data, ensuring dataset availability. The `DataLoader` class efficiently manages datasets for machine learning by enabling batch processing and iteration, with customizable options for batch size and shuffling. Additionally, the package includes the `Dataset` abstract base class, which defines a template for structured datasets, requiring subclasses to implement indexing and providing a method for length retrieval. The package supports dynamic instance loading with robust error handling and configuration options, collectively improving educational navigation and user interaction for comprehensive dataset analysis and evaluation.' class_name='textgrad.tasks.mmlu' class_objective='The `mmlu` class evaluates multiple-choice responses for correctness using regular expressions and integrates libraries for dataset management and evaluation metrics.' class_summary='The `mmlu` class is designed for evaluating multiple-choice responses, utilizing regular expressions to assess answer correctness against a provided key. It integrates various libraries for dataset management and evaluation metrics, facilitating robust assessment processes.' function_name='textgrad.tasks.mmlu.eval_string_based' function_objective='The function `eval_string_based` evaluates a response string for a correct answer (A-D) by extracting the answer using regular expressions and comparing it to a provided correct answer, returning a score of 1.0 for a match and 0.0 otherwise, while handling cases with no answer found.' function_summary='The function `eval_string_based` is designed to evaluate a response string to determine if it contains a correct answer (A-D) based on a specified pattern. It extracts the answer from the response using regular expressions and compares it to the provided correct answer. The function returns a score of 1.0 for a correct match and 0.0 for an incorrect one. It effectively handles cases where no answer is found, ensuring robust evaluation. This function is part of the `mmlu` class, which may utilize various imports such as `platformdirs`, `textgrad.variable`, and `datasets` for enhanced functionality, although it does not directly interact with any specific fields or extensions.'>\n",
    "class CodeConfluenceFunctionHiearchy(BaseModel):\n",
    "    codebase_name: str = Field(description=\"The name of the codebase\")\n",
    "    codebase_objective: str = Field(description=\"The objective of the codebase\")\n",
    "    codebase_summary: str = Field(description=\"The summary of the codebase\")\n",
    "    package_name: str = Field(description=\"The name of the package\")\n",
    "    package_objective: str = Field(description=\"The objective of the package\")\n",
    "    package_summary: str = Field(description=\"The summary of the package\")\n",
    "    class_name: str = Field(description=\"The name of the class\")\n",
    "    class_objective: str = Field(description=\"The objective of the class\")\n",
    "    class_summary: str = Field(description=\"The summary of the class\")\n",
    "    function_name: str = Field(description=\"The name of the function\")\n",
    "    function_objective: str = Field(description=\"The objective of the function\")\n",
    "    function_summary: str = Field(description=\"The summary of the function\")\n",
    "\n",
    "class CodeConfluenceFunctionHiearchySub(BaseModel):\n",
    "    codebase_name: str = Field(description=\"The name of the codebase\",default=None)\n",
    "    codebase_objective: str = Field(description=\"The objective of the codebase\",default=None)\n",
    "    package_name: str = Field(description=\"The name of the package\",default=None)\n",
    "    package_objective: str = Field(description=\"The objective of the package\",default=None)\n",
    "    class_name: str = Field(description=\"The name of the class\",default=None)\n",
    "    class_objective: str = Field(description=\"The objective of the class\",default=None)\n",
    "    function_name: str = Field(description=\"The name of the function\",default=None)\n",
    "    function_summary: str = Field(description=\"The summary of the function\",default=None)\n",
    "    relevance_score: int = Field(description=\"The relevance score of the function\",default=None)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import dspy\n",
    "\n",
    "class CodeConfluenceUserQueryResponseSignature(dspy.Signature):\n",
    "    \"\"\"Generate a comprehensive response to the user query using the code hierarchy data.\"\"\"\n",
    "    user_query: str = dspy.InputField(desc=\"The user's original query.\")\n",
    "    code_hierarchy: CodeConfluenceFunctionHiearchySub = dspy.InputField(desc=\"The code hierarchy data relevant to the user query.\")\n",
    "    existing_respone : str = dspy.InputField(default=\"No existing response yet\",desc=\"The existing response to the user query based on multiple code hiearchy. It will be empty in the first instance or if there is just one relevant code hiearchy for user query\")\n",
    "    final_response: str = dspy.OutputField(desc=\"final response based on user_query , code_hierarchy and existing_response if it exists\")\n",
    "\n",
    "class CodeConfluenceUserQueryResponseModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.response_module = dspy.ChainOfThought(CodeConfluenceUserQueryResponseSignature)\n",
    "\n",
    "    def forward(self, user_query: str, code_hierarchy: CodeConfluenceFunctionHiearchySub, existing_respone: Optional[str]):\n",
    "        if existing_respone is None:\n",
    "            final_response = self.response_module(user_query=user_query, code_hierarchy=code_hierarchy,existing_respone=\"No existing response yet\")\n",
    "        else:\n",
    "            final_response = self.response_module(user_query=user_query, code_hierarchy=code_hierarchy, existing_respone=existing_respone)\n",
    "        return dspy.Prediction(answer=final_response)\n",
    "\n",
    "response_module = CodeConfluenceUserQueryResponseModule()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(\n",
      "    reasoning='The user is looking for a function designed for handling datasets specifically from the BIG-Bench Hard challenge, which indicates a need for a detailed understanding at the function level. Therefore, the user intent is likely to be related to functional implementation.',\n",
      "    user_intent=['FUNCTIONAL_IMPLEMENTATION']\n",
      ")\n",
      "{'textgrad.tasks.BigBenchHard.__init__': 'The `__init__` function initializes an instance of the `BigBenchHard` class, which is designed for handling datasets specifically from the BIG-Bench Hard challenge. This function requires a `task_name` parameter to specify the task being addressed. Additionally, it accepts an optional `root` directory to define where the dataset is located and a `split` type that defaults to \"train\". The function performs validation checks to ensure that the `root` directory is valid and that the specified `split` is one of the acceptable options. It constructs the path to the dataset file based on the provided parameters and loads the data into a pandas DataFrame for further processing. Furthermore, the function establishes a task description aimed at facilitating the answering of reasoning questions, thereby enhancing the usability of the dataset for various analytical tasks. The class also imports necessary libraries such as `os`, `json`, `pandas`, `subprocess`, `platformdirs`, and `textgrad`, as well as extending the `Dataset` class, which may provide additional functionalities related to dataset handling.', 'textgrad.tasks.BigBenchHard.__getitem__': 'The `__getitem__` function in the `BigBenchHard` class is designed to provide efficient access to specific data points within the `data` attribute, which is a pandas DataFrame. By utilizing the `iloc` method, it retrieves a row based on the provided integer index. The function returns a tuple containing the values associated with the keys \"x\" and \"y\" from the selected row, thereby enhancing data accessibility and usability for users working with the `BigBenchHard` dataset. This functionality is crucial for data manipulation and analysis within the context of the class, which extends the `Dataset` class, ensuring compatibility with various data handling operations.', 'textgrad.tasks.BigBenchHard.__len__': 'The `__len__` function is a special method within the `BigBenchHard` class that returns the length of the `data` attribute of the class instance. This method does not take any parameters other than `self` and returns an integer representing the number of elements in `self.data`. The `BigBenchHard` class is designed to extend the `Dataset` class, indicating that it may inherit properties and methods related to dataset handling. The function is crucial for enabling the use of built-in Python functions that require length checks, such as `len()`, on instances of `BigBenchHard`.', 'textgrad.tasks.BigBenchHard.get_task_description': 'The `get_task_description` method of the `BigBenchHard` class retrieves and returns a predefined task description. This description instructs the user to answer a reasoning question in a specific format, ensuring clarity and consistency in responses. The method does not accept any parameters and returns a string that guides the user on how to format their answer effectively. This functionality is essential for maintaining structured interactions within the context of the `BigBenchHard` class, which extends the `Dataset` class, and is designed to facilitate user engagement in reasoning tasks.', 'textgrad.tasks.BigBenchHard.get_default_task_instruction': 'The function `get_default_task_instruction` is a method within the `BigBenchHard` class that returns the default task description stored in the instance variable `_task_description`. This method does not accept any parameters and has no specified return type. It serves to provide a standardized instruction for tasks within the `BigBenchHard` class, ensuring consistency in task execution. The class itself extends the `Dataset` class and utilizes various imports, including `os`, `json`, `pandas` (aliased as `pd`), `subprocess`, `platformdirs`, and `textgrad` (aliased as `tg`), which may support additional functionalities related to data handling and processing within the context of the tasks.'}\n",
      "Prediction(\n",
      "    reasoning='The user is looking for a function specifically designed for handling datasets from the BIG-Bench Hard challenge. The most relevant function is the `__init__` method of the `BigBenchHard` class, as it initializes an instance of the class specifically for this purpose. It includes parameters for specifying the task and the dataset location, making it essential for dataset handling. The other functions, while related to data access and manipulation, do not specifically address the initialization and handling of the dataset as directly as the `__init__` function does.',\n",
      "    relevant_answers={'textgrad.tasks.BigBenchHard.__init__': 10, 'textgrad.tasks.BigBenchHard.__getitem__': 6, 'textgrad.tasks.BigBenchHard.__len__': 5, 'textgrad.tasks.BigBenchHard.get_task_description': 4, 'textgrad.tasks.BigBenchHard.get_default_task_instruction': 4}\n",
      ")\n",
      "Embedding time: 8.53s\n",
      "Search time: 3.31s\n",
      "Hierarchy time: 0.02s\n",
      "Response time: 4.28s\n",
      "Total time: 16.15s\n",
      "Final response:\n",
      "Prediction(\n",
      "    rationale='We have identified a specific function within the `textgrad` codebase that is designed for handling datasets from the BIG-Bench Hard challenge. The `__init__` function of the `BigBenchHard` class initializes an instance tailored for this purpose, ensuring proper data loading and validation. This function is essential for users looking to work with datasets from the BIG-Bench Hard challenge, as it provides the necessary setup and validation to facilitate further analysis and processing of the data.',\n",
      "    final_response='The function you are looking for is `textgrad.tasks.BigBenchHard.__init__`. This function initializes an instance of the `BigBenchHard` class, specifically designed for handling datasets from the BIG-Bench Hard challenge. It requires a `task_name` parameter to specify the task, and it also accepts an optional `root` directory for the dataset location and a `split` type that defaults to \"train\". The function performs validation checks on the `root` directory and the `split` type, constructs the dataset file path, and loads the data into a pandas DataFrame for further processing. This setup enhances usability for various analytical tasks related to reasoning questions.'\n",
      ")\n",
      "Prediction(\n",
      "    reasoning='The user is asking about a function that processes string-based inputs to return an output string, which suggests they are looking for a detailed understanding of how a specific function works. This aligns with the intent of **FUNCTIONAL_IMPLEMENTATION**. We do not have indications of summarization or specific features being requested, so the most relevant intent is identified.',\n",
      "    user_intent=['FUNCTIONAL_IMPLEMENTATION']\n",
      ")\n",
      "{'textgrad.autograd.StringBasedFunction.__init__': \"The `__init__` function of the `StringBasedFunction` class initializes an autograd function specifically designed for string-based operations. It accepts a callable function for execution and a detailed description of its purpose, ensuring that the function can be effectively utilized within the broader framework. This initialization process sets up the necessary internal state for the object, which includes various conversation and objective instruction templates aimed at providing comprehensive feedback on the function's output. The class extends the base `Function` class, leveraging imports from the `textgrad` library, such as `logger`, `Variable`, and `EngineLM`, to facilitate advanced functionality and logging capabilities. Additionally, it incorporates typing annotations for enhanced code clarity and type safety, making it suitable for integration in complex systems that require string manipulation and evaluation.\", 'textgrad.autograd.StringBasedFunction._backward_through_string_fn_base': \"The `_backward_through_string_fn_base` function, part of the `StringBasedFunction` class, processes a list of variables alongside a response variable to compute gradients for those that require them. It constructs an input string from the provided variables, gathers essential information about the response and each variable, and generates a backward prompt for gradient computation. The function utilizes logging capabilities from the `logger` import to log the prompts and gradients, ensuring traceability and debugging ease. It updates the variables with the computed gradients while maintaining context for each variable's gradients, leveraging the `BackwardContext` for efficient management. Additionally, the function incorporates metadata reduction through the `_reduce_meta` function, enhancing both efficiency and clarity. Recent extensions to the function allow for the inclusion of additional variables, broadening the input set and potentially improving the accuracy of gradient computations, thereby enhancing performance feedback based on the evaluation of the response. This function is designed to work seamlessly within the broader framework of the `Function` class, utilizing various imports from the `textgrad` library to facilitate its operations.\", 'textgrad.tasks.mmlu.eval_string_based': 'The function `eval_string_based` is designed to evaluate a response string to determine if it contains a correct answer (A-D) based on a specified pattern. It extracts the answer from the response using regular expressions and compares it to the provided correct answer. The function returns a score of 1.0 for a correct match and 0.0 for an incorrect one. It effectively handles cases where no answer is found, ensuring robust evaluation. This function is part of the `mmlu` class, which may utilize various imports such as `platformdirs`, `textgrad.variable`, and `datasets` for enhanced functionality, although it does not directly interact with any specific fields or extensions.', 'textgrad.autograd.Function.__call__': \"The `__call__` method of the `Function` class allows instances to be invoked as functions. It forwards all arguments to the `forward` method, which is responsible for the actual processing. This class is designed to extend the functionality of the `ABC` class, ensuring that it adheres to the abstract base class structure. The class imports essential components such as `Variable` from `textgrad.variable` and `EngineLM` from `textgrad.engine`, facilitating advanced operations and variable management. The method's design supports extensibility and integration with other components in the text processing framework.\", 'textgrad.tasks.big_bench_hard.string_based_equality_fn': 'The function `string_based_equality_fn` is designed to compare two inputs of type `tg.Variable`, specifically `prediction` and `ground_truth_answer`. It achieves this by converting the string representations of these variables into integers and then checking for their equality. The function returns 1 if the two integers are equal, indicating a match, and 0 if they are not equal, indicating a mismatch. This function is particularly useful in scenarios where string-based comparisons are required, such as in machine learning model evaluations or data validation processes. The function leverages the `textgrad` library for handling `tg.Variable` types, ensuring compatibility with the broader framework used in the `big_bench_hard` class context.'}\n",
      "Prediction(\n",
      "    reasoning=\"The user is looking for a function that processes string-based inputs to return an output string. The most relevant function from the provided options is `textgrad.autograd.StringBasedFunction.__init__`, as it initializes a function specifically designed for string-based operations. The other functions either focus on evaluating strings or comparing them rather than processing them to return a string output. Therefore, this function is the best match for the user's query.\",\n",
      "    relevant_answers={'textgrad.autograd.StringBasedFunction.__init__': 10, 'textgrad.autograd.StringBasedFunction._backward_through_string_fn_base': 5, 'textgrad.tasks.mmlu.eval_string_based': 4, 'textgrad.autograd.Function.__call__': 3, 'textgrad.tasks.big_bench_hard.string_based_equality_fn': 2}\n",
      ")\n",
      "Embedding time: 6.70s\n",
      "Search time: 0.04s\n",
      "Hierarchy time: 0.00s\n",
      "Response time: 0.00s\n",
      "Total time: 6.74s\n",
      "Final response:\n",
      "Prediction(\n",
      "    rationale='The `StringBasedFunction` class within the `textgrad.autograd` package is specifically designed to handle string-based operations. Its `__init__` function initializes this class, allowing it to process string inputs effectively. This function is crucial for setting up the internal state necessary for string manipulations, making it the appropriate choice for processing string-based inputs to return an output string.',\n",
      "    final_response='The function that processes string-based inputs to return an output string is the `__init__` function of the `StringBasedFunction` class in the `textgrad.autograd` package. This function initializes an autograd function specifically designed for string operations, ensuring efficient handling and manipulation of string data within the framework.'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import time\n",
    "\n",
    "embedding_generator = UnoplatEmbeddingGenerator()\n",
    "# rerank_embedding = UnoplatRerankEmbedding(sentence_transformer_model=\"dunzhang/stella_en_1.5B_v5\")\n",
    "rerank_module = CodeConfluenceUserQueryReRankModule()\n",
    "context = Dict[str,CodeConfluenceFunctionHiearchySub]\n",
    "litellm.set_verbose=False\n",
    "driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"Ke7Rk7jB:Jn2Uz:\"))\n",
    "\n",
    "\n",
    "for user_query in user_query_list:\n",
    "    # Generate embedding for user query\n",
    "    embedding_start = time.time()\n",
    "    user_query_embedding = embedding_generator.generate_embeddings_for_single_text(user_query)\n",
    "    embedding_end = time.time()\n",
    "    embedding_time = embedding_end - embedding_start\n",
    "\n",
    "    user_intent_list: List[str] = intent_module(user_query=user_query).answer.user_intent\n",
    "    \n",
    "    if \"FUNCTIONAL_IMPLEMENTATION\" in user_intent_list:\n",
    "        # Search similar functions\n",
    "        search_start = time.time()\n",
    "        results = search_similar_functions(driver.session(), user_query_embedding)\n",
    "        function_objective_dict = {result[\"function_name\"]:result[\"function_summary\"] for result in results}\n",
    "        print(function_objective_dict)\n",
    "        rerank_results = rerank_module(user_query=user_query,possible_answers=function_objective_dict).answer.relevant_answers\n",
    "        filtered_rerank_results = {k: v for k, v in rerank_results.items() if v > 7}\n",
    "        context = {k: v for k, v in function_objective_dict.items() if k in filtered_rerank_results.keys()}\n",
    "        search_end = time.time()\n",
    "        search_time = search_end - search_start\n",
    "\n",
    "        # Get hierarchy for all function names\n",
    "        hierarchy_start = time.time()\n",
    "        for function_name in context.keys():\n",
    "            function_hierarchy = get_function_hierarchy_and_details(driver.session(), function_name)\n",
    "            function_hierarchy_object = CodeConfluenceFunctionHiearchySub(**function_hierarchy)\n",
    "            function_hierarchy_object.relevance_score = filtered_rerank_results[function_name]\n",
    "            context[function_name] = function_hierarchy_object\n",
    "        hierarchy_end = time.time()\n",
    "        hierarchy_time = hierarchy_end - hierarchy_start\n",
    "\n",
    "        # Generate final response\n",
    "        response_start = time.time()\n",
    "        existing_response = None\n",
    "        final_response = \"\"\n",
    "        for function_name in context.keys():\n",
    "            final_response = response_module(user_query=user_query, code_hierarchy=context[function_name], existing_respone=existing_response)\n",
    "            existing_response = final_response.answer\n",
    "        response_end = time.time()\n",
    "        response_time = response_end - response_start\n",
    "\n",
    "        print(f\"Embedding time: {embedding_time:.2f}s\")\n",
    "        print(f\"Search time: {search_time:.2f}s\")\n",
    "        print(f\"Hierarchy time: {hierarchy_time:.2f}s\")\n",
    "        print(f\"Response time: {response_time:.2f}s\")\n",
    "        print(f\"Total time: {embedding_time + search_time + hierarchy_time + response_time:.2f}s\")\n",
    "        print(\"Final response:\")\n",
    "        print(final_response.answer)\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
